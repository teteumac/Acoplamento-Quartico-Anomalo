{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cópia de LightGBM_MulticlassClassifier_WWCEP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JoWCShoF0u7A",
        "vZvkXhGH0u7B",
        "-9S0Sw940u7C",
        "OaqmAArJ0u7D",
        "rJPgXZgC0u7E",
        "HpwpgaPZ0u7F",
        "jdihPJls0u7H",
        "WaZL8tDj0u7J",
        "E2RqnnnR0u7O",
        "Hg75-f-50u7R",
        "JNV-Qc6E0u7R",
        "QHw3PW9u0u7Y",
        "p8Br8iF40u7c"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teteumac/Acoplamento-Quartico-Anomalo/blob/main/CatBoost_MulticlassClassifier_WWCEP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kiyHgZF0u50"
      },
      "source": [
        "verificar se precisa fazer a normalização\n",
        "\n",
        "arrumar o smearing\n",
        "\n",
        "treinar com features menores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYdIqmLm0u5-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d364a16c-d3a0-4e49-f4cf-4099073ef9d1"
      },
      "source": [
        "!python3 -m pip install mplhep coffea scikit-optimize\n",
        "!pip install matplotlib==3.1.3\n",
        "import lightgbm as lgb\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import mplhep as hep\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.metrics import make_scorer,fbeta_score,precision_score,recall_score,accuracy_score,log_loss,roc_auc_score,classification_report,f1_score,confusion_matrix,roc_curve,precision_recall_curve,average_precision_score\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import RFECV\n",
        "from joblib import dump"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mplhep\n",
            "  Downloading https://files.pythonhosted.org/packages/61/6b/98a02ca2d13a5eb473f482e05a6c1e914478beb8674ba73020fca5c5eefd/mplhep-0.3.7-py3-none-any.whl\n",
            "Collecting coffea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/f3/2082092c317bfbbda97cf71889b4865a809a43d12a78449f8fc281a9aa2d/coffea-0.7.3-py2.py3-none-any.whl (161kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 18.7MB/s \n",
            "\u001b[?25hCollecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mplhep) (20.9)\n",
            "Collecting uhi>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/30/7046fd215607e8af7b3360553ecadd37543251557186d0a584b7d83f6bca/uhi-0.2.1-py3-none-any.whl\n",
            "Collecting mplhep-data\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/61/83bdeb0a0a32b9b7234d0472c23a6d8172df085390d59219bb24d842d8f8/mplhep_data-0.0.2-py3-none-any.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from mplhep) (1.19.5)\n",
            "Collecting matplotlib>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/33/5568d443ba438d95d4db635dd69958056f087e57e1026bee56f959d53f9d/matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 30.7MB/s \n",
            "\u001b[?25hCollecting awkward>=1.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/55/9de22fd5aed55d5b4dfca01dcfe9ee5a8f40adf99d0728454a95930895d3/awkward-1.2.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from coffea) (3.7.4.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (1.4.1)\n",
            "Collecting uproot>=4.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/99/bf1b3995f9cafad30c8df34a99ab286f522cf762a2eb2152ac47609be04e/uproot-4.0.8-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (3.0.0)\n",
            "Requirement already satisfied: numba>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (0.51.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from coffea) (4.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from coffea) (7.6.3)\n",
            "Collecting uproot3>=3.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/69/d893c6eba0dd0d8f82d841d4b85b6e63c52a1b472aec7cf7ae0efedf5a92/uproot3-3.14.4-py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (4.41.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from coffea) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coffea) (1.1.5)\n",
            "Collecting lz4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/52/151c815a486290608e4dc6699a0cfd74141dc5191f8fe928e7d1b28b569e/lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 43.3MB/s \n",
            "\u001b[?25hCollecting hist>=2\n",
            "  Downloading https://files.pythonhosted.org/packages/25/ff/a2c633f5caa2282501e390ddac0191b87d90779ef97416ae7fc39335f8b2/hist-2.3.0-py3-none-any.whl\n",
            "Collecting uproot3-methods>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/57/598207abeb64bf3e0af3fdc19217e56936b6bebabaac6ee270fb151790ce/uproot3_methods-0.10.1-py3-none-any.whl\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mplhep) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->mplhep) (1.3.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->mplhep) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->mplhep) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->mplhep) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from awkward>=1.2.2->coffea) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50.0->coffea) (0.34.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.0.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (3.5.1)\n",
            "Collecting awkward0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b3/376b258ea021eed2c9bdaa1011e0f7b25365157de472d9fae8a2443d9ff5/awkward0-0.15.5-py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->coffea) (2018.9)\n",
            "Collecting histoprint>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/e8/8e1801d69e177f51ff4b7404af0aa0239bffd327455c5be9489fb25ef49c/histoprint-2.1.0-py3-none-any.whl\n",
            "Collecting boost-histogram~=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/3d/b42a7a9ba4b39fc7cce19fe57844df0c347dbcedd9774cb690cc7c7593fe/boost_histogram-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.4->mplhep) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.1->ipywidgets->coffea) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->coffea) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->coffea) (4.7.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->coffea) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->coffea) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->coffea) (5.3.1)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from histoprint>=1.6->hist>=2->coffea) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->coffea) (22.0.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (2.11.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (2.0.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.5.1)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: uhi, mplhep-data, matplotlib, mplhep, awkward, uproot, awkward0, uproot3-methods, uproot3, lz4, histoprint, boost-histogram, hist, coffea, pyaml, scikit-optimize\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed awkward-1.2.3 awkward0-0.15.5 boost-histogram-1.0.2 coffea-0.7.3 hist-2.3.0 histoprint-2.1.0 lz4-3.1.3 matplotlib-3.4.2 mplhep-0.3.7 mplhep-data-0.0.2 pyaml-20.4.0 scikit-optimize-0.8.1 uhi-0.2.1 uproot-4.0.8 uproot3-3.14.4 uproot3-methods-0.10.1\n",
            "Collecting matplotlib==3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/9b/35ab3469fd1509f7636a344940569ebfd33239673fd2318e80b4700a257c/matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "\u001b[31mERROR: mplhep 0.3.7 has requirement matplotlib>=3.4, but you'll have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: matplotlib\n",
            "  Found existing installation: matplotlib 3.4.2\n",
            "    Uninstalling matplotlib-3.4.2:\n",
            "      Successfully uninstalled matplotlib-3.4.2\n",
            "Successfully installed matplotlib-3.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMM54HNdw2_",
        "outputId": "fbd1fdf7-e4e1-45f6-cafa-5a95f5882599"
      },
      "source": [
        "!pip3 install -q catboost\n",
        "import catboost\n",
        "from catboost import CatBoostClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 67.3MB 75kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDQ6eU_3yUB2"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3zO-BUMigar"
      },
      "source": [
        "plt.style.use(hep.style.ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-91vUPVSqlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70b86a7-9bfe-4985-c630-88128f940cd2"
      },
      "source": [
        "!pip install matplotlib==3.1.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg7H6Onk0u6A"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/'\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2kPHDeAJNiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950de80a-16a2-4fd4-81c6-4977aeb32bfa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCSI2avX_1Gy"
      },
      "source": [
        "def open_file_MC( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi \n",
        "        return dataframe\n",
        "\n",
        "def open_file_DD( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]        \n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt',\n",
        "       'jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt',\n",
        "       'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'MultiRP1', 'MultiRP2',\n",
        "       'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi       \n",
        "        return dataframe     \n",
        "\n",
        "def open_file_Data( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi \n",
        "        return dataframe        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g2i8dbh_6_v"
      },
      "source": [
        "select_columns = ['Mww', 'Pt_W_lep', 'Acoplanaridade_Whad_Wlep', 'Acoplanaridade_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Mx',  'Mww/Mx']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHbROLJ1_7eJ"
      },
      "source": [
        "SM =       open_file_MC( PATH + 'DataSet_SM_multiRP.h5')\n",
        "ANOMALO1 = open_file_MC( PATH + 'DataSet_ANOMALO1_multiRP.h5')\n",
        "ANOMALO2 = open_file_MC( PATH + 'DataSet_ANOMALO2_multiRP.h5')\n",
        "ANOMALO3 = open_file_MC( PATH + 'DataSet_ANOMALO3_multiRP.h5')\n",
        "ANOMALO4 = open_file_MC( PATH + 'DataSet_ANOMALO4_multiRP.h5')\n",
        "ANOMALO5 = open_file_MC( PATH + 'DataSet_ANOMALO5_multiRP.h5')\n",
        "ANOMALO6 = open_file_MC( PATH + 'DataSet_ANOMALO6_multiRP.h5')\n",
        "ANOMALO7 = open_file_MC( PATH + 'DataSet_ANOMALO7_multiRP.h5')\n",
        "ANOMALO8 = open_file_MC( PATH + 'DataSet_ANOMALO8_multiRP.h5')\n",
        "\n",
        "label_signal1  = pd.DataFrame( [1]*len( ANOMALO1 ) )\n",
        "label_signal2  = pd.DataFrame( [1]*len( ANOMALO2 ) )\n",
        "label_signal3  = pd.DataFrame( [1]*len( ANOMALO3 ) )\n",
        "label_signal4  = pd.DataFrame( [1]*len( ANOMALO4 ) )\n",
        "label_signal5  = pd.DataFrame( [1]*len( ANOMALO5 ) )\n",
        "label_signal6  = pd.DataFrame( [1]*len( ANOMALO6 ) )\n",
        "label_signal7  = pd.DataFrame( [1]*len( ANOMALO7 ) )\n",
        "label_signal8  = pd.DataFrame( [1]*len( ANOMALO8 ) )\n",
        "label_signalSM = pd.DataFrame( [2]*len( SM ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsuUYk-p0u6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6676ee02-5b62-4e96-e2ae-7cd861be9345"
      },
      "source": [
        "SM = pd.concat( [ SM, label_signalSM ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Standard Model --> ', SM.shape)\n",
        "\n",
        "ANOMALO1 = pd.concat( [ ANOMALO1, label_signal1 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 1 --> ', ANOMALO1.shape)\n",
        "\n",
        "ANOMALO2 = pd.concat( [ ANOMALO2, label_signal2 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 2 --> ', ANOMALO2.shape)\n",
        "\n",
        "ANOMALO3 = pd.concat( [ ANOMALO3, label_signal3 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 3 --> ', ANOMALO3.shape)\n",
        "\n",
        "ANOMALO4 = pd.concat( [ ANOMALO4, label_signal4 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 4 --> ', ANOMALO4.shape)\n",
        "\n",
        "ANOMALO5 = pd.concat( [ ANOMALO5, label_signal5 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 5 --> ', ANOMALO5.shape)\n",
        "\n",
        "ANOMALO6 = pd.concat( [ ANOMALO6, label_signal6 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 6 --> ', ANOMALO6.shape)\n",
        "\n",
        "ANOMALO7 = pd.concat( [ ANOMALO7, label_signal7 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 7 --> ', ANOMALO7.shape)\n",
        "\n",
        "ANOMALO8 = pd.concat( [ ANOMALO8, label_signal8 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 8 --> ', ANOMALO8.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape for Standard Model -->  (112, 24)\n",
            "Shape for Anomalo 1 -->  (1544, 24)\n",
            "Shape for Anomalo 2 -->  (174, 24)\n",
            "Shape for Anomalo 3 -->  (434, 24)\n",
            "Shape for Anomalo 4 -->  (742, 24)\n",
            "Shape for Anomalo 5 -->  (169, 24)\n",
            "Shape for Anomalo 6 -->  (299, 24)\n",
            "Shape for Anomalo 7 -->  (672, 24)\n",
            "Shape for Anomalo 8 -->  (1627, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCd5q15M0u6P"
      },
      "source": [
        "# Smearing para os xi's dos protons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq2jyxyt0u6Q"
      },
      "source": [
        "alterar a composição dos xi's smearing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nheI8IxE0u6R"
      },
      "source": [
        "from random import gauss\n",
        "\n",
        "def xi_smearing(dset):\n",
        "    df_xi1 = []\n",
        "    df_xi2 = []\n",
        "    for i in range(0,len(dset)):\n",
        "        df_xi1.append( gauss( 0, 0.1 * dset['xi1'][i] ) + np.array( dset['xi1'][i] ) )\n",
        "        df_xi2.append( gauss( 0, 0.1 * dset['xi2'][i] ) + np.array( dset['xi2'][i] ) )\n",
        "    xi1_smearing = pd.DataFrame( df_xi1, columns = ['xi1_smearing'] ) \n",
        "    xi2_smearing = pd.DataFrame( df_xi2, columns = ['xi2_smearing'] ) \n",
        "    return pd.concat( [ xi1_smearing, xi2_smearing ], axis = 1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIM3rM3u0u6S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "bb835718-0c52-4029-c848-6fe9a551ab62"
      },
      "source": [
        "'''SM = pd.concat( [ SM, xi_smearing(SM) ], axis = 1 )\n",
        "ANOMALO1 = pd.concat( [ANOMALO1, xi_smearing(ANOMALO1) ], axis = 1 )\n",
        "ANOMALO2 = pd.concat( [ANOMALO2, xi_smearing(ANOMALO2) ], axis = 1 )\n",
        "ANOMALO3 = pd.concat( [ANOMALO3, xi_smearing(ANOMALO3) ], axis = 1 )\n",
        "ANOMALO4 = pd.concat( [ANOMALO4, xi_smearing(ANOMALO4) ], axis = 1 )\n",
        "ANOMALO5 = pd.concat( [ANOMALO5, xi_smearing(ANOMALO5) ], axis = 1 )\n",
        "ANOMALO6 = pd.concat( [ANOMALO6, xi_smearing(ANOMALO6) ], axis = 1 )\n",
        "ANOMALO7 = pd.concat( [ANOMALO7, xi_smearing(ANOMALO7) ], axis = 1 )\n",
        "ANOMALO8 = pd.concat( [ANOMALO8, xi_smearing(ANOMALO8) ], axis = 1 )'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SM = pd.concat( [ SM, xi_smearing(SM) ], axis = 1 )\\nANOMALO1 = pd.concat( [ANOMALO1, xi_smearing(ANOMALO1) ], axis = 1 )\\nANOMALO2 = pd.concat( [ANOMALO2, xi_smearing(ANOMALO2) ], axis = 1 )\\nANOMALO3 = pd.concat( [ANOMALO3, xi_smearing(ANOMALO3) ], axis = 1 )\\nANOMALO4 = pd.concat( [ANOMALO4, xi_smearing(ANOMALO4) ], axis = 1 )\\nANOMALO5 = pd.concat( [ANOMALO5, xi_smearing(ANOMALO5) ], axis = 1 )\\nANOMALO6 = pd.concat( [ANOMALO6, xi_smearing(ANOMALO6) ], axis = 1 )\\nANOMALO7 = pd.concat( [ANOMALO7, xi_smearing(ANOMALO7) ], axis = 1 )\\nANOMALO8 = pd.concat( [ANOMALO8, xi_smearing(ANOMALO8) ], axis = 1 )'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BETRQDZ0u6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "796da865-7b08-41ee-8cc7-05ceb0f58d34"
      },
      "source": [
        "'''DataSet_multiRP_DrellYan = open_file_MC(  PATH +'DataSet_multiRP_DrellYan.h5' )\n",
        "DataSet_multiRP_QCD = open_file_MC(  PATH +'DataSet_multiRP_QCD.h5' )\n",
        "DataSet_multiRP_single_top = open_file_MC( PATH + 'DataSet_multiRP_single_top.h5' )\n",
        "DataSet_multiRP_TTbar = open_file_MC(  PATH +'DataSet_multiRP_TTbar.h5' )\n",
        "DataSet_multiRP_VV_inclusivo = open_file_MC( PATH + 'DataSet_multiRP_VV_inclusivo.h5' )\n",
        "DataSet_multiRP_WJets = open_file_MC( PATH + 'DataSet_multiRP_WJets.h5' )'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"DataSet_multiRP_DrellYan = open_file_MC(  PATH +'DataSet_multiRP_DrellYan.h5' )\\nDataSet_multiRP_QCD = open_file_MC(  PATH +'DataSet_multiRP_QCD.h5' )\\nDataSet_multiRP_single_top = open_file_MC( PATH + 'DataSet_multiRP_single_top.h5' )\\nDataSet_multiRP_TTbar = open_file_MC(  PATH +'DataSet_multiRP_TTbar.h5' )\\nDataSet_multiRP_VV_inclusivo = open_file_MC( PATH + 'DataSet_multiRP_VV_inclusivo.h5' )\\nDataSet_multiRP_WJets = open_file_MC( PATH + 'DataSet_multiRP_WJets.h5' )\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixD4lvJg0u6U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "7a90dfc6-3a39-46c7-b6a4-4c6a57007098"
      },
      "source": [
        "'''label_DrellYan = pd.DataFrame( [0]*len( DataSet_multiRP_DrellYan ) )\n",
        "DataSet_multiRP_DrellYan = pd.concat( [ DataSet_multiRP_DrellYan, label_DrellYan ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "label_QCD = pd.DataFrame( [0]*len( DataSet_multiRP_QCD ) )\n",
        "DataSet_multiRP_QCD = pd.concat( [ DataSet_multiRP_QCD, label_QCD ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "label_SingleTop = pd.DataFrame( [0]*len( DataSet_multiRP_single_top ) )\n",
        "DataSet_multiRP_single_top = pd.concat( [ DataSet_multiRP_single_top, label_SingleTop ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "label_TTbar = pd.DataFrame( [0]*len( DataSet_multiRP_TTbar ) )\n",
        "DataSet_multiRP_TTbar = pd.concat( [ DataSet_multiRP_TTbar, label_TTbar ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "label_VVInclusive = pd.DataFrame( [0]*len( DataSet_multiRP_VV_inclusivo ) )\n",
        "DataSet_multiRP_VV_inclusivo = pd.concat( [ DataSet_multiRP_VV_inclusivo, label_VVInclusive ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "label_WJets = pd.DataFrame( [0]*len( DataSet_multiRP_WJets ) )\n",
        "DataSet_multiRP_WJets = pd.concat( [ DataSet_multiRP_WJets, label_WJets ], axis = 1 ).rename(columns={0: 'label'})'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"label_DrellYan = pd.DataFrame( [0]*len( DataSet_multiRP_DrellYan ) )\\nDataSet_multiRP_DrellYan = pd.concat( [ DataSet_multiRP_DrellYan, label_DrellYan ], axis = 1 ).rename(columns={0: 'label'})\\n\\nlabel_QCD = pd.DataFrame( [0]*len( DataSet_multiRP_QCD ) )\\nDataSet_multiRP_QCD = pd.concat( [ DataSet_multiRP_QCD, label_QCD ], axis = 1 ).rename(columns={0: 'label'})\\n\\nlabel_SingleTop = pd.DataFrame( [0]*len( DataSet_multiRP_single_top ) )\\nDataSet_multiRP_single_top = pd.concat( [ DataSet_multiRP_single_top, label_SingleTop ], axis = 1 ).rename(columns={0: 'label'})\\n\\nlabel_TTbar = pd.DataFrame( [0]*len( DataSet_multiRP_TTbar ) )\\nDataSet_multiRP_TTbar = pd.concat( [ DataSet_multiRP_TTbar, label_TTbar ], axis = 1 ).rename(columns={0: 'label'})\\n\\nlabel_VVInclusive = pd.DataFrame( [0]*len( DataSet_multiRP_VV_inclusivo ) )\\nDataSet_multiRP_VV_inclusivo = pd.concat( [ DataSet_multiRP_VV_inclusivo, label_VVInclusive ], axis = 1 ).rename(columns={0: 'label'})\\n\\nlabel_WJets = pd.DataFrame( [0]*len( DataSet_multiRP_WJets ) )\\nDataSet_multiRP_WJets = pd.concat( [ DataSet_multiRP_WJets, label_WJets ], axis = 1 ).rename(columns={0: 'label'})\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEHxS1GDlPom"
      },
      "source": [
        "data_set_back_multirp = open_file_DD( PATH + 'DataDriven_Background_multiRP.h5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PpFMCqH0u6V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "36c32e63-ca15-4137-a397-5526e5f9e364"
      },
      "source": [
        "label_back = pd.DataFrame( [0]*len( data_set_back_multirp ) )\n",
        "data_set_back_multirp = pd.concat( [ data_set_back_multirp, label_back ], axis = 1 ).rename(columns={0: 'label'})\n",
        "data_set_back_multirp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mww</th>\n",
              "      <th>Pt_W_lep</th>\n",
              "      <th>dPhi_Whad_Wlep</th>\n",
              "      <th>dPhi_jatos_MET</th>\n",
              "      <th>jetAK8_pt</th>\n",
              "      <th>jetAK8_eta</th>\n",
              "      <th>jetAK8_prunedMass</th>\n",
              "      <th>jetAK8_tau21</th>\n",
              "      <th>METPt</th>\n",
              "      <th>muon_pt</th>\n",
              "      <th>muon_eta</th>\n",
              "      <th>ExtraTracks</th>\n",
              "      <th>Yww</th>\n",
              "      <th>xi1</th>\n",
              "      <th>xi2</th>\n",
              "      <th>MultiRP1</th>\n",
              "      <th>MultiRP2</th>\n",
              "      <th>Mx</th>\n",
              "      <th>Yx</th>\n",
              "      <th>Mww/Mx</th>\n",
              "      <th>Yww_Yx</th>\n",
              "      <th>weight</th>\n",
              "      <th>Acoplanaridade_Whad_Wlep</th>\n",
              "      <th>Acoplanaridade_jatos_MET</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>706.608342</td>\n",
              "      <td>229.433921</td>\n",
              "      <td>3.104932</td>\n",
              "      <td>3.016140</td>\n",
              "      <td>244.365721</td>\n",
              "      <td>0.758463</td>\n",
              "      <td>8.573437</td>\n",
              "      <td>0.735708</td>\n",
              "      <td>42.810390</td>\n",
              "      <td>186.830750</td>\n",
              "      <td>1.003712</td>\n",
              "      <td>34.0</td>\n",
              "      <td>1.106191</td>\n",
              "      <td>0.052109</td>\n",
              "      <td>0.111753</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>992.039526</td>\n",
              "      <td>0.381479</td>\n",
              "      <td>0.712278</td>\n",
              "      <td>0.724713</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.011669</td>\n",
              "      <td>0.039933</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>736.491948</td>\n",
              "      <td>329.974506</td>\n",
              "      <td>3.067420</td>\n",
              "      <td>2.368258</td>\n",
              "      <td>369.061507</td>\n",
              "      <td>1.535922</td>\n",
              "      <td>16.263502</td>\n",
              "      <td>0.706058</td>\n",
              "      <td>97.642898</td>\n",
              "      <td>262.862366</td>\n",
              "      <td>1.575521</td>\n",
              "      <td>42.0</td>\n",
              "      <td>1.494392</td>\n",
              "      <td>0.077207</td>\n",
              "      <td>0.113172</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1215.179468</td>\n",
              "      <td>0.191209</td>\n",
              "      <td>0.606077</td>\n",
              "      <td>1.303183</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.023610</td>\n",
              "      <td>0.246160</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1012.055130</td>\n",
              "      <td>250.191224</td>\n",
              "      <td>-2.989816</td>\n",
              "      <td>3.101588</td>\n",
              "      <td>447.352242</td>\n",
              "      <td>0.476539</td>\n",
              "      <td>7.604977</td>\n",
              "      <td>0.855670</td>\n",
              "      <td>197.791872</td>\n",
              "      <td>67.529274</td>\n",
              "      <td>1.426694</td>\n",
              "      <td>46.0</td>\n",
              "      <td>1.392727</td>\n",
              "      <td>0.099899</td>\n",
              "      <td>0.120155</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1424.281045</td>\n",
              "      <td>0.092312</td>\n",
              "      <td>0.710573</td>\n",
              "      <td>1.300415</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.048312</td>\n",
              "      <td>0.012734</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>801.627682</td>\n",
              "      <td>401.144449</td>\n",
              "      <td>2.196309</td>\n",
              "      <td>2.259357</td>\n",
              "      <td>394.456184</td>\n",
              "      <td>1.852053</td>\n",
              "      <td>8.174912</td>\n",
              "      <td>0.834630</td>\n",
              "      <td>135.615686</td>\n",
              "      <td>265.935516</td>\n",
              "      <td>0.936441</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.957040</td>\n",
              "      <td>0.043641</td>\n",
              "      <td>0.091501</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>821.491532</td>\n",
              "      <td>0.370178</td>\n",
              "      <td>0.975820</td>\n",
              "      <td>0.586861</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.300893</td>\n",
              "      <td>0.280824</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>956.729620</td>\n",
              "      <td>344.981106</td>\n",
              "      <td>2.988486</td>\n",
              "      <td>2.411655</td>\n",
              "      <td>526.427156</td>\n",
              "      <td>0.508421</td>\n",
              "      <td>171.405609</td>\n",
              "      <td>0.320342</td>\n",
              "      <td>91.846592</td>\n",
              "      <td>272.636658</td>\n",
              "      <td>0.215901</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.207985</td>\n",
              "      <td>0.102238</td>\n",
              "      <td>0.042712</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>859.065180</td>\n",
              "      <td>-0.436410</td>\n",
              "      <td>1.113687</td>\n",
              "      <td>0.644395</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.048735</td>\n",
              "      <td>0.232346</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417340</th>\n",
              "      <td>670.308551</td>\n",
              "      <td>235.492546</td>\n",
              "      <td>-2.849825</td>\n",
              "      <td>-2.691901</td>\n",
              "      <td>300.836813</td>\n",
              "      <td>1.093756</td>\n",
              "      <td>18.986254</td>\n",
              "      <td>0.844727</td>\n",
              "      <td>186.298483</td>\n",
              "      <td>59.261715</td>\n",
              "      <td>0.238643</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.251635</td>\n",
              "      <td>0.100328</td>\n",
              "      <td>0.043516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>858.969850</td>\n",
              "      <td>-0.417659</td>\n",
              "      <td>0.780363</td>\n",
              "      <td>0.669293</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.092873</td>\n",
              "      <td>0.143141</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417341</th>\n",
              "      <td>1160.930547</td>\n",
              "      <td>243.647700</td>\n",
              "      <td>2.740906</td>\n",
              "      <td>2.720288</td>\n",
              "      <td>217.646715</td>\n",
              "      <td>0.780753</td>\n",
              "      <td>65.993027</td>\n",
              "      <td>0.417648</td>\n",
              "      <td>139.734327</td>\n",
              "      <td>103.982986</td>\n",
              "      <td>1.947919</td>\n",
              "      <td>50.0</td>\n",
              "      <td>2.293765</td>\n",
              "      <td>0.073859</td>\n",
              "      <td>0.100618</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1120.683552</td>\n",
              "      <td>0.154591</td>\n",
              "      <td>1.035913</td>\n",
              "      <td>2.139174</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.127542</td>\n",
              "      <td>0.134105</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417342</th>\n",
              "      <td>715.553066</td>\n",
              "      <td>228.009132</td>\n",
              "      <td>-3.040333</td>\n",
              "      <td>-2.828814</td>\n",
              "      <td>201.868402</td>\n",
              "      <td>0.601398</td>\n",
              "      <td>7.845040</td>\n",
              "      <td>0.798828</td>\n",
              "      <td>187.275407</td>\n",
              "      <td>59.687149</td>\n",
              "      <td>1.564484</td>\n",
              "      <td>64.0</td>\n",
              "      <td>-1.481526</td>\n",
              "      <td>0.072363</td>\n",
              "      <td>0.066277</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>900.291512</td>\n",
              "      <td>-0.043930</td>\n",
              "      <td>0.794802</td>\n",
              "      <td>-1.437596</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.032232</td>\n",
              "      <td>0.099561</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417343</th>\n",
              "      <td>708.358903</td>\n",
              "      <td>108.294984</td>\n",
              "      <td>-2.476245</td>\n",
              "      <td>-2.448281</td>\n",
              "      <td>234.299604</td>\n",
              "      <td>0.933830</td>\n",
              "      <td>66.672958</td>\n",
              "      <td>0.647939</td>\n",
              "      <td>54.611703</td>\n",
              "      <td>53.726337</td>\n",
              "      <td>2.363542</td>\n",
              "      <td>53.0</td>\n",
              "      <td>-1.742363</td>\n",
              "      <td>0.056347</td>\n",
              "      <td>0.118468</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1062.134849</td>\n",
              "      <td>0.371552</td>\n",
              "      <td>0.666920</td>\n",
              "      <td>-2.113915</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.211787</td>\n",
              "      <td>0.220688</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417344</th>\n",
              "      <td>1105.014206</td>\n",
              "      <td>19.040141</td>\n",
              "      <td>-2.409515</td>\n",
              "      <td>-3.055847</td>\n",
              "      <td>256.651117</td>\n",
              "      <td>1.251722</td>\n",
              "      <td>10.547384</td>\n",
              "      <td>0.551719</td>\n",
              "      <td>205.331167</td>\n",
              "      <td>190.476944</td>\n",
              "      <td>1.241173</td>\n",
              "      <td>45.0</td>\n",
              "      <td>-0.005464</td>\n",
              "      <td>0.077680</td>\n",
              "      <td>0.127957</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1296.069550</td>\n",
              "      <td>0.249549</td>\n",
              "      <td>0.852589</td>\n",
              "      <td>-0.255013</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.233028</td>\n",
              "      <td>0.027294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>417345 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                Mww    Pt_W_lep  ...  Acoplanaridade_jatos_MET  label\n",
              "0        706.608342  229.433921  ...                  0.039933      0\n",
              "1        736.491948  329.974506  ...                  0.246160      0\n",
              "2       1012.055130  250.191224  ...                  0.012734      0\n",
              "3        801.627682  401.144449  ...                  0.280824      0\n",
              "4        956.729620  344.981106  ...                  0.232346      0\n",
              "...             ...         ...  ...                       ...    ...\n",
              "417340   670.308551  235.492546  ...                  0.143141      0\n",
              "417341  1160.930547  243.647700  ...                  0.134105      0\n",
              "417342   715.553066  228.009132  ...                  0.099561      0\n",
              "417343   708.358903  108.294984  ...                  0.220688      0\n",
              "417344  1105.014206   19.040141  ...                  0.027294      0\n",
              "\n",
              "[417345 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKFQy0Up0u6V"
      },
      "source": [
        "Dataset_Signal_Back1  = pd.concat( [ ANOMALO1 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back2  = pd.concat( [ ANOMALO2 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back3  = pd.concat( [ ANOMALO3 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back4  = pd.concat( [ ANOMALO4 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back5  = pd.concat( [ ANOMALO5 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back6  = pd.concat( [ ANOMALO6 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back7  = pd.concat( [ ANOMALO7 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back8  = pd.concat( [ ANOMALO8 , data_set_back_multirp, SM  ], axis = 0, sort = False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDtrkc_40u6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "e02521c1-a24d-4075-91ec-f708ee931d7e"
      },
      "source": [
        "data_set_dados_multirp = open_file_Data( PATH + 'DataSet_dados_multiRP.h5' )\n",
        "data_set_dados_multirp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mww</th>\n",
              "      <th>Pt_W_lep</th>\n",
              "      <th>dPhi_Whad_Wlep</th>\n",
              "      <th>dPhi_jatos_MET</th>\n",
              "      <th>jetAK8_pt</th>\n",
              "      <th>jetAK8_eta</th>\n",
              "      <th>jetAK8_prunedMass</th>\n",
              "      <th>jetAK8_tau21</th>\n",
              "      <th>METPt</th>\n",
              "      <th>muon_pt</th>\n",
              "      <th>muon_eta</th>\n",
              "      <th>ExtraTracks</th>\n",
              "      <th>Yww</th>\n",
              "      <th>xi1</th>\n",
              "      <th>xi2</th>\n",
              "      <th>Mx</th>\n",
              "      <th>Yx</th>\n",
              "      <th>Mww/Mx</th>\n",
              "      <th>Yww_Yx</th>\n",
              "      <th>weight</th>\n",
              "      <th>Acoplanaridade_Whad_Wlep</th>\n",
              "      <th>Acoplanaridade_jatos_MET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>741.827618</td>\n",
              "      <td>213.047177</td>\n",
              "      <td>3.010690</td>\n",
              "      <td>1.859894</td>\n",
              "      <td>226.343484</td>\n",
              "      <td>0.390933</td>\n",
              "      <td>74.849960</td>\n",
              "      <td>0.565876</td>\n",
              "      <td>57.706423</td>\n",
              "      <td>196.705261</td>\n",
              "      <td>1.649120</td>\n",
              "      <td>34.0</td>\n",
              "      <td>1.269749e-321</td>\n",
              "      <td>-1.482681</td>\n",
              "      <td>0.105611</td>\n",
              "      <td>0.073181</td>\n",
              "      <td>1142.870196</td>\n",
              "      <td>-0.183416</td>\n",
              "      <td>0.649092</td>\n",
              "      <td>-1.299265</td>\n",
              "      <td>0.041668</td>\n",
              "      <td>0.407977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>877.505354</td>\n",
              "      <td>123.897340</td>\n",
              "      <td>2.012689</td>\n",
              "      <td>2.549330</td>\n",
              "      <td>233.792329</td>\n",
              "      <td>0.997459</td>\n",
              "      <td>32.459290</td>\n",
              "      <td>0.487617</td>\n",
              "      <td>92.757254</td>\n",
              "      <td>64.812515</td>\n",
              "      <td>2.137774</td>\n",
              "      <td>33.0</td>\n",
              "      <td>1.269749e-321</td>\n",
              "      <td>1.974074</td>\n",
              "      <td>0.059030</td>\n",
              "      <td>0.074471</td>\n",
              "      <td>861.936296</td>\n",
              "      <td>0.116178</td>\n",
              "      <td>1.018063</td>\n",
              "      <td>1.857896</td>\n",
              "      <td>0.359341</td>\n",
              "      <td>0.188523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>781.795723</td>\n",
              "      <td>350.697573</td>\n",
              "      <td>-3.013293</td>\n",
              "      <td>-2.860292</td>\n",
              "      <td>402.634166</td>\n",
              "      <td>1.507299</td>\n",
              "      <td>37.748211</td>\n",
              "      <td>0.642403</td>\n",
              "      <td>259.948342</td>\n",
              "      <td>101.810310</td>\n",
              "      <td>1.184208</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.168085</td>\n",
              "      <td>0.099538</td>\n",
              "      <td>0.095934</td>\n",
              "      <td>1270.353032</td>\n",
              "      <td>-0.018436</td>\n",
              "      <td>0.615416</td>\n",
              "      <td>1.186522</td>\n",
              "      <td>0.040839</td>\n",
              "      <td>0.089541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>650.542403</td>\n",
              "      <td>294.405514</td>\n",
              "      <td>-3.018462</td>\n",
              "      <td>2.739984</td>\n",
              "      <td>253.529887</td>\n",
              "      <td>0.734596</td>\n",
              "      <td>87.312218</td>\n",
              "      <td>0.249258</td>\n",
              "      <td>96.085753</td>\n",
              "      <td>216.662903</td>\n",
              "      <td>0.263637</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.254712</td>\n",
              "      <td>0.085283</td>\n",
              "      <td>0.073905</td>\n",
              "      <td>1032.072127</td>\n",
              "      <td>-0.071600</td>\n",
              "      <td>0.630326</td>\n",
              "      <td>-0.183113</td>\n",
              "      <td>0.039194</td>\n",
              "      <td>0.127836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1251.788316</td>\n",
              "      <td>604.512709</td>\n",
              "      <td>-3.082175</td>\n",
              "      <td>3.061210</td>\n",
              "      <td>566.258455</td>\n",
              "      <td>0.385871</td>\n",
              "      <td>164.440262</td>\n",
              "      <td>0.215055</td>\n",
              "      <td>303.104532</td>\n",
              "      <td>307.281799</td>\n",
              "      <td>0.911545</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.910245</td>\n",
              "      <td>0.085583</td>\n",
              "      <td>0.092613</td>\n",
              "      <td>1157.367409</td>\n",
              "      <td>0.039472</td>\n",
              "      <td>1.081582</td>\n",
              "      <td>-0.949717</td>\n",
              "      <td>0.018913</td>\n",
              "      <td>0.025586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4212</th>\n",
              "      <td>748.899085</td>\n",
              "      <td>201.394726</td>\n",
              "      <td>3.077837</td>\n",
              "      <td>-3.125663</td>\n",
              "      <td>488.959260</td>\n",
              "      <td>1.246388</td>\n",
              "      <td>7.140489</td>\n",
              "      <td>0.767577</td>\n",
              "      <td>148.520666</td>\n",
              "      <td>54.639683</td>\n",
              "      <td>0.050968</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.064142</td>\n",
              "      <td>0.091740</td>\n",
              "      <td>0.115538</td>\n",
              "      <td>1338.392770</td>\n",
              "      <td>0.115320</td>\n",
              "      <td>0.559551</td>\n",
              "      <td>-0.051178</td>\n",
              "      <td>0.020294</td>\n",
              "      <td>0.005071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4213</th>\n",
              "      <td>1595.418427</td>\n",
              "      <td>276.905510</td>\n",
              "      <td>-2.768412</td>\n",
              "      <td>-2.916644</td>\n",
              "      <td>425.321662</td>\n",
              "      <td>2.213353</td>\n",
              "      <td>137.410568</td>\n",
              "      <td>0.447905</td>\n",
              "      <td>170.338762</td>\n",
              "      <td>111.314743</td>\n",
              "      <td>0.700486</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.736542</td>\n",
              "      <td>0.041849</td>\n",
              "      <td>0.112344</td>\n",
              "      <td>891.372274</td>\n",
              "      <td>0.493753</td>\n",
              "      <td>1.789845</td>\n",
              "      <td>0.242789</td>\n",
              "      <td>0.118787</td>\n",
              "      <td>0.071604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4214</th>\n",
              "      <td>958.358525</td>\n",
              "      <td>239.519734</td>\n",
              "      <td>3.077765</td>\n",
              "      <td>2.677275</td>\n",
              "      <td>229.409691</td>\n",
              "      <td>0.436397</td>\n",
              "      <td>9.861922</td>\n",
              "      <td>0.855264</td>\n",
              "      <td>50.609485</td>\n",
              "      <td>193.921387</td>\n",
              "      <td>2.053018</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.178533</td>\n",
              "      <td>0.074463</td>\n",
              "      <td>0.129167</td>\n",
              "      <td>1274.936342</td>\n",
              "      <td>0.275401</td>\n",
              "      <td>0.751691</td>\n",
              "      <td>1.903132</td>\n",
              "      <td>0.020317</td>\n",
              "      <td>0.147797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4215</th>\n",
              "      <td>905.681403</td>\n",
              "      <td>410.041595</td>\n",
              "      <td>-2.741795</td>\n",
              "      <td>-2.756343</td>\n",
              "      <td>353.064677</td>\n",
              "      <td>1.242856</td>\n",
              "      <td>47.823807</td>\n",
              "      <td>0.656382</td>\n",
              "      <td>276.760091</td>\n",
              "      <td>133.371582</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.005690</td>\n",
              "      <td>0.090407</td>\n",
              "      <td>0.107519</td>\n",
              "      <td>1281.699892</td>\n",
              "      <td>0.086676</td>\n",
              "      <td>0.706625</td>\n",
              "      <td>-0.092366</td>\n",
              "      <td>0.127260</td>\n",
              "      <td>0.122629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4216</th>\n",
              "      <td>704.511470</td>\n",
              "      <td>185.553955</td>\n",
              "      <td>-3.019336</td>\n",
              "      <td>-3.127814</td>\n",
              "      <td>308.191461</td>\n",
              "      <td>0.074451</td>\n",
              "      <td>98.377960</td>\n",
              "      <td>0.319823</td>\n",
              "      <td>280.213601</td>\n",
              "      <td>97.835045</td>\n",
              "      <td>0.195836</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.073741</td>\n",
              "      <td>0.057762</td>\n",
              "      <td>0.105692</td>\n",
              "      <td>1015.750881</td>\n",
              "      <td>0.302097</td>\n",
              "      <td>0.693587</td>\n",
              "      <td>-0.375838</td>\n",
              "      <td>0.038916</td>\n",
              "      <td>0.004386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4217 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              Mww  ...  Acoplanaridade_jatos_MET\n",
              "0      741.827618  ...                  0.407977\n",
              "1      877.505354  ...                  0.188523\n",
              "2      781.795723  ...                  0.089541\n",
              "3      650.542403  ...                  0.127836\n",
              "4     1251.788316  ...                  0.025586\n",
              "...           ...  ...                       ...\n",
              "4212   748.899085  ...                  0.005071\n",
              "4213  1595.418427  ...                  0.071604\n",
              "4214   958.358525  ...                  0.147797\n",
              "4215   905.681403  ...                  0.122629\n",
              "4216   704.511470  ...                  0.004386\n",
              "\n",
              "[4217 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzr4jJR0u6h"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q59D_87Q0u6h"
      },
      "source": [
        "test_size = 0.35\n",
        "DataSet_Train1_, DataSet_Test1_ = train_test_split( Dataset_Signal_Back1, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back1.label )\n",
        "DataSet_Train2_, DataSet_Test2_ = train_test_split( Dataset_Signal_Back2, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back2.label )\n",
        "DataSet_Train3_, DataSet_Test3_ = train_test_split( Dataset_Signal_Back3, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back3.label )\n",
        "DataSet_Train4_, DataSet_Test4_ = train_test_split( Dataset_Signal_Back4, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back4.label )\n",
        "DataSet_Train5_, DataSet_Test5_ = train_test_split( Dataset_Signal_Back5, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back5.label )\n",
        "DataSet_Train6_, DataSet_Test6_ = train_test_split( Dataset_Signal_Back6, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back6.label )\n",
        "DataSet_Train7_, DataSet_Test7_ = train_test_split( Dataset_Signal_Back7, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back7.label )\n",
        "DataSet_Train8_, DataSet_Test8_ = train_test_split( Dataset_Signal_Back8, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back8.label )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHieYhdx0u6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "3a5a1e6e-2763-4308-cb40-a08b97fbca54"
      },
      "source": [
        "'''with h5py.File( 'DataSet_Test1_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test1_ )\n",
        "with h5py.File( 'DataSet_Test2_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test2_ )\n",
        "with h5py.File( 'DataSet_Test3_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test3_ ) \n",
        "with h5py.File( 'DataSet_Test4_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test4_ )\n",
        "with h5py.File( 'DataSet_Test5_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test5_ )    \n",
        "with h5py.File( 'DataSet_Test6_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test6_ )\n",
        "with h5py.File( 'DataSet_Test7_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test7_ )\n",
        "with h5py.File( 'DataSet_Test8_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test8_ )    \n",
        "\n",
        "\n",
        "with h5py.File( 'DataSet_Train1_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train1_ )\n",
        "with h5py.File( 'DataSet_Train2_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train2_ )\n",
        "with h5py.File( 'DataSet_Train3_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train3_ ) \n",
        "with h5py.File( 'DataSet_Train4_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train4_ )\n",
        "with h5py.File( 'DataSet_Train5_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train5_ )    \n",
        "with h5py.File( 'DataSet_Train6_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train6_ )\n",
        "with h5py.File( 'DataSet_Train7_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train7_ )\n",
        "with h5py.File( 'DataSet_Train8_.h5', 'w') as f:\n",
        "    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train8_ )    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"with h5py.File( 'DataSet_Test1_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test1_ )\\nwith h5py.File( 'DataSet_Test2_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test2_ )\\nwith h5py.File( 'DataSet_Test3_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test3_ ) \\nwith h5py.File( 'DataSet_Test4_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test4_ )\\nwith h5py.File( 'DataSet_Test5_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test5_ )    \\nwith h5py.File( 'DataSet_Test6_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test6_ )\\nwith h5py.File( 'DataSet_Test7_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test7_ )\\nwith h5py.File( 'DataSet_Test8_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Test8_ )    \\n\\n\\nwith h5py.File( 'DataSet_Train1_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train1_ )\\nwith h5py.File( 'DataSet_Train2_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train2_ )\\nwith h5py.File( 'DataSet_Train3_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train3_ ) \\nwith h5py.File( 'DataSet_Train4_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train4_ )\\nwith h5py.File( 'DataSet_Train5_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train5_ )    \\nwith h5py.File( 'DataSet_Train6_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train6_ )\\nwith h5py.File( 'DataSet_Train7_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train7_ )\\nwith h5py.File( 'DataSet_Train8_.h5', 'w') as f:\\n    DataSet_Test5_ = f.create_dataset( 'dados', data = DataSet_Train8_ )    \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29GkBUp-0u6k"
      },
      "source": [
        "y_train1 = DataSet_Train1_['label']\n",
        "y_test1  = DataSet_Test1_['label']\n",
        "\n",
        "y_train2 = DataSet_Train2_['label']\n",
        "y_test2  = DataSet_Test2_['label']\n",
        "\n",
        "y_train3 = DataSet_Train3_['label']\n",
        "y_test3  = DataSet_Test3_['label']\n",
        "\n",
        "y_train4 = DataSet_Train4_['label']\n",
        "y_test4  = DataSet_Test4_['label']\n",
        "\n",
        "y_train5 = DataSet_Train5_['label']\n",
        "y_test5  = DataSet_Test5_['label']\n",
        "\n",
        "y_train6 = DataSet_Train6_['label']\n",
        "y_test6  = DataSet_Test6_['label']\n",
        "\n",
        "y_train7 = DataSet_Train7_['label']\n",
        "y_test7  = DataSet_Test7_['label']\n",
        "\n",
        "y_train8 = DataSet_Train8_['label']\n",
        "y_test8  = DataSet_Test8_['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlzog6J60u6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a08071-7756-4445-e999-6db2985deb1e"
      },
      "source": [
        "print('--- Weight Anomalo 8 --- \\n')\n",
        "DataSet_Test8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==1]['weight'] \n",
        "DataSet_TestSM8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==2]['weight'] \n",
        "DataSet_Test8_weight_backgr =DataSet_Test8_[DataSet_Test8_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train8_weight_signal = DataSet_Train8_[DataSet_Train8_['label']==1]['weight']\n",
        "DataSet_TrainSM8_weight_signal =  DataSet_Train8_[DataSet_Train8_['label']==2]['weight']\n",
        "DataSet_Train8_weight_backgr = DataSet_Train8_[DataSet_Train8_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test8_weight =  DataSet_Test8_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test8_weight_signal.shape)\n",
        "print( 'Shape Test SM', DataSet_TestSM8_weight_signal.shape)\n",
        "print( 'Shape Test Background', DataSet_Test8_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 7 --- \\n')\n",
        "\n",
        "DataSet_Test7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==1]['weight'] \n",
        "DataSet_TestSM7_weight_signal =DataSet_Test7_[DataSet_Test7_['label']==2]['weight'] \n",
        "DataSet_Test7_weight_backgr =  DataSet_Test7_[DataSet_Test7_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train7_weight_signal =  DataSet_Train7_[DataSet_Train7_['label']==1]['weight']\n",
        "DataSet_TrainSM7_weight_signal =  DataSet_Train7_[DataSet_Train7_['label']==2]['weight']\n",
        "DataSet_Train7_weight_backgr =  DataSet_Train7_[DataSet_Train7_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test7_weight =  DataSet_Test7_['weight']\n",
        "\n",
        "print('--- Weight Anomalo 6 --- \\n')\n",
        "\n",
        "DataSet_Test6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==1]['weight'] \n",
        "DataSet_TestSM6_weight_signal =DataSet_Test6_[DataSet_Test6_['label']==2]['weight'] \n",
        "DataSet_Test6_weight_backgr =  DataSet_Test6_[DataSet_Test6_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train6_weight_signal =  DataSet_Train6_[DataSet_Train6_['label']==1]['weight']\n",
        "DataSet_TrainSM6_weight_signal =  DataSet_Train6_[DataSet_Train6_['label']==2]['weight']\n",
        "DataSet_Train6_weight_backgr =  DataSet_Train6_[DataSet_Train6_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test6_weight =  DataSet_Test6_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test SM', DataSet_TestSM7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 5 --- \\n')\n",
        "\n",
        "DataSet_Test5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==1]['weight'] \n",
        "DataSet_TestSM5_weight_signal =DataSet_Test5_[DataSet_Test5_['label']==2]['weight'] \n",
        "DataSet_Test5_weight_backgr =  DataSet_Test5_[DataSet_Test5_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train5_weight_signal =  DataSet_Train5_[DataSet_Train5_['label']==1]['weight']\n",
        "DataSet_TrainSM5_weight_signal =  DataSet_Train5_[DataSet_Train5_['label']==2]['weight']\n",
        "DataSet_Train5_weight_backgr =  DataSet_Train5_[DataSet_Train5_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test5_weight =  DataSet_Test5_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test SM', DataSet_TestSM7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 1 --- \\n')\n",
        "\n",
        "DataSet_Test1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==1]['weight'] \n",
        "DataSet_TestSM1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==2]['weight'] \n",
        "DataSet_Test1_weight_backgr =  DataSet_Test1_[DataSet_Test1_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train1_weight_signal =  DataSet_Train1_[DataSet_Train1_['label']==1]['weight']\n",
        "DataSet_TrainSM1_weight_signal =  DataSet_Train1_[DataSet_Train1_['label']==2]['weight']\n",
        "DataSet_Train1_weight_backgr = DataSet_Train1_[DataSet_Train1_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test1_weight =  DataSet_Test1_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test1_weight_signal.shape)\n",
        "print('Shape Test SM',  DataSet_TestSM1_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test1_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 2 --- \\n')\n",
        "\n",
        "DataSet_Test2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==1]['weight'] \n",
        "DataSet_TestSM2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==2]['weight'] \n",
        "DataSet_Test2_weight_backgr =  DataSet_Test2_[DataSet_Test2_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train2_weight_signal =  DataSet_Train2_[DataSet_Train2_['label']==1]['weight']\n",
        "DataSet_TrainSM2_weight_signal =  DataSet_Train2_[DataSet_Train2_['label']==2]['weight']\n",
        "DataSet_Train2_weight_backgr = DataSet_Train2_[DataSet_Train2_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test2_weight =  DataSet_Test2_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test2_weight_signal.shape)\n",
        "print('Shape Test SM',  DataSet_TestSM2_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test2_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 3 --- \\n')\n",
        "\n",
        "DataSet_Test3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==1]['weight'] \n",
        "DataSet_TestSM3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==2]['weight'] \n",
        "DataSet_Test3_weight_backgr =  DataSet_Test3_[DataSet_Test3_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train3_weight_signal =  DataSet_Train3_[DataSet_Train3_['label']==1]['weight']\n",
        "DataSet_TrainSM3_weight_signal =  DataSet_Train3_[DataSet_Train3_['label']==2]['weight']\n",
        "DataSet_Train3_weight_backgr = DataSet_Train3_[DataSet_Train3_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test3_weight =  DataSet_Test2_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test3_weight_signal.shape)\n",
        "print('Shape Test SM',  DataSet_TestSM3_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test3_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 4 --- \\n')\n",
        "\n",
        "DataSet_Test4_weight_signal = DataSet_Test4_[DataSet_Test4_['label']==1]['weight'] \n",
        "DataSet_TestSM4_weight_signal =  DataSet_Test4_[DataSet_Test4_['label']==2]['weight'] \n",
        "DataSet_Test4_weight_backgr = DataSet_Test4_[DataSet_Test4_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Train4_weight_signal = DataSet_Train4_[DataSet_Train4_['label']==1]['weight']\n",
        "DataSet_TrainSM4_weight_signal =  DataSet_Train4_[DataSet_Train4_['label']==2]['weight']\n",
        "DataSet_Train4_weight_backgr = DataSet_Train4_[DataSet_Train4_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test4_weight =  DataSet_Test4_['weight']\n",
        "\n",
        "print( 'Shape Test SM' , DataSet_Test4_weight_signal.shape)\n",
        "print( 'Shape Test SM', DataSet_TestSM4_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test4_weight_backgr.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Weight Anomalo 8 --- \n",
            "\n",
            "Shape Test Signal (570,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 7 --- \n",
            "\n",
            "--- Weight Anomalo 6 --- \n",
            "\n",
            "Shape Test Signal (235,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146072,) \n",
            "\n",
            "--- Weight Anomalo 5 --- \n",
            "\n",
            "Shape Test Signal (235,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146072,) \n",
            "\n",
            "--- Weight Anomalo 1 --- \n",
            "\n",
            "Shape Test Signal (541,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 2 --- \n",
            "\n",
            "Shape Test Signal (61,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 3 --- \n",
            "\n",
            "Shape Test Signal (152,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 4 --- \n",
            "\n",
            "Shape Test SM (260,)\n",
            "Shape Test SM (39,)\n",
            "Shape Test Background (146071,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh9mGF8W0u6m"
      },
      "source": [
        "\n",
        "DataSet_Train8 = DataSet_Train8_[select_columns] \n",
        "DataSet_Test8 = DataSet_Test8_[select_columns] \n",
        "\n",
        "DataSet_Train7 = DataSet_Train7_[select_columns] \n",
        "DataSet_Test7 = DataSet_Test7_[select_columns] \n",
        "\n",
        "DataSet_Train6 = DataSet_Train6_[select_columns] \n",
        "DataSet_Test6 = DataSet_Test6_[select_columns] \n",
        "\n",
        "DataSet_Train5 = DataSet_Train5_[select_columns] \n",
        "DataSet_Test5 = DataSet_Test5_[select_columns] \n",
        "\n",
        "DataSet_Train1 = DataSet_Train1_[select_columns] \n",
        "DataSet_Test1 = DataSet_Test1_[select_columns] \n",
        "\n",
        "DataSet_Train2 = DataSet_Train2_[select_columns] \n",
        "DataSet_Test2 = DataSet_Test2_[select_columns] \n",
        "\n",
        "DataSet_Train3 = DataSet_Train3_[select_columns] \n",
        "DataSet_Test3 = DataSet_Test3_[select_columns] \n",
        "\n",
        "DataSet_Train4 = DataSet_Train4_[select_columns] \n",
        "DataSet_Test4 = DataSet_Test4_[select_columns] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDimCTZ2n4Uq"
      },
      "source": [
        "scaler1 = StandardScaler()\n",
        "X_train_scaled1 = scaler1.fit_transform( DataSet_Train1 )\n",
        "X_test_scaled1 = scaler1.transform( DataSet_Test1 )\n",
        "\n",
        "\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled2 = scaler2.fit_transform( DataSet_Train2 )\n",
        "X_test_scaled2 = scaler2.transform( DataSet_Test2 )\n",
        "\n",
        "scaler3 = StandardScaler()\n",
        "X_train_scaled3 = scaler3.fit_transform( DataSet_Train3 )\n",
        "X_test_scaled3 = scaler3.transform( DataSet_Test3 )\n",
        "\n",
        "scaler4 = StandardScaler()\n",
        "X_train_scaled4 = scaler4.fit_transform( DataSet_Train4 )\n",
        "X_test_scaled4 = scaler4.transform( DataSet_Test4 )\n",
        "\n",
        "scaler5 = StandardScaler()\n",
        "X_train_scaled5 = scaler5.fit_transform( DataSet_Train5 )\n",
        "X_test_scaled5 = scaler5.transform( DataSet_Test5 )\n",
        "\n",
        "scaler6 = StandardScaler()\n",
        "X_train_scaled6 = scaler6.fit_transform( DataSet_Train6 )\n",
        "X_test_scaled6 = scaler6.transform( DataSet_Test6 )\n",
        "\n",
        "scaler7 = StandardScaler()\n",
        "X_train_scaled7 = scaler7.fit_transform( DataSet_Train7 )\n",
        "X_test_scaled7 = scaler7.transform( DataSet_Test7 )\n",
        "\n",
        "scaler8 = StandardScaler()\n",
        "X_train_scaled8 = scaler8.fit_transform( DataSet_Train8 )\n",
        "X_test_scaled8 = scaler8.transform( DataSet_Test8 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeSnvrJF0u6p"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9-da5_kvczw"
      },
      "source": [
        "n_iter = 150\n",
        "cv = 2\n",
        "scoring = 'f1_weighted'\n",
        "param_grid = {\n",
        "    'learning_rate': [0.008, 0.0004,  1],\n",
        "    'depth': [0,5,15],\n",
        "    'iterations':[ 600, 1000],\n",
        "    'l2_leaf_reg':[ 6, 50],\n",
        "    'border_count':[20, 150]\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUbuU49m0u6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b08f258-fef3-489e-f720-6f48008cb690"
      },
      "source": [
        "\n",
        "print('Escolhendo...')\n",
        "#mdl8_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train8_weight_backgr.sum(), 1: DataSet_Train8_weight_signal.sum(), 2:DataSet_TrainSM8_weight_signal.sum() } )\n",
        "mdl8_binary_logloss_scale_pos_weight =  CatBoostClassifier( loss_function='MultiClass'  )\n",
        "search8_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl8_binary_logloss_scale_pos_weight, param_grid=param_grid ,cv=cv, verbose=3, n_jobs=-1  )\n",
        "search_result8_binary_logloss_scale_pos_weight = search8_binary_logloss_scale_pos_weight.fit( X_train_scaled8, y_train8 )\n",
        "print(\"Best: %f using %s\" % ( search_result8_binary_logloss_scale_pos_weight.best_score_, search_result8_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result8_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO8_DataDriven.joblib') \n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl6_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train6_weight_backgr.sum(), 1: DataSet_Train6_weight_signal.sum(), 2:DataSet_TrainSM6_weight_signal.sum() } )\n",
        "mdl6_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass')\n",
        "time_s_ = time.time()\n",
        "search6_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl6_binary_logloss_scale_pos_weight, param_grid=param_grid ,cv=cv, verbose=3, n_jobs=-1   )\n",
        "search_result6_binary_logloss_scale_pos_weight = search6_binary_logloss_scale_pos_weight.fit( X_train_scaled6, y_train6 )\n",
        "print(\"Best: %f using %s\" % ( search_result6_binary_logloss_scale_pos_weight.best_score_, search_result6_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result6_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO6_DataDriven.joblib') \n",
        "time_e_ = time.time()\n",
        "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl7_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train7_weight_backgr.sum(), 1: DataSet_Train7_weight_signal.sum(), 2:DataSet_TrainSM7_weight_signal.sum() } )\n",
        "mdl7_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass' )\n",
        "time_s_ = time.time()\n",
        "search7_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl7_binary_logloss_scale_pos_weight, param_grid=param_grid,cv=cv, verbose=3, n_jobs=-1  )\n",
        "search_result7_binary_logloss_scale_pos_weight = search7_binary_logloss_scale_pos_weight.fit( X_train_scaled7, y_train7 )\n",
        "print(\"Best: %f using %s\" % ( search_result7_binary_logloss_scale_pos_weight.best_score_, search_result7_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result7_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO7_DataDriven.joblib') \n",
        "time_e_ = time.time()\n",
        "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl5_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train5_weight_backgr.sum(), 1: DataSet_Train5_weight_signal.sum(), 2:DataSet_TrainSM5_weight_signal.sum() } )\n",
        "mdl5_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass')\n",
        "time_s_ = time.time()\n",
        "search5_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl5_binary_logloss_scale_pos_weight, param_grid=param_grid ,cv=cv, verbose=3, n_jobs=-1  )\n",
        "search_result5_binary_logloss_scale_pos_weight = search5_binary_logloss_scale_pos_weight.fit( X_train_scaled5, y_train5 )\n",
        "print(\"Best: %f using %s\" % ( search_result5_binary_logloss_scale_pos_weight.best_score_, search_result5_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result5_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO5_DataDriven.joblib') \n",
        "time_e_ = time.time()\n",
        "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl1_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train1_weight_backgr.sum(), 1: DataSet_Train1_weight_signal.sum(), 2:DataSet_TrainSM1_weight_signal.sum() } )\n",
        "mdl1_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass' )\n",
        "time_s_ = time.time()\n",
        "search1_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl1_binary_logloss_scale_pos_weight, param_grid=param_grid ,cv=cv, verbose=3, n_jobs=-1   )\n",
        "search_result1_binary_logloss_scale_pos_weight = search1_binary_logloss_scale_pos_weight.fit( X_train_scaled1, y_train1 )\n",
        "print(\"Best: %f using %s\" % ( search_result1_binary_logloss_scale_pos_weight.best_score_, search_result1_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result1_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO1_DataDriven.joblib') \n",
        "time_e_ = time.time()\n",
        "print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl2_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train2_weight_backgr.sum(), 1: DataSet_Train2_weight_signal.sum(), 2:DataSet_TrainSM2_weight_signal.sum() } )\n",
        "mdl2_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass' )\n",
        "search2_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl2_binary_logloss_scale_pos_weight, param_grid=param_grid,cv=cv, verbose=3, n_jobs=-1  )\n",
        "search_result2_binary_logloss_scale_pos_weight = search2_binary_logloss_scale_pos_weight.fit( X_train_scaled2, y_train2 )\n",
        "print(\"Best: %f using %s\" % ( search_result2_binary_logloss_scale_pos_weight.best_score_, search_result2_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result2_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO2_DataDriven.joblib') \n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl3_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train3_weight_backgr.sum(), 1: DataSet_Train3_weight_signal.sum(), 2:DataSet_TrainSM3_weight_signal.sum() } )\n",
        "mdl3_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass' )\n",
        "search3_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl3_binary_logloss_scale_pos_weight, param_grid=param_grid ,cv=cv, verbose=3, n_jobs=-1  )\n",
        "search_result3_binary_logloss_scale_pos_weight = search3_binary_logloss_scale_pos_weight.fit( X_train_scaled3, y_train3 )\n",
        "print(\"Best: %f using %s\" % ( search_result3_binary_logloss_scale_pos_weight.best_score_, search_result3_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result3_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO3_DataDriven.joblib') \n",
        "\n",
        "print('Escolhendo...')\n",
        "#mdl4_binary_logloss_scale_pos_weight =  LGBMClassifier( class_weight = { 0: DataSet_Train4_weight_backgr.sum(), 1: DataSet_Train4_weight_signal.sum(), 2:DataSet_TrainSM4_weight_signal.sum() } )\n",
        "mdl4_binary_logloss_scale_pos_weight = CatBoostClassifier( loss_function='MultiClass' )\n",
        "search4_binary_logloss_scale_pos_weight = GridSearchCV( estimator = mdl4_binary_logloss_scale_pos_weight,param_grid=param_grid ,cv=cv, verbose=3 , n_jobs=-1  )\n",
        "search_result4_binary_logloss_scale_pos_weight = search4_binary_logloss_scale_pos_weight.fit( X_train_scaled4, y_train4 )\n",
        "print(\"Best: %f using %s\" % ( search_result4_binary_logloss_scale_pos_weight.best_score_, search_result4_binary_logloss_scale_pos_weight.best_params_ ) )\n",
        "print('\\n')\n",
        "dump(search_result4_binary_logloss_scale_pos_weight,PATH + 'CatBoost_Multiclass_ANOMALO4_DataDriven.joblib') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Escolhendo...\n",
            "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  8.9min\n",
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQDzffm9ZHag"
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "def result( X_test, Y_test, X_train, Y_train, model, label, verbose = 1 ):\n",
        "    print(' ---------------- ' + label + '----------------' )\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_test = scaler.transform(X_test)\n",
        "    X_scaled_train = scaler.fit_transform(X_train)\n",
        "    \n",
        "    predict_proba_test = model.predict_proba(X_scaled_test)  # calculate the probability\n",
        "    predict_proba_train = model.predict_proba(X_scaled_train)  # calculate the probability\n",
        "\n",
        "    label_test = label_binarize( Y_test, classes = [ *range(3) ] )\n",
        "    precision_test = dict()\n",
        "    recall_test = dict()\n",
        "    threshs_test = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision_test[i], recall_test[i], threshs_test[i] = precision_recall_curve( label_test[:,i], predict_proba_test[:,i] )\n",
        "\n",
        "    thresholds_test = np.concatenate( [ threshs_test[1], [1] ], axis = 0 )\n",
        "    bidx_test = np.argmax( precision_test[1] * recall_test[1] )\n",
        "    best_cut_test = threshs_test[1][bidx_test]\n",
        "    print('best cut test--> ', best_cut_test)\n",
        "    y_preds_test = [ np.argmax(line) for line in predict_proba_test > best_cut_test ]\n",
        "\n",
        "    label_train = label_binarize( Y_train, classes = [ *range(3) ] )\n",
        "    precision_train = dict()\n",
        "    recall_train = dict()\n",
        "    threshs_train = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision_train[i], recall_train[i], threshs_train[i] = precision_recall_curve( label_train[:,i], predict_proba_train[:,i] )\n",
        "\n",
        "    thresholds_train = np.concatenate( [ threshs_train[1], [1] ], axis = 0 )\n",
        "    bidx_train = np.argmax( precision_train[1] * recall_train[1] )\n",
        "    best_cut_train = threshs_test[1][bidx_train]\n",
        "    print('best cut train -->', best_cut_train)\n",
        "    y_preds_train = [ np.argmax(line) for line in predict_proba_train > best_cut_train ]    \n",
        "\n",
        "    if verbose == 1:\n",
        "        print(\"Purity in test sample     : {:2.2f}%\".format(100 * precision_score(Y_test, y_preds_test, average = 'weighted')))\n",
        "        print(\"Efficiency in test sample : {:2.2f}%\".format(100 * recall_score(Y_test, y_preds_test, average = 'weighted')))\n",
        "        print( \"F1_score in test sample  : {:2.2f}%\".format(100 * f1_score(Y_test, y_preds_test, average = 'weighted')))\n",
        "        print('\\n')\n",
        "        print(\"Purity in train sample     : {:2.2f}%\".format(100 * precision_score(Y_train, y_preds_train, average = 'weighted')))\n",
        "        print(\"Efficiency in train sample : {:2.2f}%\".format(100 * recall_score(Y_train, y_preds_train, average = 'weighted')))\n",
        "        print( \"F1_score in train sample  : {:2.2f}%\".format(100 * f1_score(Y_train, y_preds_train, average = 'weighted')))    \n",
        "\n",
        "    return predict_proba_test[:,1], best_cut_test        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-FJwewK728G"
      },
      "source": [
        "predict_proba1, best_cut1 = result(DataSet_Test1, y_test1, DataSet_Train1, y_train1, search_result1_binary_logloss_scale_pos_weight, 'ANOMALO1' )\n",
        "predict_proba2, best_cut2 = result(DataSet_Test2, y_test2, DataSet_Train2, y_train2, search_result2_binary_logloss_scale_pos_weight, 'ANOMALO2' )\n",
        "predict_proba3, best_cut3 = result(DataSet_Test3, y_test3, DataSet_Train3, y_train3, search_result3_binary_logloss_scale_pos_weight, 'ANOMALO3' )\n",
        "predict_proba4, best_cut4 = result(DataSet_Test4, y_test4, DataSet_Train4, y_train4, search_result4_binary_logloss_scale_pos_weight, 'ANOMALO4' )\n",
        "predict_proba5, best_cut5 = result(DataSet_Test5, y_test5, DataSet_Train5, y_train5, search_result5_binary_logloss_scale_pos_weight, 'ANOMALO5' )\n",
        "predict_proba6, best_cut6 = result(DataSet_Test6, y_test6, DataSet_Train6, y_train6, search_result6_binary_logloss_scale_pos_weight, 'ANOMALO6' )\n",
        "predict_proba7, best_cut7 = result(DataSet_Test7, y_test7, DataSet_Train7, y_train7, search_result7_binary_logloss_scale_pos_weight, 'ANOMALO7' )\n",
        "predict_proba8, best_cut8 = result(DataSet_Test8, y_test8, DataSet_Train8, y_train8, search_result8_binary_logloss_scale_pos_weight, 'ANOMALO8' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmCcbXNGv_k"
      },
      "source": [
        "sys.exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEamxHa0u6r"
      },
      "source": [
        "'''search1_binary_logloss_scale_pos_weight = {'subsample': 0.7, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.008, 'min_child_samples': 90, 'metric': 'multi_logloss', 'max_depth': 150, 'learning_rate': 0.05, 'feature_pre_filter': 'False', 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "search2_binary_logloss_scale_pos_weight = {'subsample': 1, 'objective': 'multiclass', 'num_leaves': 100, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.5, 'min_child_samples': 90, 'metric': 'multi_logloss', 'max_depth': 150, 'learning_rate': 0.05, 'feature_pre_filter': 'False', 'colsample_bytree': 1, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "search3_binary_logloss_scale_pos_weight = {'subsample': 0.1, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.5, 'min_child_samples': 190, 'metric': 'multi_logloss', 'max_depth': 20, 'learning_rate': 0.008, 'feature_pre_filter': 'False', 'colsample_bytree': 1, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "\n",
        "search4_binary_logloss_scale_pos_weight = {'subsample': 0.7, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.5, 'min_child_samples': 190, 'metric': 'multi_logloss', 'max_depth': 400, 'learning_rate': 0.05, 'feature_pre_filter': 'False', 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "search5_binary_logloss_scale_pos_weight = {'subsample': 0.7, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.5, 'min_child_samples': 20, 'metric': 'multi_logloss', 'max_depth': 1, 'learning_rate': 0.1, 'feature_pre_filter': 'False', 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "\n",
        "search6_binary_logloss_scale_pos_weight = {'subsample': 1, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.5, 'min_child_samples': 190, 'metric': 'multi_logloss', 'max_depth': 400, 'learning_rate': 0.008, 'feature_pre_filter': 'False', 'colsample_bytree': 1, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "search7_binary_logloss_scale_pos_weight = {'subsample': 0.7, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.001, 'min_child_samples': 190, 'metric': 'multi_logloss', 'max_depth': 150, 'learning_rate': 0.05, 'feature_pre_filter': 'False', 'colsample_bytree': 1, 'boosting_type': 'gbdt'}\n",
        "\n",
        "\n",
        "\n",
        "search8_binary_logloss_scale_pos_weight = {'subsample': 0.7, 'objective': 'multiclass', 'num_leaves': 50, 'num_class': 3, 'n_estimators': 500, 'min_child_weight': 0.008, 'min_child_samples': 90, 'metric': 'multi_logloss', 'max_depth': 150, 'learning_rate': 0.05, 'feature_pre_filter': 'False', 'colsample_bytree': 0.5, 'boosting_type': 'gbdt'}\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEY2B1b0u6s"
      },
      "source": [
        "print('Treinando os modelos...\\n')\n",
        "\n",
        "bst8_binary_logloss_scale_pos_weight = lgb.train(search_result8_binary_logloss_scale_pos_weight.best_params_  , train_LGB8)\n",
        "bst7_binary_logloss_scale_pos_weight = lgb.train(search_result7_binary_logloss_scale_pos_weight.best_params_  , train_LGB7)\n",
        "bst6_binary_logloss_scale_pos_weight = lgb.train(search_result6_binary_logloss_scale_pos_weight.best_params_  , train_LGB6)\n",
        "bst5_binary_logloss_scale_pos_weight = lgb.train(search_result5_binary_logloss_scale_pos_weight.best_params_ , train_LGB5)\n",
        "\n",
        "bst1_binary_logloss_scale_pos_weight = lgb.train(search_result1_binary_logloss_scale_pos_weight.best_params_ , train_LGB1)\n",
        "bst2_binary_logloss_scale_pos_weight = lgb.train(search_result2_binary_logloss_scale_pos_weight.best_params_  , train_LGB2)\n",
        "bst3_binary_logloss_scale_pos_weight = lgb.train(search_result3_binary_logloss_scale_pos_weight.best_params_ , train_LGB3)\n",
        "bst4_binary_logloss_scale_pos_weight = lgb.train(search_result4_binary_logloss_scale_pos_weight.best_params_  , train_LGB4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JisiOxSX0u6s"
      },
      "source": [
        "'''from joblib import dump, load\n",
        "dump( bst8_binary_logloss_scale_pos_weight, \"bst8_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst7_binary_logloss_scale_pos_weight, \"bst7_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst6_binary_logloss_scale_pos_weight, \"bst6_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst5_binary_logloss_scale_pos_weight, \"bst5_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst4_binary_logloss_scale_pos_weight, \"bst4_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst3_binary_logloss_scale_pos_weight, \"bst3_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst2_binary_logloss_scale_pos_weight, \"bst2_binary_logloss_scale_pos_weight.joblib\" )\n",
        "dump( bst1_binary_logloss_scale_pos_weight, \"bst1_binary_logloss_scale_pos_weight.joblib\" )'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smlMzqXc0u6t"
      },
      "source": [
        "predict_proba_LGBM8_binary_logloss_scale_pos_weight = bst8_binary_logloss_scale_pos_weight.predict(DataSet_Test8)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_LGBM8_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM7_binary_logloss_scale_pos_weight = bst7_binary_logloss_scale_pos_weight.predict(DataSet_Test7)\n",
        "print('Predict Proba Anomalo 7 --> \\n', predict_proba_LGBM7_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM6_binary_logloss_scale_pos_weight = bst6_binary_logloss_scale_pos_weight.predict(DataSet_Test6)\n",
        "print('Predict Proba Anomalo 6 --> \\n', predict_proba_LGBM6_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM5_binary_logloss_scale_pos_weight = bst5_binary_logloss_scale_pos_weight.predict(DataSet_Test5)\n",
        "print('Predict Proba Anomalo 5 --> \\n', predict_proba_LGBM5_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM1_binary_logloss_scale_pos_weight = bst1_binary_logloss_scale_pos_weight.predict(DataSet_Test1)\n",
        "print('Predict Proba Anomalo 1 --> \\n', predict_proba_LGBM1_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM2_binary_logloss_scale_pos_weight = bst2_binary_logloss_scale_pos_weight.predict(DataSet_Test2)\n",
        "print('Predict Proba Anomalo 2 --> \\n', predict_proba_LGBM2_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM3_binary_logloss_scale_pos_weight = bst3_binary_logloss_scale_pos_weight.predict(DataSet_Test3)\n",
        "print('Predict Proba Anomalo 3 --> \\n', predict_proba_LGBM3_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM4_binary_logloss_scale_pos_weight = bst4_binary_logloss_scale_pos_weight.predict(DataSet_Test4)\n",
        "print('Predict Proba Anomalo 4 --> \\n', predict_proba_LGBM4_binary_logloss_scale_pos_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIQlbbnk0u6t"
      },
      "source": [
        "from sklearn.preprocessing import label_binarize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujW9Da3a0u6u"
      },
      "source": [
        "def plot_precision_recall_curve( label_test, predicit, title, nome ):\n",
        "    label_test = label_binarize( label_test, classes = [*range(3) ] )\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    threshs = dict()\n",
        "    fontsize = 18\n",
        "\n",
        "    fig, axes = plt.subplots( 1, 2, figsize=(19,10) )\n",
        "\n",
        "    for i in range(3):\n",
        "        precision[i], recall[i], threshs[i] = precision_recall_curve( label_test[:,i], predicit[:, i])\n",
        "        axes[0].plot( recall[i], precision[i], lw = 2, label = ' class {}'.format(i) )\n",
        "    axes[0].set_xlabel( \"Recall\", fontsize = fontsize )\n",
        "    axes[0].set_ylabel( \"Precision\", fontsize = fontsize )\n",
        "    axes[0].legend( loc = \"best\", fontsize = 14 )\n",
        "    axes[0].set_title( title)\n",
        "\n",
        "    thresholds = np.concatenate( [ threshs[1], [1] ], axis = 0 )\n",
        "    bidx = np.argmax( precision[1] * recall[1] )\n",
        "    best_cut = threshs[1][bidx] \n",
        "    print( '\\n',' Best Cut ', best_cut,'\\n' )\n",
        "\n",
        "    axes[1].plot( thresholds , precision[1] * recall[1],  color = 'blue' )\n",
        "    axes[1].plot( [ best_cut , best_cut ] , [-0.1,(precision[1] * recall[1]).max()*2] ,\"-.r\",label='Best Cut : {:2.5f}'.format(best_cut) )\n",
        "    axes[1].plot( [-0.1,1.1] , [ ( precision[1]*recall[1] ).max(),( precision[1]*recall[1] ).max() ], \":g\",label=r'Precision $\\times$ Recall : {:2.5f}'.format( (precision[1]*recall[1]).max() ) )\n",
        "    axes[1].set_ylabel( r'Precision $\\times$ Recall', fontsize = fontsize )\n",
        "    axes[1].set_xlabel(    'Thresholds', fontsize = fontsize    )\n",
        "    axes[1].set_ylim( -0.1 , 1 )\n",
        "    axes[1].set_xlim( -0.01 , 1 )\n",
        "    axes[1].set_title( title )\n",
        "    axes[1].legend( loc = \"best\", fontsize = 14)  \n",
        "    #plt.savefig('/afs/cern.ch/user/m/matheus/WWCEP_multiclass_ML/output_multiclass/' + title + '.jpg')\n",
        "    #plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6liwjHSX0u6u"
      },
      "source": [
        "def best_cut( label_test, predicit ):\n",
        "    label_test = label_binarize( label_test, classes = [ *range(3) ] )\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    threshs = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision[i], recall[i], threshs[i] = precision_recall_curve( label_test[:,i], predicit[:, i] )\n",
        "\n",
        "    thresholds = np.concatenate( [ threshs[1], [1] ], axis = 0 )\n",
        "    bidx = np.argmax( precision[1] * recall[1] )\n",
        "    return threshs[1][bidx]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs7Ck07G0u6v"
      },
      "source": [
        "plot_precision_recall_curve(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo8' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwcGRl760u6v"
      },
      "source": [
        "plot_precision_recall_curve(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo7' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhCQtvqp0u6w"
      },
      "source": [
        "plot_precision_recall_curve(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 1.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo6' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVvkynwu0u6w"
      },
      "source": [
        "plot_precision_recall_curve(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 0.5 \\times 10^{-6} $', 'Precision_Recal_Anomalo5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhMX5okk0u60"
      },
      "source": [
        "plot_precision_recall_curve(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-5} $', 'Precision_Recal_Anomalo1' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSFWQZni0u61"
      },
      "source": [
        "plot_precision_recall_curve(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo2' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-oLEXgG0u62"
      },
      "source": [
        "plot_precision_recall_curve(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 8.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo4' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8fFMk8U0u63"
      },
      "source": [
        "plot_precision_recall_curve(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', 'Precision_Recal_Anomalo3' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG3Pq5mo0u66"
      },
      "source": [
        "select_columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf71QHUR0u67"
      },
      "source": [
        "#argmax() method \n",
        "y_pred_1 = [np.argmax(line) for line in predict_proba_LGBM1_binary_logloss_scale_pos_weight > best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )]\n",
        "y_pred_2 = [np.argmax(line) for line in predict_proba_LGBM2_binary_logloss_scale_pos_weight > best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )]\n",
        "y_pred_3 = [np.argmax(line) for line in predict_proba_LGBM3_binary_logloss_scale_pos_weight > best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )]\n",
        "y_pred_4 = [np.argmax(line) for line in predict_proba_LGBM4_binary_logloss_scale_pos_weight > best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )]\n",
        "y_pred_5 = [np.argmax(line) for line in predict_proba_LGBM5_binary_logloss_scale_pos_weight > best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )]\n",
        "y_pred_6 = [np.argmax(line) for line in predict_proba_LGBM6_binary_logloss_scale_pos_weight > best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )]\n",
        "y_pred_7 = [np.argmax(line) for line in predict_proba_LGBM7_binary_logloss_scale_pos_weight > best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )]\n",
        "y_pred_8 = [np.argmax(line) for line in predict_proba_LGBM8_binary_logloss_scale_pos_weight > best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )]\n",
        "\n",
        "from sklearn.metrics import recall_score,classification_report, precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pFFPti3iSyF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ2gYlw50u67"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst4_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test4_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', weights = DataSet_Test4_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM4_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight),best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 0 ][:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 1 ][:,1] > best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 2 ][:,1] > best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut4 = predict_dados[ predict_dados >= best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(\" --------------- Anomalo 4 --------------- \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test4_weight_signal[ n_events_signal_after_cut4].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test1_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=20 \\times 10^{-6}$', weights = DataSet_Test1_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM1_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight),best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut1 = predict_dados[ predict_dados >= best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' --------------- Anomalo 1 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test1_weight_signal[ n_events_signal_after_cut1].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut1 ) )\n",
        "print('Recall Score',recall_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst3_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test3_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test3_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM3_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight),best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 0 ][:,1] >= best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 1 ][:,1] >= best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 2 ][:,1] >= best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut3 = predict_dados[ predict_dados >= best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' --------------- Anomalo 3 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test3_weight_signal[ n_events_signal_after_cut3].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut3 ) )\n",
        "print('Recall Score',recall_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst2_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test2_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test2_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM2_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight),best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 0 ][:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 1 ][:,1] > best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 2 ][:,1] > best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut2 = predict_dados[ predict_dados > best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' --------------- Anomalo 2 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut2 ) )\n",
        "print('Recall Score',recall_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2LOCwNG0u68"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst5_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test5_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=0.5 \\times 10^{-6}$', weights = DataSet_Test5_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM5_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight),best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 0 ][:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 1 ][:,1] > best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 2 ][:,1] > best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut5 = predict_dados[ predict_dados >= best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(\"Anomalo 5 \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test5_weight_backgr[ n_events_back_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test5_weight_signal[ n_events_signal_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM5_weight_signal[ n_events_SM_after_cut5 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst6_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test6_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=1.0 \\times 10^{-5}$', weights = DataSet_Test6_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM6_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight),best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 0 ][:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 1 ][:,1] > best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 2 ][:,1] > best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut6 = predict_dados[ predict_dados >= best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 6')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test6_weight_backgr[ n_events_back_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test6_weight_signal[ n_events_signal_after_cut6].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM6_weight_signal[ n_events_SM_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut6 ) )\n",
        "print('Recall Score',recall_score(y_test6,y_pred_6, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test6,y_pred_6, average = 'weighted') ) \n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst7_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test7_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test7_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM7_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight),best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 0 ][:,1] >= best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 1 ][:,1] >= best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 2 ][:,1] >= best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut7 = predict_dados[ predict_dados >= best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 7')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test7_weight_backgr[ n_events_back_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test7_weight_signal[ n_events_signal_after_cut7].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM7_weight_signal[ n_events_SM_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut7 ) )\n",
        "print('Recall Score',recall_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test8_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test8_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM8_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight),best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut8 = predict_dados[ predict_dados > best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 8')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test8_weight_backgr[ n_events_back_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test8_weight_signal[ n_events_signal_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM8_weight_signal[ n_events_SM_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut8 ) )\n",
        "print('Recall Score',recall_score(y_test8,y_pred_8, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test8,y_pred_8, average = 'weighted') )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgLjLZ6Mqwy5"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = model_ANOMALO4.predict(data_set_dados_multirp)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_ANOMALO4[ y_test4 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test4_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_ANOMALO4[ y_test4 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', weights = DataSet_Test4_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_ANOMALO4[ y_test4 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM4_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut_ANOMALO4,best_cut_ANOMALO4],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO4 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO4 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut4 = predict_ANOMALO4[ y_test4 == 0 ][:,1] > best_cut_ANOMALO4 \n",
        "n_events_signal_after_cut4 = predict_ANOMALO4[ y_test4 == 1 ][:,1] > best_cut_ANOMALO4\n",
        "n_events_SM_after_cut4 = predict_ANOMALO4[ y_test4 == 2 ][:,1] > best_cut_ANOMALO4\n",
        "n_eventos_Data_after_cut4 = predict_dados[ predict_dados >= best_cut_ANOMALO4]\n",
        "\n",
        "print(\" --------------- Anomalo 4 --------------- \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test4_weight_signal[ n_events_signal_after_cut4].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size )\n",
        "\n",
        "\n",
        "ax[0,0].text(best_cut_ANOMALO4+0.05 , 0.1, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = model_ANOMALO1.predict(data_set_dados_multirp)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_ANOMALO1[ y_test1 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Drive Background', weights = DataSet_Test1_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_ANOMALO1[ y_test1 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=20 \\times 10^{-6}$', weights = DataSet_Test1_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_ANOMALO1[ y_test1 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM1_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut_ANOMALO1,best_cut_ANOMALO1],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO1 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO1] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut1 = predict_ANOMALO1[ y_test1 == 0 ][:,1] > best_cut_ANOMALO1\n",
        "n_events_signal_after_cut1 = predict_ANOMALO1[ y_test1 == 1 ][:,1] > best_cut_ANOMALO1\n",
        "n_events_SM_after_cut1 = predict_ANOMALO1[ y_test1 == 2 ][:,1] > best_cut_ANOMALO1\n",
        "n_eventos_Data_after_cut1 = predict_dados[ predict_dados >= best_cut_ANOMALO1 ]\n",
        "\n",
        "print(' --------------- Anomalo 1 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test1_weight_signal[ n_events_signal_after_cut1].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut1 ) )\n",
        "\n",
        "\n",
        "ax[0,1].text(best_cut_ANOMALO1 + 0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = model_ANOMALO3.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_ANOMALO3[ y_test3 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test3_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_ANOMALO3[ y_test3 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test3_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_ANOMALO3[ y_test3 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM3_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut_ANOMALO3,best_cut_ANOMALO3],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO3 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO3 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Dados-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut3 = predict_ANOMALO3[ y_test3 == 0 ][:,1] >= best_cut_ANOMALO3\n",
        "n_events_signal_after_cut3 = predict_ANOMALO3[ y_test3 == 1 ][:,1] >= best_cut_ANOMALO3\n",
        "n_events_SM_after_cut3 = predict_ANOMALO3[ y_test3 == 2 ][:,1] >= best_cut_ANOMALO3\n",
        "n_eventos_Data_after_cut3 = predict_dados[ predict_dados >= best_cut_ANOMALO3 ]\n",
        "\n",
        "print(' --------------- Anomalo 3 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test3_weight_signal[ n_events_signal_after_cut3].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut3 ) )\n",
        "\n",
        "\n",
        "ax[1,0].text(best_cut_ANOMALO3+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = model_ANOMALO2.predict(data_set_dados_multirp)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_ANOMALO2[ y_test2 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test2_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_ANOMALO2[ y_test2 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test2_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_ANOMALO2[ y_test2 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM2_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test2,predict_ANOMALO2),best_cut_ANOMALO2],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO2 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO2 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut2 = predict_ANOMALO2[ y_test2 == 0 ][:,1] > best_cut_ANOMALO2 \n",
        "n_events_signal_after_cut2 = predict_ANOMALO2[ y_test2 == 1 ][:,1] > best_cut_ANOMALO2\n",
        "n_events_SM_after_cut2 = predict_ANOMALO2[ y_test2 == 2 ][:,1] > best_cut_ANOMALO2\n",
        "n_eventos_Data_after_cut2 = predict_dados[ predict_dados > best_cut_ANOMALO2 ]\n",
        "\n",
        "print(' --------------- Anomalo 2 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut2 ) )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut_ANOMALO2+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2piusKRgyPlb"
      },
      "source": [
        "# **Aplicando o resultado do treinamento das constantes maiores nas menores**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APTEGaR-zLcc"
      },
      "source": [
        "predict_proba_TraningAnomalo8_overAnomalo5 = bst8_binary_logloss_scale_pos_weight.predict(DataSet_Test5)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo8_overAnomalo5)\n",
        "\n",
        "predict_proba_TraningAnomalo8_overAnomalo6 = bst8_binary_logloss_scale_pos_weight.predict(DataSet_Test6)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo8_overAnomalo6)\n",
        "\n",
        "predict_proba_TraningAnomalo8_overAnomalo7 = bst8_binary_logloss_scale_pos_weight.predict(DataSet_Test7)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo8_overAnomalo7)\n",
        "\n",
        "\n",
        "predict_proba_TraningAnomalo1_overAnomalo2 = bst1_binary_logloss_scale_pos_weight.predict(DataSet_Test2)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo1_overAnomalo2)\n",
        "\n",
        "predict_proba_TraningAnomalo1_overAnomalo3 = bst1_binary_logloss_scale_pos_weight.predict(DataSet_Test3)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo1_overAnomalo3)\n",
        "\n",
        "predict_proba_TraningAnomalo1_overAnomalo4 = bst1_binary_logloss_scale_pos_weight.predict(DataSet_Test4)\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_TraningAnomalo1_overAnomalo4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVgPITTcya1y"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test5_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=0.5 \\times 10^{-6}$', weights = DataSet_Test5_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM5_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test5,predict_proba_TraningAnomalo8_overAnomalo5),best_cut( y_test5,predict_proba_TraningAnomalo8_overAnomalo5)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test5, predict_proba_TraningAnomalo8_overAnomalo5) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test5,predict_proba_TraningAnomalo8_overAnomalo5 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut5 = predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 0 ][:,1] > best_cut( y_test5, predict_proba_TraningAnomalo8_overAnomalo5 ) \n",
        "n_events_signal_after_cut5 = predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 1 ][:,1] > best_cut(y_test5,predict_proba_TraningAnomalo8_overAnomalo5 )\n",
        "n_events_SM_after_cut5 = predict_proba_TraningAnomalo8_overAnomalo5[ y_test5 == 2 ][:,1] > best_cut(y_test5,predict_proba_TraningAnomalo8_overAnomalo5 )\n",
        "n_eventos_Data_after_cut5 = predict_dados[ predict_dados >= best_cut( y_test5,predict_proba_TraningAnomalo8_overAnomalo5 ) ]\n",
        "\n",
        "print(\"Anomalo 5 \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test5_weight_backgr[ n_events_back_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test5_weight_signal[ n_events_signal_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM5_weight_signal[ n_events_SM_after_cut5 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test5, predict_proba_TraningAnomalo8_overAnomalo5 )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test6_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=1.0 \\times 10^{-5}$', weights = DataSet_Test6_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM6_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test6,predict_proba_TraningAnomalo8_overAnomalo6),best_cut(y_test6,predict_proba_TraningAnomalo8_overAnomalo6)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test6, predict_proba_TraningAnomalo8_overAnomalo6) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test6,predict_proba_TraningAnomalo8_overAnomalo6 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut6 = predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 0 ][:,1] > best_cut( y_test6, predict_proba_TraningAnomalo8_overAnomalo6 ) \n",
        "n_events_signal_after_cut6 = predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 1 ][:,1] > best_cut(y_test6,predict_proba_TraningAnomalo8_overAnomalo6 )\n",
        "n_events_SM_after_cut6 = predict_proba_TraningAnomalo8_overAnomalo6[ y_test6 == 2 ][:,1] > best_cut(y_test6,predict_proba_TraningAnomalo8_overAnomalo6 )\n",
        "n_eventos_Data_after_cut6 = predict_dados[ predict_dados >= best_cut( y_test6,predict_proba_TraningAnomalo8_overAnomalo6 ) ]\n",
        "\n",
        "print('Anomalo 6')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test6_weight_backgr[ n_events_back_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test6_weight_signal[ n_events_signal_after_cut6].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM6_weight_signal[ n_events_SM_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut6 ) )\n",
        "print('Recall Score',recall_score(y_test6,y_pred_6, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test6,y_pred_6, average = 'weighted') ) \n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test7_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test7_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM7_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test7,predict_proba_TraningAnomalo8_overAnomalo7),best_cut(y_test7,predict_proba_TraningAnomalo8_overAnomalo7)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test7, predict_proba_TraningAnomalo8_overAnomalo7) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test7,predict_proba_TraningAnomalo8_overAnomalo7 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut7 = predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 0 ][:,1] >= best_cut( y_test7, predict_proba_TraningAnomalo8_overAnomalo7 ) \n",
        "n_events_signal_after_cut7 = predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 1 ][:,1] >= best_cut(y_test7,predict_proba_TraningAnomalo8_overAnomalo7 )\n",
        "n_events_SM_after_cut7 = predict_proba_TraningAnomalo8_overAnomalo7[ y_test7 == 2 ][:,1] >= best_cut(y_test7,predict_proba_TraningAnomalo8_overAnomalo7 )\n",
        "n_eventos_Data_after_cut7 = predict_dados[ predict_dados >= best_cut( y_test7,predict_proba_TraningAnomalo8_overAnomalo7 ) ]\n",
        "\n",
        "print('Anomalo 7')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test7_weight_backgr[ n_events_back_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test7_weight_signal[ n_events_signal_after_cut7].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM7_weight_signal[ n_events_SM_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut7 ) )\n",
        "print('Recall Score',recall_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test8_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test8_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM8_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight),best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut8 = predict_dados[ predict_dados > best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 8')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test8_weight_backgr[ n_events_back_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test8_weight_signal[ n_events_signal_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM8_weight_signal[ n_events_SM_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut8 ) )\n",
        "print('Recall Score',recall_score(y_test8,y_pred_8, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test8,y_pred_8, average = 'weighted') )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z08D2y7w4wuL"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test4_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', weights = DataSet_Test4_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM4_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test4,predict_proba_TraningAnomalo1_overAnomalo4),best_cut( y_test4,predict_proba_TraningAnomalo1_overAnomalo4)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test4, predict_proba_TraningAnomalo1_overAnomalo4) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test4,predict_proba_TraningAnomalo1_overAnomalo4 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut4 = predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 0 ][:,1] > best_cut( y_test4, predict_proba_TraningAnomalo1_overAnomalo4 ) \n",
        "n_events_signal_after_cut4 = predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 1 ][:,1] > best_cut(y_test4,predict_proba_TraningAnomalo1_overAnomalo4 )\n",
        "n_events_SM_after_cut4 = predict_proba_TraningAnomalo1_overAnomalo4[ y_test4 == 2 ][:,1] > best_cut(y_test4,predict_proba_TraningAnomalo1_overAnomalo4 )\n",
        "n_eventos_Data_after_cut4 = predict_dados[ predict_dados >= best_cut( y_test4,predict_proba_TraningAnomalo1_overAnomalo4 ) ]\n",
        "\n",
        "print(\" --------------- Anomalo 4 --------------- \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test4_weight_signal[ n_events_signal_after_cut4].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test4, predict_proba_TraningAnomalo1_overAnomalo4 )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test1_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=20 \\times 10^{-6}$', weights = DataSet_Test1_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM1_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight),best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut1 = predict_dados[ predict_dados >= best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' --------------- Anomalo 1 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test1_weight_signal[ n_events_signal_after_cut1].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut1 ) )\n",
        "print('Recall Score',recall_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_TraningAnomalo1_overAnomalo3[ y_test3 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test3_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_TraningAnomalo1_overAnomalo3[ y_test3 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test3_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_TraningAnomalo1_overAnomalo3[ y_test3 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM3_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test3,predict_proba_TraningAnomalo1_overAnomalo3),best_cut(y_test3,predict_proba_TraningAnomalo1_overAnomalo3)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test3, predict_proba_TraningAnomalo1_overAnomalo3) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test3,predict_proba_TraningAnomalo1_overAnomalo3 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut3 = predict_proba_TraningAnomalo1_overAnomalo3[ y_test3 == 0 ][:,1] >= best_cut( y_test3, predict_proba_TraningAnomalo1_overAnomalo3 ) \n",
        "n_events_signal_after_cut3 = predict_proba_TraningAnomalo1_overAnomalo3[ y_test3 == 1 ][:,1] >= best_cut(y_test3,predict_proba_TraningAnomalo1_overAnomalo3 )\n",
        "n_events_SM_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 2 ][:,1] >= best_cut(y_test3,predict_proba_TraningAnomalo1_overAnomalo3 )\n",
        "n_eventos_Data_after_cut3 = predict_dados[ predict_dados >= best_cut( y_test3,predict_proba_TraningAnomalo1_overAnomalo3 ) ]\n",
        "\n",
        "print(' --------------- Anomalo 3 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test3_weight_signal[ n_events_signal_after_cut3].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut3 ) )\n",
        "print('Recall Score',recall_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test3, predict_proba_TraningAnomalo1_overAnomalo3 )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'gold', label = 'Background', weights = DataSet_Test2_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test2_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM2_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test2,predict_proba_TraningAnomalo1_overAnomalo2),best_cut(y_test2,predict_proba_TraningAnomalo1_overAnomalo2)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test2, predict_proba_TraningAnomalo1_overAnomalo2) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test2,predict_proba_TraningAnomalo1_overAnomalo2 ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut2 = predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 0 ][:,1] > best_cut( y_test2, predict_proba_TraningAnomalo1_overAnomalo2 ) \n",
        "n_events_signal_after_cut2 = predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 1 ][:,1] > best_cut(y_test2,predict_proba_TraningAnomalo1_overAnomalo2 )\n",
        "n_events_SM_after_cut2 = predict_proba_TraningAnomalo1_overAnomalo2[ y_test2 == 2 ][:,1] > best_cut(y_test2,predict_proba_TraningAnomalo1_overAnomalo2 )\n",
        "n_eventos_Data_after_cut2 = predict_dados[ predict_dados > best_cut( y_test2,predict_proba_TraningAnomalo1_overAnomalo2 ) ]\n",
        "\n",
        "print(' --------------- Anomalo 2 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut2 ) )\n",
        "print('Recall Score',recall_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test2, predict_proba_TraningAnomalo1_overAnomalo2 )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OInwYB6tJFAu"
      },
      "source": [
        "sys.exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcXEuQi_0u69"
      },
      "source": [
        "SM['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "SM['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "SM['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( SM[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fk6f8YB0u69"
      },
      "source": [
        "ANOMALO8['class_predict'] = ( bst8_binary_logloss_scale_pos_weight.predict( ANOMALO8[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO7['class_predict'] = ( bst7_binary_logloss_scale_pos_weight.predict( ANOMALO7[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO1['class_predict'] = ( bst1_binary_logloss_scale_pos_weight.predict( ANOMALO1[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO4['class_predict'] = ( bst4_binary_logloss_scale_pos_weight.predict( ANOMALO4[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO6['class_predict'] = ( bst6_binary_logloss_scale_pos_weight.predict( ANOMALO6[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO5['class_predict'] = ( bst5_binary_logloss_scale_pos_weight.predict( ANOMALO5[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO3['class_predict'] = ( bst3_binary_logloss_scale_pos_weight.predict( ANOMALO3[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "ANOMALO2['class_predict'] = ( bst2_binary_logloss_scale_pos_weight.predict( ANOMALO2[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfjI1g-j0u6-"
      },
      "source": [
        "# Fazendo um dicionário de tal forma que pudesse alocar todos os anomalos numa mesma estrutura apenas para aqueles classificados como sinal (signal region)\n",
        "labels_signals = [r'ANOMALO8', r'ANOMALO7', r'ANOMALO1',r'ANOMALO4']\n",
        "df_signals_protons_multiRP_events_sigreg = {}\n",
        "df_signals_protons_multiRP_events_sigreg[labels_signals[0]] = ANOMALO8[ ANOMALO8['class_predict'] == 1 ]\n",
        "df_signals_protons_multiRP_events_sigreg[labels_signals[1]] = ANOMALO7[ ANOMALO7['class_predict'] == 1 ]\n",
        "df_signals_protons_multiRP_events_sigreg[labels_signals[2]] = ANOMALO1[ ANOMALO1['class_predict'] == 1 ]\n",
        "df_signals_protons_multiRP_events_sigreg[labels_signals[3]] = ANOMALO4[ ANOMALO4['class_predict'] == 1 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8jpzc5E0u6-"
      },
      "source": [
        "DataSet_multiRP_DrellYan['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns])[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns])[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns])[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns]  )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns]  )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "\n",
        "DataSet_multiRP_DrellYan['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_DrellYan[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_QCD['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_QCD[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_single_top['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_single_top[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_TTbar['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_TTbar[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_VV_inclusivo['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_VV_inclusivo[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "DataSet_multiRP_WJets['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( DataSet_multiRP_WJets[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_back_multirp['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( data_set_back_multirp[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17-FrOhe0u6_"
      },
      "source": [
        "data_set_dados_multirp['class_predict8'] = ( bst8_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict7'] = ( bst7_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict1'] = ( bst1_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict4'] = ( bst4_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict5'] = ( bst5_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict2'] = ( bst2_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict6'] = ( bst6_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ).astype(int)\n",
        "data_set_dados_multirp['class_predict3'] = ( bst3_binary_logloss_scale_pos_weight.predict( data_set_dados_multirp[select_columns] )[:,1] > best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoWCShoF0u7A"
      },
      "source": [
        "# Método ABCD\n",
        "\n",
        "# Estiamando o Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Qpnmhw0u7A"
      },
      "source": [
        "A partir das duas variáveis $ \\left (  \\dfrac{M_{WW}}{M_{PPS}} - 1  \\;\\; \\text{e} \\;\\; Y_{WW} - Y_{PPS} \\right ) $ que têm distribuições não correlacionadas para o background. Um corte em cada uma dessas variáveis terá uma eficiência independente de quaisquer seleções feitas na outra variável. Digamos que dividamos o plano 2D em quatro regiões (A, B, C, D) sendo A a região do sinal final onde os requisitos de seleção das variáveis são aplicados.\n",
        "\n",
        "Se as seleções são de fato não correlacionadas para o background, então pode-se supor que a proporção de eventos em cada região pode ser relacionada como: \n",
        "\n",
        "$$\n",
        "\\dfrac{ N_{A}^{bkg} }{ N_{B}^{bkg} } = \\dfrac{ N_{C}^{bkg} }{ N_{D}^{bkg} } \\Leftrightarrow N_{A}^{bkg} = N_{B}^{bkg} \\cdot \\dfrac{N_{C}^{bkg}}{N_{D}^{bkg}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZvkXhGH0u7B"
      },
      "source": [
        "## Para as amostras de Background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYq288Yb0u7B"
      },
      "source": [
        "df_protons_multiRP_bkg_events = data_set_back_multirp\n",
        "df_protons_multiRP_bkg_events['shiftedRatioMWW_MX'] =  df_protons_multiRP_bkg_events[ \"Mww/Mx\" ] - 1 \n",
        "df_protons_multiRP_bkg_events['diffYWW_YX'] = df_protons_multiRP_bkg_events['Yww_Yx'] \n",
        "\n",
        "cutMww_Mx = 0.30\n",
        "cutYww_Yx = 0.50 \n",
        "\n",
        "msk_bkg_cut1 = ( np.abs( df_protons_multiRP_bkg_events[ \"shiftedRatioMWW_MX\" ] ) <= cutMww_Mx )\n",
        "msk_bkg_cut2 = ( np.abs( df_protons_multiRP_bkg_events[ \"diffYWW_YX\" ] ) <= cutYww_Yx )\n",
        "msk_bkg_A =  msk_bkg_cut1 &  msk_bkg_cut2\n",
        "msk_bkg_B = ~msk_bkg_cut1 &  msk_bkg_cut2\n",
        "msk_bkg_C =  msk_bkg_cut1 & ~msk_bkg_cut2\n",
        "msk_bkg_D = ~msk_bkg_cut1 & ~msk_bkg_cut2\n",
        "\n",
        "n_events_bkg   = np.sum( df_protons_multiRP_bkg_events[ \"weight\" ]  )\n",
        "n_events_bkg_A = np.sum( df_protons_multiRP_bkg_events[ msk_bkg_A ][ \"weight\" ] )\n",
        "n_events_bkg_B = np.sum( df_protons_multiRP_bkg_events[ msk_bkg_B ][ \"weight\" ])\n",
        "n_events_bkg_C = np.sum( df_protons_multiRP_bkg_events[ msk_bkg_C ][ \"weight\" ])\n",
        "n_events_bkg_D = np.sum( df_protons_multiRP_bkg_events[ msk_bkg_D ][ \"weight\" ])\n",
        "\n",
        "print ( \"Number of events for background: {}\".format( n_events_bkg ) )\n",
        "print ( \"Number of events (A): {}\".format( n_events_bkg_A ) )\n",
        "print ( \"Number of events (B): {}\".format( n_events_bkg_B ) )\n",
        "print ( \"Number of events (C): {}\".format( n_events_bkg_C ) )\n",
        "print ( \"Number of events (D): {}\".format( n_events_bkg_D ), '\\n')\n",
        "\n",
        "x_min = -1.2\n",
        "x_max = 3.\n",
        "y_min = -3.5\n",
        "y_max =  3.5\n",
        "x_center = ( x_min + x_max ) / 2.\n",
        "y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "fig = plt.figure( figsize=(16,10) )\n",
        "plt.plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "plt.plot( (0,0), (y_min,y_max), color='blue' )\n",
        "ax = plt.gca()\n",
        "label_str_ = r\"A ($|M_{WW} / M_{PPS}| < 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "df_protons_multiRP_bkg_events[ msk_bkg_A ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='lime', label=label_str_ )\n",
        "label_str_ = r\"B ($|M_{WW} / M_{PPS}| > 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "df_protons_multiRP_bkg_events[ msk_bkg_B ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label=label_str_ )\n",
        "label_str_ = r\"C ($|M_{WW} / M_{PPS}| < 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "df_protons_multiRP_bkg_events[ msk_bkg_C ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label=label_str_ )\n",
        "label_str_ = r\"D ($|M_{WW} / M_{PPS}| > 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "df_protons_multiRP_bkg_events[ msk_bkg_D ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='lightcoral', label=label_str_ )\n",
        "plt.legend( loc='best', fontsize=12, framealpha=0.9, frameon=True, fancybox=True )\n",
        "ax.text( 0.80, 0.95, \"Background\", horizontalalignment='left', verticalalignment='center', transform=ax.transAxes, fontsize=18 )\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xlabel( r\"$\\dfrac{M_{WW}}{M_{PPS}} - 1$\", fontsize=22 )\n",
        "plt.ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "hep.cms.label(llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$\" )\n",
        "#plt.savefig( pwd_savefig+'ABCD_multiRP_bkg_events.pdf' )\n",
        "#plt.show()\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9S0Sw940u7C"
      },
      "source": [
        "## Para os Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8rTGMP40u7C"
      },
      "source": [
        "df_protons_multiRP_data_events = data_set_dados_multirp\n",
        "df_protons_multiRP_data_events['shiftedRatioMWW_MX'] =  df_protons_multiRP_data_events[ \"Mww/Mx\" ] - 1 \n",
        "df_protons_multiRP_data_events['diffYWW_YX'] = df_protons_multiRP_data_events['Yww_Yx'] \n",
        "\n",
        "\n",
        "msk_data_cut1 = ( np.abs( df_protons_multiRP_data_events[ \"shiftedRatioMWW_MX\" ] ) <= 0.30 )\n",
        "msk_data_cut2 = ( np.abs( df_protons_multiRP_data_events[ \"diffYWW_YX\" ] ) <= 0.50 )\n",
        "msk_data_A =  msk_data_cut1 &  msk_data_cut2\n",
        "msk_data_B = ~msk_data_cut1 &  msk_data_cut2\n",
        "msk_data_C =  msk_data_cut1 & ~msk_data_cut2\n",
        "msk_data_D = ~msk_data_cut1 & ~msk_data_cut2\n",
        "msk_data = ~msk_data_A\n",
        "\n",
        "\n",
        "n_events_data_B = df_protons_multiRP_data_events[ msk_data_B ].shape[0]\n",
        "n_events_data_C = df_protons_multiRP_data_events[ msk_data_C ].shape[0]\n",
        "n_events_data_D = df_protons_multiRP_data_events[ msk_data_D ].shape[0]\n",
        "resample_factor = 20\n",
        "print ( \"Number of events for Data (B): {} - Ratio: {}\".format( n_events_data_B, ( n_events_data_B / ( n_events_bkg_B / resample_factor ) ) ) )\n",
        "print ( \"Number of events for Data (C): {} - Ratio: {}\".format( n_events_data_C, ( n_events_data_C / ( n_events_bkg_C / resample_factor ) ) ) )\n",
        "print ( \"Number of events for Data (D): {} - Ratio: {}\".format( n_events_data_D, ( n_events_data_D / ( n_events_bkg_D / resample_factor ) ) ), '\\n' )\n",
        "\n",
        "\n",
        "x_min = -1.5\n",
        "x_max =  1.5\n",
        "y_min = -3.0\n",
        "y_max =  3.0\n",
        "x_center = ( x_min + x_max ) / 2.\n",
        "y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "fig = plt.figure( figsize=(15,10) )\n",
        "plt.plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "plt.plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "ax = plt.gca()\n",
        "df_protons_multiRP_data_events[ msk_data_B ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label = label_str_ )\n",
        "label_str_ = r\"B ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "df_protons_multiRP_data_events[ msk_data_C ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label = label_str_ )\n",
        "label_str_ = r\"C ($|M_{WW} / M_{PPS} - 1| < 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "df_protons_multiRP_data_events[ msk_data_D ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='lightcoral', label = label_str_ )\n",
        "label_str_ = r\"D ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$\"\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xlabel( r\"$\\dfrac{M_{WW}}{ M_{PPS} } - 1$\", fontsize=22 )\n",
        "plt.ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "plt.legend( loc='best', fontsize=12, framealpha=0.9, frameon=True, fancybox=True )\n",
        "ax.text( -1.3, 2, \"Data-2016\", fontsize=18 )\n",
        "hep.cms.label(llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$\" )\n",
        "#plt.savefig( pwd_savefig+'ABCD_multiRP_data_events.pdf' )\n",
        "#plt.show()\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaqmAArJ0u7D"
      },
      "source": [
        "## Para os eventos de background na região de Background após o resultado da classificação do LightGBM com o anômalo 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5h7w3TU0u7D"
      },
      "source": [
        "df_protons_multiRP_bkg_events_bkgreg = df_protons_multiRP_bkg_events[ df_protons_multiRP_bkg_events[ 'class_predict8' ] == 0 ]\n",
        "\n",
        "msk_bkg_cut1 = ( np.abs( df_protons_multiRP_bkg_events_bkgreg[ \"shiftedRatioMWW_MX\" ] ) <= 0.30 )\n",
        "msk_bkg_cut2 = ( np.abs( df_protons_multiRP_bkg_events_bkgreg[ \"diffYWW_YX\" ] ) <= 0.50 )\n",
        "msk_bkg_A =  msk_bkg_cut1 &  msk_bkg_cut2\n",
        "msk_bkg_B = ~msk_bkg_cut1 &  msk_bkg_cut2\n",
        "msk_bkg_C =  msk_bkg_cut1 & ~msk_bkg_cut2\n",
        "msk_bkg_D = ~msk_bkg_cut1 & ~msk_bkg_cut2\n",
        "\n",
        "n_events_bkg_bkgreg   = np.sum(df_protons_multiRP_bkg_events_bkgreg[ \"weight\" ])\n",
        "n_events_bkg_bkgreg_A = np.sum(df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_A ][ \"weight\" ] )\n",
        "n_events_bkg_bkgreg_B = np.sum(df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_B ][ \"weight\" ]  )\n",
        "n_events_bkg_bkgreg_C = np.sum(df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_C ][ \"weight\" ] )\n",
        "n_events_bkg_bkgreg_D = np.sum(df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_D ][ \"weight\" ])\n",
        "\n",
        "print ( \"Number of events of the background on Signal Region: {}\".format( n_events_bkg_bkgreg ) )\n",
        "print ( \"Number of events (A): {}\".format( n_events_bkg_bkgreg_A ) )\n",
        "print ( \"Number of events (B): {}\".format( n_events_bkg_bkgreg_B ) )\n",
        "print ( \"Number of events (C): {}\".format( n_events_bkg_bkgreg_C ) )\n",
        "print ( \"Number of events (D): {}\".format( n_events_bkg_bkgreg_D ),'\\n' )\n",
        "\n",
        "x_min = -1.5\n",
        "x_max =  1.5\n",
        "y_min = -3.5\n",
        "y_max =  3.5\n",
        "x_center = ( x_min + x_max ) / 2.\n",
        "y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "fig = plt.figure( figsize=(18,10) )\n",
        "plt.plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "plt.plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "ax = plt.gca()\n",
        "label_str_ = r\"A ($|M_{WW} / M_{PPS} - 1| < 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_A ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='lime', label=label_str_ )\n",
        "label_str_ = r\"B ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_B ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label=label_str_ )\n",
        "label_str_ = r\"C ($|M_{WW} / M_{PPS} - 1| < 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_C ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='green', label=label_str_ )\n",
        "label_str_ = r\"D ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "df_protons_multiRP_bkg_events_bkgreg[ msk_bkg_D ].plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='lightcoral', label=label_str_ )\n",
        "plt.legend( loc='best', fontsize=12, framealpha=0.9, frameon=True, fancybox=True )\n",
        "title_str_ = \"Background \\n$\\\\rm{{Cut Prob.}} < {:.2f}$\\n\\nBackground Region\".format( best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight) )\n",
        "title_str_ = r\"{}\".format( title_str_ )\n",
        "ax.text( -1.4, 2, title_str_, fontsize=18 )\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "#plt.title(r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $')\n",
        "plt.xlabel( r\"$\\dfrac{M_{WW}}{M_{PPS}} - 1$\", fontsize=22 )\n",
        "plt.ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "#plt.savefig( pwd_savefig+'ABCD_multiRP_bkg_events_BACKGROUND_REGION.pdf' )\n",
        "#plt.show()\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJPgXZgC0u7E"
      },
      "source": [
        "## Para o Background na Região de Signal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OVtU0_90u7F"
      },
      "source": [
        "labels_signals = [r'ANOMALO8', r'ANOMALO7', r'ANOMALO1',r'ANOMALO4']\n",
        "df_protons_multiRP_bkg_events_sigreg = {}\n",
        "df_protons_multiRP_bkg_events_sigreg[labels_signals[0]] = data_set_back_multirp[ data_set_back_multirp['class_predict8'] == 1 ]\n",
        "df_protons_multiRP_bkg_events_sigreg[labels_signals[1]] = data_set_back_multirp[ data_set_back_multirp['class_predict7'] == 1 ]\n",
        "df_protons_multiRP_bkg_events_sigreg[labels_signals[2]] = data_set_back_multirp[ data_set_back_multirp['class_predict1'] == 1 ]\n",
        "df_protons_multiRP_bkg_events_sigreg[labels_signals[3]] = data_set_back_multirp[ data_set_back_multirp['class_predict4'] == 1 ]\n",
        "\n",
        "labels_samples = [\n",
        "    r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $',\n",
        "    r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $',\n",
        "    r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=2.0 \\times 10^{-5}$',\n",
        "    r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8 \\times 10^{-6}$' ]\n",
        "\n",
        "nrows_ = ( int( len(labels_signals)/2 ) + 1 if ( len(labels_signals) % 2 ) > 0 else  int( len(labels_signals)/2 ) )\n",
        "fig, axes = plt.subplots( nrows_, 2, figsize=(24,nrows_*10) )\n",
        "\n",
        "row_ = 0\n",
        "col_ = 0\n",
        "for label_ in labels_signals:\n",
        "    print ( label_ )\n",
        "    \n",
        "    df_protons_multiRP_bkg_events_sigreg[ label_ ]['shiftedRatioMWW_MX'] = df_protons_multiRP_bkg_events_sigreg[ label_ ]['Mww/Mx'] - 1\n",
        "    msk_sig_cut1 = ( np.abs( df_protons_multiRP_bkg_events_sigreg[ label_ ][ \"shiftedRatioMWW_MX\" ] ) <= 0.30 )\n",
        "    msk_sig_cut2 = ( np.abs( df_protons_multiRP_bkg_events_sigreg[ label_ ][ \"Yww_Yx\" ] ) <= 0.50 )\n",
        "    msk_sig_A =  msk_sig_cut1 &  msk_sig_cut2\n",
        "    msk_sig_B = ~msk_sig_cut1 &  msk_sig_cut2\n",
        "    msk_sig_C =  msk_sig_cut1 & ~msk_sig_cut2\n",
        "    msk_sig_D = ~msk_sig_cut1 & ~msk_sig_cut2\n",
        "\n",
        "    n_events_sig   = np.sum( df_protons_multiRP_bkg_events_sigreg[ label_ ][ \"weight\" ] )\n",
        "    err_n_events_sig = np.sqrt( np.sum(np.square(  df_protons_multiRP_bkg_events_sigreg[ label_ ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_A = np.sum( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_A ][ \"weight\" ] )\n",
        "    err_n_events_sig_A = np.sqrt( np.sum(np.square( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_A ][ \"weight\" ]  ) ) )\n",
        "    n_events_sig_B = np.sum( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_B ][ \"weight\" ] )\n",
        "    err_n_events_sig_B = np.sqrt( np.sum(np.square(  df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_B ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_C = np.sum(  df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_C ][ \"weight\" ] )\n",
        "    err_n_events_sig_C = np.sqrt( np.sum(np.square( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_C ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_D = np.sum( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_D ][ \"weight\" ] )\n",
        "    err_n_events_sig_D = np.sqrt( np.sum(np.square( df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_D ][ \"weight\" ] ) ) )\n",
        "\n",
        "    print ( \"Number of events: {} +/- {}\".format( n_events_sig, err_n_events_sig ) )\n",
        "    print ( \"Number of events (A): {} +/- {}\".format( n_events_sig_A, err_n_events_sig_A ) )\n",
        "    print ( \"Number of events (B): {} +/- {}\".format( n_events_sig_B, err_n_events_sig_B ) )\n",
        "    print ( \"Number of events (C): {} +/- {}\".format( n_events_sig_C, err_n_events_sig_C ) )\n",
        "    print ( \"Number of events (D): {} +/- {}\".format( n_events_sig_D, err_n_events_sig_D ) )\n",
        "\n",
        "    x_min = -1.0\n",
        "    x_max =  1.0\n",
        "    y_min = -3.0\n",
        "    y_max =  3.0\n",
        "    x_center = ( x_min + x_max ) / 2.\n",
        "    y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "    print ( row_, col_ )\n",
        "    axes[ row_, col_ ].plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "    axes[ row_, col_ ].plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "    if np.sum( msk_sig_A ) > 0:\n",
        "        label_str_ = r\"A ($|M_{WW} / M_{pp} - 1| < 0.30$  $|Y_{WW} - Y_{pp}| < 0.50$)\"\n",
        "        df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_A ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='lime', label = label_str_ )\n",
        "    if np.sum( msk_sig_B ) > 0:\n",
        "        label_str_ = r\"B ($|M_{WW} / M_{pp} - 1| > 0.30$  $|Y_{WW} - Y_{pp}| < 0.50$)\"\n",
        "        df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_B ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='green', label = label_str_ )\n",
        "    if np.sum( msk_sig_C ) > 0:\n",
        "        label_str_ = r\"C ($|M_{WW} / M_{pp} - 1| < 0.30$  $|Y_{WW} - Y_{pp}| > 0.50$)\"\n",
        "        df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_C ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='green', label = label_str_ )\n",
        "    if np.sum( msk_sig_D ) > 0:\n",
        "        label_str_ = r\"D ($|M_{WW} / M_{pp} - 1| > 0.30$  $|Y_{WW} - Y_{pp}| > 0.50$)\"\n",
        "        df_protons_multiRP_bkg_events_sigreg[ label_ ][ msk_sig_D ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='lightcoral', label = label_str_ )\n",
        "    #title_str_ = \"{}\\n$\\\\rm{{Prob.}} > {:.2f}$\".format( labels_samples[ label_ ], prob_cut )\n",
        "    #title_str_ = r\"{}\".format( title_str_ )\n",
        "    #axes[ row_, col_ ].text( 0.70, 0.92, title_str_, horizontalalignment='left', verticalalignment='center', transform=axes[ row_, col_ ].transAxes, fontsize=18 )\n",
        "    axes[ row_, col_ ].set_xlim(x_min, x_max)\n",
        "    axes[ row_, col_ ].set_ylim(y_min, y_max)\n",
        "    axes[ row_, col_ ].set_xlabel( r\"$\\dfrac{M_{WW}}{M_{PPS}} - 1$\", fontsize=22 )\n",
        "    axes[ row_, col_ ].set_ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "    axes[ row_, col_ ].set_title( \"Background Sample\", fontsize=22 )\n",
        "    axes[ 0 , 0].text( -0.9, 2.4 , labels_samples[0], fontsize=18)\n",
        "    axes[ 0 , 0].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight)), fontsize=18) \n",
        "    axes[ 0 , 1].text( -0.9, 2.4 , labels_samples[1], fontsize=18)\n",
        "    axes[ 0 , 1].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 1 , 0].text( -0.9, 2.4 , labels_samples[2], fontsize=18)\n",
        "    axes[ 1 , 0].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 1 , 1].text( -0.9, 2.4 , labels_samples[3], fontsize=18)\n",
        "    axes[ 1 , 1].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 0 , 0].legend(loc='lower right', fontsize=13,framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 0 , 1].legend(loc='lower right', fontsize=13,framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 1 , 0].legend(loc='lower right', fontsize=13,framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 1 , 1].legend(loc='lower right', fontsize=13,framealpha=0.9, frameon=True, fancybox=True)\n",
        "    #plt.savefig(pwd_savefig+'multiRP_events_sigreg_BACKGROUND_SignalRegion.pdf')\n",
        "    col_ += 1\n",
        "    if col_ >= 2:\n",
        "        row_ += 1\n",
        "        col_  = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpwpgaPZ0u7F"
      },
      "source": [
        "## Para o Signal na Região de Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8YJ8sR0u7F"
      },
      "source": [
        "labels_samples = [\n",
        "    r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $',\n",
        "    r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $',\n",
        "    r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=2.0 \\times 10^{-5}$',\n",
        "    r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8 \\times 10^{-6}$' ]\n",
        "\n",
        "nrows_ = ( int( len(labels_samples)/2 ) + 1 if ( len(labels_samples) % 2 ) > 0 else  int( len(labels_samples)/2 ) )\n",
        "fig, axes = plt.subplots( nrows_, 2, figsize=(24,nrows_*10) )\n",
        "\n",
        "row_ = 0\n",
        "col_ = 0\n",
        "for label_ in labels_signals:\n",
        "    print ( label_ )\n",
        "    \n",
        "    df_signals_protons_multiRP_events_sigreg[ label_ ]['shiftedRatioMWW_MX'] = df_signals_protons_multiRP_events_sigreg[ label_ ]['Mww/Mx'] - 1\n",
        "    msk_sig_cut1 = ( np.abs( df_signals_protons_multiRP_events_sigreg[ label_ ][ \"shiftedRatioMWW_MX\" ] ) <= 0.30 )\n",
        "    msk_sig_cut2 = ( np.abs( df_signals_protons_multiRP_events_sigreg[ label_ ][ \"Yww_Yx\" ] ) <= 0.50 )\n",
        "    msk_sig_A =  msk_sig_cut1 &  msk_sig_cut2\n",
        "    msk_sig_B = ~msk_sig_cut1 &  msk_sig_cut2\n",
        "    msk_sig_C =  msk_sig_cut1 & ~msk_sig_cut2\n",
        "    msk_sig_D = ~msk_sig_cut1 & ~msk_sig_cut2\n",
        "\n",
        "    n_events_sig   = np.sum(  df_signals_protons_multiRP_events_sigreg[ label_ ][ \"weight\" ] )\n",
        "    err_n_events_sig = np.sqrt( np.sum( np.square( df_signals_protons_multiRP_events_sigreg[ label_ ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_A = np.sum(  df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_A ][ \"weight\" ] )\n",
        "    err_n_events_sig_A = np.sqrt( np.sum( np.square( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_A ][ \"weight\" ]  ) ) )\n",
        "    n_events_sig_B = np.sum( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_B ][ \"weight\" ] )\n",
        "    err_n_events_sig_B = np.sqrt( np.sum( np.square( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_B ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_C = np.sum( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_C ][ \"weight\" ] )\n",
        "    err_n_events_sig_C = np.sqrt( np.sum( np.square( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_C ][ \"weight\" ] ) ) )\n",
        "    n_events_sig_D = np.sum(  df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_D ][ \"weight\" ] )\n",
        "    err_n_events_sig_D = np.sqrt( np.sum( np.square( df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_D ][ \"weight\" ] ) ) )\n",
        "\n",
        "    print ( \"Number of events: {} +/- {}\".format( n_events_sig, err_n_events_sig ) )\n",
        "    print ( \"Number of events (A): {} +/- {}\".format( n_events_sig_A, err_n_events_sig_A ) )\n",
        "    print ( \"Number of events (B): {} +/- {}\".format( n_events_sig_B, err_n_events_sig_B ) )\n",
        "    print ( \"Number of events (C): {} +/- {}\".format( n_events_sig_C, err_n_events_sig_C ) )\n",
        "    print ( \"Number of events (D): {} +/- {}\".format( n_events_sig_D, err_n_events_sig_D ) )\n",
        "\n",
        "    x_min = -1.0\n",
        "    x_max =  1.0\n",
        "    y_min = -3.0\n",
        "    y_max =  3.0\n",
        "    x_center = ( x_min + x_max ) / 2.\n",
        "    y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "    print ( row_, col_ )\n",
        "    axes[ row_, col_ ].plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "    axes[ row_, col_ ].plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "    if np.sum( msk_sig_A ) > 0:\n",
        "        label_str_ = r\"A ($|M_{WW} / M_{PPS} - 1| < 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "        df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_A ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='lime', label = label_str_ )\n",
        "    if np.sum( msk_sig_B ) > 0:\n",
        "        label_str_ = r\"B ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| < 0.50$)\"\n",
        "        df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_B ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='green', label = label_str_ )\n",
        "    if np.sum( msk_sig_C ) > 0:\n",
        "        label_str_ = r\"C ($|M_{WW} / M_{PPS} - 1| < 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "        df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_C ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='green', label = label_str_ )\n",
        "    if np.sum( msk_sig_D ) > 0:\n",
        "        label_str_ = r\"D ($|M_{WW} / M_{PPS} - 1| > 0.30$  $|Y_{WW} - Y_{PPS}| > 0.50$)\"\n",
        "        df_signals_protons_multiRP_events_sigreg[ label_ ][ msk_sig_D ].plot( \"shiftedRatioMWW_MX\", \"Yww_Yx\", 'scatter', ax=axes[ row_, col_ ], color='lightcoral', label = label_str_ )\n",
        "    #title_str_ = \"{}\\n$\\\\rm{{Prob.}} > {:.2f}$\".format( labels_samples[ label_ ], prob_cut )\n",
        "    #title_str_ = r\"{}\".format( title_str_ )\n",
        "    #axes[ row_, col_ ].text( 0.70, 0.92, title_str_, horizontalalignment='left', verticalalignment='center', transform=axes[ row_, col_ ].transAxes, fontsize=18 )\n",
        "    axes[ row_, col_ ].set_xlim(x_min, x_max)\n",
        "    axes[ row_, col_ ].set_ylim(y_min, y_max)\n",
        "    axes[ row_, col_ ].set_xlabel( r\"$M_{WW} / M_{PPS} - 1$\", fontsize=22 )\n",
        "    axes[ row_, col_ ].set_ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "    axes[ row_, col_ ].set_title( \"Signal Sample\", fontsize=22 )\n",
        "    axes[ 0 , 0].text( -0.9, 2.4 , labels_samples[0], fontsize=18)\n",
        "    axes[ 0 , 0].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 0 , 1].text( -0.9, 2.4 , labels_samples[1], fontsize=18)\n",
        "    axes[ 0 , 1].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 1 , 0].text( -0.9, 2.4 , labels_samples[2], fontsize=18)\n",
        "    axes[ 1 , 0].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 1 , 1].text( -0.9, 2.4 , labels_samples[3], fontsize=18)\n",
        "    axes[ 1 , 1].text( 0.5, 2.3 , 'Signal Region\\nCut Prob > {:2.3f}'.format(best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight)), fontsize=18)\n",
        "    axes[ 0 , 0].legend(loc='lower right', fontsize=13, framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 0 , 1].legend(loc='lower right', fontsize=13, framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 1 , 0].legend(loc='lower right', fontsize=13, framealpha=0.9, frameon=True, fancybox=True)\n",
        "    axes[ 1 , 1].legend(loc='lower right', fontsize=13, framealpha=0.9, frameon=True, fancybox=True)\n",
        "    #plt.savefig(pwd_savefig+'multiRP_events_sigreg_SIGNAL_SignalRegion.pdf')\n",
        "    col_ += 1\n",
        "    if col_ >= 2:\n",
        "        row_ += 1\n",
        "        col_  = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdihPJls0u7H"
      },
      "source": [
        "## Para os dados na região de background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEjoZ1o70u7I"
      },
      "source": [
        "df_protons_multiRP_data_events_bkgreg = data_set_dados_multirp[ data_set_dados_multirp['class_predict8'] == 0 ]\n",
        "df_protons_multiRP_data_events_bkgreg['shiftedRatioMWW_MX'] =  df_protons_multiRP_data_events_bkgreg[ \"Mww/Mx\" ] - 1 \n",
        "df_protons_multiRP_data_events_bkgreg['diffYWW_YX'] = df_protons_multiRP_data_events_bkgreg['Yww_Yx'] \n",
        "\n",
        "x_min = -1.5\n",
        "x_max =  1.5\n",
        "y_min = -3.0\n",
        "y_max =  3.0\n",
        "x_center = ( x_min + x_max ) / 2.\n",
        "y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "fig = plt.figure( figsize=(19,10) )\n",
        "plt.plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "plt.plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "ax = plt.gca()\n",
        "df_protons_multiRP_data_events_bkgreg.plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='black' )\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xlabel( r\"$\\dfrac{M_{WW}}{M_{PPS}} - 1$\", fontsize=22 )\n",
        "plt.ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "#plt.savefig( pwd_savefig+'ABCD_multiRP_data_events_BACKGROUND_REGION.pdf' )\n",
        "#plt.show()\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaZL8tDj0u7J"
      },
      "source": [
        "## Para os Dados na região de Signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg7qFcNq0u7J"
      },
      "source": [
        "df_protons_multiRP_data_events_bkgreg = data_set_dados_multirp[ data_set_dados_multirp['class_predict8'] == 1 ]\n",
        "df_protons_multiRP_data_events_bkgreg['shiftedRatioMWW_MX'] =  df_protons_multiRP_data_events_bkgreg[ \"Mww/Mx\" ] - 1 \n",
        "df_protons_multiRP_data_events_bkgreg['diffYWW_YX'] = df_protons_multiRP_data_events_bkgreg['Yww_Yx'] \n",
        "\n",
        "x_min = -1.5\n",
        "x_max =  1.5\n",
        "y_min = -3.0\n",
        "y_max =  3.0\n",
        "x_center = ( x_min + x_max ) / 2.\n",
        "y_center = ( y_min + y_max ) / 2.\n",
        "\n",
        "fig = plt.figure( figsize=(19,10) )\n",
        "plt.plot( (x_min,x_max), (y_center,y_center), color='blue' )\n",
        "plt.plot( (x_center,x_center), (y_min,y_max), color='blue' )\n",
        "ax = plt.gca()\n",
        "df_protons_multiRP_data_events_bkgreg.plot( \"shiftedRatioMWW_MX\", \"diffYWW_YX\", 'scatter', ax=ax, color='black' )\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.xlabel( r\"$\\dfrac{M_{WW}}{M_{PPS}} - 1$\", fontsize=22 )\n",
        "plt.ylabel( r\"$Y_{WW} - Y_{PPS}$\", fontsize=22 )\n",
        "#plt.savefig( pwd_savefig+'ABCD_multiRP_data_events_SIGNAL_REGION.pdf' )\n",
        "#plt.show()\n",
        "#plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2RqnnnR0u7O"
      },
      "source": [
        "## Contagem dos eventos depois do corte"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RsFAU5P0u7P"
      },
      "source": [
        "DrellYan_counting = np.sum(DataSet_multiRP_DrellYan['weight'])\n",
        "QCD_counting = np.sum(DataSet_multiRP_QCD['weight'])\n",
        "Single_top_counting = np.sum(DataSet_multiRP_single_top['weight'])\n",
        "VV_inclusive_counting = np.sum(DataSet_multiRP_VV_inclusivo['weight'])\n",
        "WJets_counting = np.sum(DataSet_multiRP_WJets['weight'])\n",
        "ttbar_counting = np.sum(DataSet_multiRP_TTbar['weight'])\n",
        "ANOMALO8_counting = np.sum(ANOMALO8['weight'])\n",
        "SM_counting = np.sum(SM['weight'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6b8vXK0u7P"
      },
      "source": [
        "DrellYan_counting_8 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict8']==1]['weight'])\n",
        "QCD_counting_8 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict8']==1]['weight'])\n",
        "Single_top_counting_8 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict8']==1]['weight'])\n",
        "VV_inclusive_counting_8 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict8']==1]['weight'])\n",
        "WJets_counting_8 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict8']==1]['weight'])\n",
        "ttbar_counting_8 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict8']==1]['weight'])\n",
        "ANOMALO8_counting_8 = np.sum(ANOMALO8[ANOMALO8['class_predict']==1]['weight'])\n",
        "SM_counting_8 = np.sum(SM[SM['class_predict8']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_7 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict7']==1]['weight'])\n",
        "QCD_counting_7 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict7']==1]['weight'])\n",
        "Single_top_counting_7 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict7']==1]['weight'])\n",
        "VV_inclusive_counting_7 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict7']==1]['weight'])\n",
        "WJets_counting_7 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict7']==1]['weight'])\n",
        "ttbar_counting_7 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict7']==1]['weight'])\n",
        "ANOMALO7_counting_7 = np.sum(ANOMALO7[ANOMALO7['class_predict']==1]['weight'])\n",
        "SM_counting_7 = np.sum(SM[SM['class_predict7']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_6 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict6']==1]['weight'])\n",
        "QCD_counting_6 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict6']==1]['weight'])\n",
        "Single_top_counting_6 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict6']==1]['weight'])\n",
        "VV_inclusive_counting_6 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict6']==1]['weight'])\n",
        "WJets_counting_6 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict6']==1]['weight'])\n",
        "ttbar_counting_6 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict6']==1]['weight'])\n",
        "ANOMALO6_counting_6 = np.sum(ANOMALO6[ANOMALO6['class_predict']==1]['weight'])\n",
        "SM_counting_6 = np.sum(SM[SM['class_predict6']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_5 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict5']==1]['weight'])\n",
        "QCD_counting_5 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict5']==1]['weight'])\n",
        "Single_top_counting_5 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict5']==1]['weight'])\n",
        "VV_inclusive_counting_5 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict5']==1]['weight'])\n",
        "WJets_counting_5 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict7']==1]['weight'])\n",
        "ttbar_counting_5 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict5']==1]['weight'])\n",
        "ANOMALO5_counting_5 = np.sum(ANOMALO5[ANOMALO5['class_predict']==1]['weight'])\n",
        "SM_counting_5 = np.sum(SM[SM['class_predict5']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_1 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict1']==1]['weight'])\n",
        "QCD_counting_1 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict1']==1]['weight'])\n",
        "Single_top_counting_1 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict1']==1]['weight'])\n",
        "VV_inclusive_counting_1 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict1']==1]['weight'])\n",
        "WJets_counting_1 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict1']==1]['weight'])\n",
        "ttbar_counting_1 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict1']==1]['weight'])\n",
        "ANOMALO7_counting_1 = np.sum(ANOMALO1[ANOMALO1['class_predict']==1]['weight'])\n",
        "SM_counting_1 = np.sum(SM[SM['class_predict1']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_2 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict2']==1]['weight'])\n",
        "QCD_counting_2 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict2']==1]['weight'])\n",
        "Single_top_counting_2 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict2']==1]['weight'])\n",
        "VV_inclusive_counting_2 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict2']==1]['weight'])\n",
        "WJets_counting_2 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict2']==1]['weight'])\n",
        "ttbar_counting_2 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict2']==1]['weight'])\n",
        "ANOMALO2_counting_2 = np.sum(ANOMALO2[ANOMALO2['class_predict']==1]['weight'])\n",
        "SM_counting_2 = np.sum(SM[SM['class_predict2']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_3 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict3']==1]['weight'])\n",
        "QCD_counting_3 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict3']==1]['weight'])\n",
        "Single_top_counting_3 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict3']==1]['weight'])\n",
        "VV_inclusive_counting_3 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict3']==1]['weight'])\n",
        "WJets_counting_3 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict3']==1]['weight'])\n",
        "ttbar_counting_3 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict3']==1]['weight'])\n",
        "ANOMALO7_counting_3 = np.sum(ANOMALO3[ANOMALO3['class_predict']==1]['weight'])\n",
        "SM_counting_3 = np.sum(SM[SM['class_predict3']==1]['weight'])\n",
        "\n",
        "DrellYan_counting_4 = np.sum(DataSet_multiRP_DrellYan[DataSet_multiRP_DrellYan['class_predict4']==1]['weight'])\n",
        "QCD_counting_4 = np.sum(DataSet_multiRP_QCD[DataSet_multiRP_QCD['class_predict4']==1]['weight'])\n",
        "Single_top_counting_4 = np.sum(DataSet_multiRP_single_top[DataSet_multiRP_single_top['class_predict4']==1]['weight'])\n",
        "VV_inclusive_counting_4 = np.sum(DataSet_multiRP_VV_inclusivo[DataSet_multiRP_VV_inclusivo['class_predict4']==1]['weight'])\n",
        "WJets_counting_4 = np.sum(DataSet_multiRP_WJets[DataSet_multiRP_WJets['class_predict4']==1]['weight'])\n",
        "ttbar_counting_4 = np.sum(DataSet_multiRP_TTbar[DataSet_multiRP_TTbar['class_predict4']==1]['weight'])\n",
        "ANOMALO4_counting_4 = np.sum(ANOMALO4[ANOMALO4['class_predict']==1]['weight'])\n",
        "SM_counting_4 = np.sum(SM[SM['class_predict4']==1]['weight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg75-f-50u7R"
      },
      "source": [
        "# Datacard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNV-Qc6E0u7R"
      },
      "source": [
        "* ### O layout do datacard é o seguinte:\n",
        "\n",
        "\n",
        "No topo estão os números imax, jmax e kmax que representam o número de bins, processos e parâmetros de incômodo(nuisance), respectivamente.\n",
        "Aqui, um \"bin\" pode se referir a uma contagem literal de evento único, ou uma distribuição completa que estamos ajustando, em geral com muitos bins de histograma. É possível substituir esses números por * e eles serão deduzidos automaticamente.\n",
        "\n",
        "A primeira linha começando com bin fornece um rótulo exclusivo para cada canal, e a linha seguinte começando com observação fornece o número de eventos observados dos $\\textbf{DADOS}$.\n",
        "\n",
        "As primeiras quatro linhas rotuladas bin, process, process e rate fornecem o rótulo do canal, o rótulo do processo, um identificador de processo (<= 0 para signal,> 0 para background) e o número de eventos esperados, respectivamente.\n",
        "\n",
        "As linhas restantes descrevem fontes de incerteza sistemática. Cada linha dá o nome da incerteza (que se tornará o nome do parâmetro incômodo dentro de nosso modelo RooFit), o tipo de incerteza (\"lnN\" = incerteza de normalização logarítmica) e o efeito em cada processo em cada canal. Por exemplo. uma incerteza de 20% no rendimento é escrita como 1,20.\n",
        "\n",
        "Também é possível adicionar um símbolo hash (#) no início de uma linha, que a combinação irá ignorar quando ler o cartão.\n",
        "\n",
        "* ### Limites assintóticos\n",
        "\n",
        "\n",
        "Como estamos procurando por um processo de sinal que não existe no modelo padrão, é natural definir um limite superior na seção de choque vezes a fração de ramificação do processo (assumindo que nosso conjunto de dados não contém uma descoberta significativa de nova física). $\\texttt{Combine}$ tem método dedicado para calcular os limites superiores. O mais comumente usado é $\\texttt{AsymptoticLimits}$, que implementa o critério $\\texttt{CLs}$ e usa a razão de verossimilhança do perfil como estatística de teste. Como o nome indica, as distribuições estatísticas de teste são determinadas analiticamente na aproximação assintótica, portanto, não há necessidade de mais tempo para arremessar e ajustar o brinquedo. Executando o seguinte comando: \n",
        "\n",
        "$\\texttt{combine -M AsymptoticLimits datacard_part1.txt -n .part1A}$\n",
        "\n",
        "Você deve ver os resultados dos cálculos dos limites observados e esperados impressos na tela. Aqui adicionamos uma opção extra, -n .part1A, que é a abreviação de --name, e é usada para rotular os arquivos de saída combinados produzidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwcGsesg0u7S"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_5e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut8 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo8       Standard_Model      Background        \\n')\n",
        "dc.write('process         0                 1                2            \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}             {2:.3f}    \\n'.format(DataSet_Test8_weight_signal[ n_events_signal_after_cut8 ].sum() / test_size,\n",
        "                                                                                     DataSet_TestSM8_weight_signal[ n_events_SM_after_cut8 ].sum() / test_size,\n",
        "                                                                                     DataSet_Test8_weight_backgr[ n_events_back_after_cut8 ].sum() / test_size\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                1.0              1.0         \\n')\n",
        "#dc.write('effic_pps lnN  1.10                 -               -       \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_5e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCSL6sMd0u7T"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_2e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut7 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region     \\n')\n",
        "dc.write('process     Anomalo7       Standard_Model       Background         \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}             {2:.3f}     \\n'.format(DataSet_Test7_weight_signal[ n_events_signal_after_cut7 ].sum() / test_size ,\n",
        "                                                                                      DataSet_TestSM7_weight_signal[ n_events_SM_after_cut7 ].sum() / test_size,\n",
        "                                                                                      DataSet_Test7_weight_backgr[ n_events_back_after_cut7 ].sum() / test_size\n",
        "                                                                               \n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025               1.0               1.0      \\n')\n",
        "#dc.write('effic_pps lnN  1.10              1.10             1.10    \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_2e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qdp9RPw0u7U"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_1e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut6 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo6      Standard_Model       Background       \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}             {2:.3f}   \\n'.format(DataSet_Test6_weight_signal[ n_events_signal_after_cut6].sum() / test_size,\n",
        "                                                                                    DataSet_TestSM6_weight_signal[ n_events_SM_after_cut6 ].sum() / test_size,\n",
        "                                                                                    DataSet_Test6_weight_backgr[ n_events_back_after_cut6 ].sum() / test_size\n",
        "                                                                            ,\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                1.0              1.0humm          \\n')\n",
        "#dc.write('effic_pps lnN  1.10              1.10             1.10          \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_1e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwjVpMbO0u7U"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_0.5e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut5 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region       \\n')\n",
        "dc.write('process     Anomalo5       Standard_Model      Background         \\n')\n",
        "dc.write('process         0                 1                2              \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}         {2:.3f}                                                                             \\n'.format(DataSet_Test5_weight_signal[ n_events_signal_after_cut5 ].sum() / test_size,\n",
        "                                                                                                                                                   DataSet_TestSM5_weight_signal[ n_events_SM_after_cut5 ].sum() / test_size,\n",
        "                                                                                                                                                   DataSet_Test5_weight_backgr[ n_events_back_after_cut5 ].sum() / test_size\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                -               -                 \\n')\n",
        "#dc.write('xsecttbar lnN    -                 -              1.013              \\n')\n",
        "dc.write('effic_pps lnN  1.10              1.10             1.10                \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alpha0_0.5e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBYhoS9qg7-t"
      },
      "source": [
        "dc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0svhoMa0u7V"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_8e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut4 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo4       Standard_Model       Background       \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}               {1:.3f}         {2:.3f}   \\n'.format(DataSet_Test4_weight_signal[ n_events_signal_after_cut4 ].sum() / test_size,\n",
        "                                                                                DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size,\n",
        "                                                                                DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size,\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025             1.025            1.025     \\n')\n",
        "#dc.write('xsecttbar lnN    -                 -              1.013  \\n')\n",
        "dc.write('effic_pps lnN  1.10              1.10             1.10    \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_8e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHyejx8h0u7W"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_2e-5.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut2 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo1       Standard_Model       Background       \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}         {2:.3f}   \\n'.format(DataSet_Test1_weight_signal[ n_events_signal_after_cut1 ].sum() / test_size,\n",
        "                                                                                DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size,\n",
        "                                                                                DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size,\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                -               -     \\n')\n",
        "#dc.write('xsecttbar lnN    -                 -              1.013  \\n')\n",
        "dc.write('effic_pps lnN  1.10              1.10             1.10    \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_2e-5.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc8J-JTR0u7X"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_5e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut3 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo3       Standard_Model       Background       \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}         {2:.3f}   \\n'.format(DataSet_Test3_weight_signal[ n_events_signal_after_cut3 ].sum() / test_size,\n",
        "                                                                                DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size,\n",
        "                                                                                DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size,\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                -               -     \\n')\n",
        "#dc.write('xsecttbar lnN    -                 -              1.013  \\n')\n",
        "dc.write('effic_pps lnN  1.10              1.10             1.10    \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_5e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkg7x8XS0u7X"
      },
      "source": [
        "print('Inciando o Datacard com o método simple_counting\\n')\n",
        "\n",
        "dc=open('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_2e-6.txt','w')\n",
        "\n",
        "dc.write('imax *  number of channels\\n')\n",
        "dc.write('jmax *  number of processes -1\\n')\n",
        "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the data observations\n",
        "dc.write('bin           signal_region  \\n')\n",
        "dc.write('observation       {}\\n'.format(len( n_eventos_Data_after_cut2 )))\n",
        "\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the expectations for signal and background\n",
        "dc.write('bin       signal_region    signal_region      signal_region    \\n')\n",
        "dc.write('process     Anomalo2       Standard_Model       Background       \\n')\n",
        "dc.write('process         0                 1                2           \\n')\n",
        "dc.write('rate         {0:.3f}             {1:.3f}             {2:.3f}   \\n'.format(DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size,\n",
        "                                                                                DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size,\n",
        "                                                                                DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size,\n",
        "                                                                                                                                                     ))\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "dc.write('----------------------------------------------------------------------------------------------------------------------------------------------------- \\n')\n",
        "\n",
        "#add the sources of systematic uncertainty\n",
        "dc.write('lumi      lnN  1.025                -               -     \\n')\n",
        "#dc.write('xsecttbar lnN    -                 -              1.013  \\n')\n",
        "dc.write('effic_pps lnN  1.10              1.10             1.10    \\n')\n",
        "\n",
        "\n",
        "       \n",
        "#all done  cross_section_TT\n",
        "dc.close()\n",
        "\n",
        "print('/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard.dat is ready to be used with combine\\n\\n')\n",
        "\n",
        "import os\n",
        "\n",
        "_ = os.system('cat /content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/datacard_simple_counting_alphaC_2e-6.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHw3PW9u0u7Y"
      },
      "source": [
        "# Inspecionando o arquivo de shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmo7wYBq0u7Y"
      },
      "source": [
        "As linhas de código a seguir abrem o arquivo de formas e fazem uma comparação rápida de dados/expectativas antes de qualquer ajuste. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHAV4srD0u7b"
      },
      "source": [
        "def plots_results( var, label_x, range_ ):\n",
        "    \n",
        "    fig, ax = plt.subplots( 2,2, figsize=(19,13) )\n",
        "    var = var\n",
        "    label_x = label_x\n",
        "    range_ = range_\n",
        "    bins = 15\n",
        "    \n",
        "    data_set_dados_multirp8 = data_set_dados_multirp[data_set_dados_multirp[ 'class_predict8' ] == 1 ]\n",
        "    data_set_dados_multirp7 = data_set_dados_multirp[data_set_dados_multirp[ 'class_predict7' ] == 1 ]\n",
        "    data_set_dados_multirp1 = data_set_dados_multirp[data_set_dados_multirp[ 'class_predict1' ] == 1 ]\n",
        "    data_set_dados_multirp4 = data_set_dados_multirp[data_set_dados_multirp[ 'class_predict4' ] == 1 ]\n",
        "    \n",
        "    counts_8, bin_edges_8 = np.histogram( data_set_dados_multirp8[var] , bins = bins, range = range_ )\n",
        "    errors_8 = np.sqrt( counts_8 )\n",
        "    bin_centres_8 = ( bin_edges_8[:-1] + bin_edges_8[1:] ) / 2.\n",
        "\n",
        "    counts_7, bin_edges_7 = np.histogram( data_set_dados_multirp7[var] , bins = bins, range = range_ )\n",
        "    errors_7 = np.sqrt( counts_7 )\n",
        "    bin_centres_7 = ( bin_edges_7[:-1] + bin_edges_7[1:] ) / 2.\n",
        "    \n",
        "    counts_1, bin_edges_1 = np.histogram( data_set_dados_multirp1[var] , bins = bins, range = range_ )\n",
        "    errors_1 = np.sqrt( counts_1 )\n",
        "    bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "    \n",
        "    counts_4, bin_edges_4 = np.histogram( data_set_dados_multirp4[var] , bins = bins, range = range_ )\n",
        "    errors_4 = np.sqrt( counts_4 )\n",
        "    bin_centres_4 = ( bin_edges_4[:-1] + bin_edges_4[1:] ) / 2.    \n",
        "    \n",
        "    num = 8\n",
        "    df_background8 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ][var], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ][var] ]\n",
        "\n",
        "    df_background_weight8 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ]['weight'], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ]['weight'] ,\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ]['weight'] ]\n",
        "    num = 7\n",
        "    df_background7 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ][var], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ][var] ]\n",
        "\n",
        "    df_background_weight7 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ]['weight'], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ]['weight'] ,\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ]['weight'] ]\n",
        "\n",
        "    num = 1\n",
        "    df_background1 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ][var], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ][var] ]\n",
        "\n",
        "    df_background_weight1 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ]['weight'], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ]['weight'] ,\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ]['weight'] ]\n",
        "\n",
        "    num = 4\n",
        "    df_background4 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ][var], \n",
        "                      DataSet_multiRP_QCD[ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ][var],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ][var] ]\n",
        "\n",
        "    df_background_weight4 = [ DataSet_multiRP_DrellYan[ DataSet_multiRP_DrellYan['class_predict{}'.format(num)] == 1 ]['weight'], \n",
        "                      DataSet_multiRP_QCD [ DataSet_multiRP_QCD['class_predict{}'.format(num)] == 1 ]['weight'] ,\n",
        "                      DataSet_multiRP_single_top[ DataSet_multiRP_single_top['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_TTbar[ DataSet_multiRP_TTbar['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_VV_inclusivo[ DataSet_multiRP_VV_inclusivo['class_predict{}'.format(num)] == 1 ]['weight'],\n",
        "                      DataSet_multiRP_WJets[ DataSet_multiRP_WJets['class_predict{}'.format(num)] == 1 ]['weight'] ]\n",
        "\n",
        "\n",
        "\n",
        "    label_back = [ 'Drell-Yan', 'QCD', 'Single Top', r'$t\\bar{t}$', '(WW,WZ,ZZ) Inclusive', 'W+Jets' ]\n",
        "\n",
        "    ax[0,0].hist( df_background8, stacked = True, bins = bins, range = range_, histtype = 'bar', label = label_back, weights = df_background_weight8 )\n",
        "    ax[0,0].hist( ANOMALO8[ANOMALO8['class_predict'] == 1 ][var], ls=':', linewidth=3, bins = bins, range = range_, histtype = 'step', color = 'blue', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5 \\times 10^{-6} $', weights = ANOMALO8[ANOMALO8['class_predict'] == 1 ]['weight'] )\n",
        "    #ax[0,0].errorbar( bin_centres_8, counts_8, yerr=errors_8, xerr=abs(bin_centres_8[0] - bin_centres_8[1])/2,fmt='.', label = 'Data-2016', color = 'black' )\n",
        "    ax[0,0].legend(loc='best', fontsize = 12)\n",
        "    ax[0,0].set_xlabel(label_x, fontsize = 15)\n",
        "    ax[0,0].set_ylabel('Events', fontsize = 15)\n",
        "\n",
        "    ax[1,0].hist( df_background7, stacked = True, bins = bins, range = range_, histtype = 'bar', label = label_back, weights = df_background_weight7 )\n",
        "    ax[1,0].hist( ANOMALO7[ANOMALO7['class_predict'] == 1 ][var], ls=':', linewidth=3, bins = bins, range = range_, histtype = 'step', color = 'blue', label =  r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = ANOMALO7[ANOMALO7['class_predict'] == 1 ]['weight'] )\n",
        "    #ax[1,0].errorbar( bin_centres_7, counts_7, yerr=errors_7, xerr=abs(bin_centres_7[0] - bin_centres_7[1])/2,fmt='.', label = 'Data-2016', color = 'black' )    \n",
        "    ax[1,0].legend(loc='best', fontsize = 12)\n",
        "    ax[1,0].set_xlabel(label_x, fontsize = 15)\n",
        "    ax[1,0].set_ylabel('Events', fontsize = 15)\n",
        "\n",
        "    ax[0,1].hist( df_background1, stacked = True, bins = bins, range = range_, histtype = 'bar', label = label_back, weights = df_background_weight1 )\n",
        "    ax[0,1].hist( ANOMALO1[ANOMALO1['class_predict'] == 1 ][var], ls=':', linewidth=3, bins = bins, range = range_, histtype = 'step', color = 'blue', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=2.0 \\times 10^{-5}$', weights = ANOMALO1[ANOMALO1['class_predict'] == 1 ]['weight'] )\n",
        "    #ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[0] - bin_centres_1[1])/2,fmt='.', label = 'Data-2016', color = 'black' )    \n",
        "    ax[0,1].legend(loc='best', fontsize = 12)\n",
        "    ax[0,1].set_xlabel(label_x, fontsize = 15)\n",
        "    ax[0,1].set_ylabel('Events', fontsize = 15)\n",
        "\n",
        "    ax[1,1].hist( df_background4, stacked = True, bins = bins, range = range_, histtype = 'bar', label = label_back, weights = df_background_weight4 )\n",
        "    ax[1,1].hist( ANOMALO4[ANOMALO4['class_predict'] == 1 ][var], ls=':', linewidth=3, bins = bins, range = range_, histtype = 'step', color = 'blue', label =  r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8 \\times 10^{-6}$', weights = ANOMALO4[ANOMALO4['class_predict'] == 1 ]['weight'] )\n",
        "    #ax[1,1].errorbar( bin_centres_4, counts_4, yerr=errors_4, xerr=abs(bin_centres_4[0] - bin_centres_4[1])/2,fmt='.', label = 'Data-2016', color = 'black' )    \n",
        "    ax[1,1].legend(loc='best', fontsize = 12)\n",
        "    ax[1,1].set_xlabel(label_x, fontsize = 15)\n",
        "    ax[1,1].set_ylabel('Events', fontsize = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Br8iF40u7c"
      },
      "source": [
        "# Histogramas das variáveis após o treinamento e corte no discriminante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIvyAXYF0u7d"
      },
      "source": [
        "plots_results('Yww_Yx','Yww - Yx', (-2.5,2.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXaonI5C0u7e"
      },
      "source": [
        "plots_results('Mww/Mx','$\\dfrac{Mww}{M_{PPS}}$', (0,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr-Wibxs0u7e"
      },
      "source": [
        "plots_results('ExtraTracks', 'ExtraTracks', (0,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckx8wkVN0u7e"
      },
      "source": [
        "plots_results('METPt','$P_{T}^{MET}$', (80,500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbw3h-Ut0u7e"
      },
      "source": [
        "plots_results('jetAK8_prunedMass','$PrunedMass^{JetAK8}$', (0,200))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2_myhmT0u7f"
      },
      "source": [
        "plots_results('jetAK8_pt','$P_{T}^{JetAK8}$', (200,1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVvWbeYj0u7g"
      },
      "source": [
        "plots_results('Pt_W_lep', '$P_{T}^{W_{lep}}$', (200,1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTChxyjF0u7g"
      },
      "source": [
        "plots_results('Mww','$M_{WW}$', (500,1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCBGTwgt0u7h"
      },
      "source": [
        "plots_results('muon_pt','$\\mu_{pt}$',(100,600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0DReMbB0u7h"
      },
      "source": [
        "# Usando o Smearing no resultado do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzaJvcX50u7h"
      },
      "source": [
        "from random import gauss\n",
        "\n",
        "def xi_smearing(dset):\n",
        "    df_xi1 = []\n",
        "    df_xi2 = []\n",
        "    for i in range(0,len(DataSet_Test5_)):\n",
        "        df_xi1.append( gauss( 0, 0.1 * dset['xi1'][i] ) + np.array( dset['xi1'][i] ) )\n",
        "        df_xi2.append( gauss( 0, 0.1 * dset['xi2'][i] ) + np.array( dset['xi2'][i] ) )\n",
        "    xi1_smearing = pd.DataFrame( df_xi1, columns = ['xi1_smearing'] ) \n",
        "    xi2_smearing = pd.DataFrame( df_xi2, columns = ['xi2_smearing'] ) \n",
        "    return pd.concat( [ xi1_smearing, xi2_smearing ], axis = 1 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp2Jlf7J0u7h"
      },
      "source": [
        "select_columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', \n",
        "                  'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1_smearing', 'xi2_smearing', 'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XfuxvbI0u7j"
      },
      "source": [
        "from tqdm import tqdm # mostra o tempo que passou e quanto tempo falta para terminar o loop\n",
        "def smearing_loop( dset, n_interation, bst, label_ ):\n",
        "    select_columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', \n",
        "                      'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx']\n",
        "    n_events_smearing_Signal = []\n",
        "    n_events_Signal = []\n",
        "    for i in tqdm(range(n_interation)):\n",
        "        if i == 0: \n",
        "            dset_SM_mod = dset[dset['label']==2]\n",
        "        df_xi1_SM = []\n",
        "        df_xi2_SM = []\n",
        "        for j in range( 0, len( dset[ dset[ 'label' ] == 2 ] ) ):\n",
        "            df_xi1_SM.append( gauss( 0, 0.1 * np.array(dset[dset['label']==2]['xi1'])[j] ) + np.array(dset[dset['label']==2]['xi1'])[j] ) \n",
        "            df_xi2_SM.append( gauss( 0, 0.1 * np.array(dset[dset['label']==2]['xi2'])[j] ) + np.array(dset[dset['label']==2]['xi2'])[j] )\n",
        "        dset_SM_mod['xi1'] = df_xi1_SM\n",
        "        dset_SM_mod['xi2'] = df_xi2_SM\n",
        "        dset_SM_mod['Mx'] = 13000 * ( dset_SM_mod['xi1'] * dset_SM_mod['xi2'] )**0.5\n",
        "        dset_SM_mod['Yx'] = 0.5 * ( np.log( dset_SM_mod['xi1'] / dset_SM_mod['xi2'] ) )\n",
        "        dset_SM_mod['Mww/Mx'] = dset_SM_mod['Mww'] / dset_SM_mod['Mx'] \n",
        "        dset_SM_mod['Yww_Yx'] = dset_SM_mod['Yww'] - dset_SM_mod['Yx']\n",
        "\n",
        "        if i == 0: \n",
        "            dset_ANOMALO_mod = dset[ dset[ 'label' ] == 1 ]\n",
        "        df_xi1_ANOMALO = []\n",
        "        df_xi2_ANOMALO = []\n",
        "        for j in range( 0, len( dset[ dset[ 'label' ] == 1 ] ) ):\n",
        "            df_xi1_ANOMALO.append( gauss( 0, 0.1 * np.array(dset[dset['label']==1]['xi1'])[j] ) + np.array(dset[dset['label']==1]['xi1'])[j] ) \n",
        "            df_xi2_ANOMALO.append( gauss( 0, 0.1 * np.array(dset[dset['label']==1]['xi2'])[j] ) + np.array(dset[dset['label']==1]['xi2'])[j] )\n",
        "\n",
        "        dset_ANOMALO_mod['xi1'] = df_xi1_ANOMALO\n",
        "        dset_ANOMALO_mod['xi2'] = df_xi2_ANOMALO\n",
        "        dset_ANOMALO_mod['Mx'] = 13000 * ( dset_ANOMALO_mod['xi1'] * dset_ANOMALO_mod['xi2'] )**0.5\n",
        "        dset_ANOMALO_mod['Yx'] = 0.5 * ( np.log( dset_ANOMALO_mod['xi1'] / dset_ANOMALO_mod['xi2'] ) )\n",
        "        dset_ANOMALO_mod['Mww/Mx'] = dset_ANOMALO_mod['Mww'] / dset_ANOMALO_mod['Mx'] \n",
        "        dset_ANOMALO_mod['Yww_Yx'] = dset_ANOMALO_mod['Yww'] - dset_ANOMALO_mod['Yx']\n",
        "        \n",
        "        dset_back_mod = dset[ dset[ 'label' ] == 0 ] \n",
        "        \n",
        "        dset_back_mod['xi1'] = dset[dset['label']==0]['xi1']\n",
        "        dset_back_mod['xi2'] = dset[dset['label']==0]['xi2']\n",
        "\n",
        "        dset_full = pd.concat( [ dset_SM_mod, dset_ANOMALO_mod, dset_back_mod ], axis = 0 )    \n",
        "        y_test = dset_full.label\n",
        "        predict_proba = bst.predict(dset_full[select_columns])\n",
        "\n",
        "        n_events_back_after_cut = predict_proba[ y_test == 0 ][:,1] > best_cut( y_test, predict_proba ) \n",
        "        n_events_signal_after_cut = predict_proba[ y_test == 1 ][:,1] > best_cut( y_test,predict_proba )\n",
        "        n_events_SM_after_cut = predict_proba[ y_test == 2 ][:,1] > best_cut( y_test,predict_proba )\n",
        "        print('Numero de eventos de background depois do corte COM smearing -->', dset_full[dset_full['label']==0]['weight'][ n_events_back_after_cut ].sum() / test_size )\n",
        "        print('Numero de eventos de signal depois do corte COM smearing -->', dset_full[dset_full['label']==1]['weight'][ n_events_signal_after_cut ].sum() / test_size )\n",
        "        print('Numero de eventos de SM depois do corte COM smearing -->', dset_full[dset_full['label']==2]['weight'][ n_events_SM_after_cut ].sum() / test_size )\n",
        "        '''\n",
        "        dataset_sem_smearing = dset\n",
        "\n",
        "        predict_proba_SEM = bst.predict(dataset_sem_smearing[select_columns])\n",
        "\n",
        "        n_events_back_after_cut_SEM = predict_proba_SEM[ y_test == 0 ][:,1] > best_cut( y_test, predict_proba_SEM ) \n",
        "        n_events_signal_after_cut_SEM = predict_proba_SEM[ y_test == 1 ][:,1] > best_cut(y_test,predict_proba_SEM )\n",
        "        n_events_SM_after_cut_SEM = predict_proba_SEM[ y_test == 2 ][:,1] > best_cut(y_test,predict_proba_SEM )\n",
        "        print('Numero de eventos de background depois do corte SEM smearing -->', dataset_sem_smearing[dataset_sem_smearing['label'] == 0]['weight'][ n_events_back_after_cut_SEM ].sum() / test_size )\n",
        "        print('Numero de eventos de signal depois do corte SEM smearing -->', dataset_sem_smearing[dataset_sem_smearing['label'] == 1]['weight'][ n_events_signal_after_cut_SEM ].sum() / test_size )\n",
        "        print('Numero de eventos de SM depois do corte SEM smearing -->', dataset_sem_smearing[dataset_sem_smearing['label'] == 2]['weight'][ n_events_SM_after_cut_SEM ].sum() / test_size )\n",
        "        '''\n",
        "        n_events_smearing_Signal.append( dset_full[dset_full['label']==1]['weight'][ n_events_signal_after_cut ].sum() / test_size )\n",
        "        #n_events_Signal.append( dataset_sem_smearing[dataset_sem_smearing['label'] == 1 ]['weight'][n_events_signal_after_cut_SEM].sum() / test_size )\n",
        "\n",
        "    with h5py.File( '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/output_smearing' + label_ + '.h5', 'w') as f:\n",
        "        dset = f.create_dataset( 'dados', data = n_events_smearing_Signal )    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VvsFDKNzMg"
      },
      "source": [
        "#smearing_loop( DataSet_Test8_, 500, bst8_binary_logloss_scale_pos_weight, 'anomalo8_parte2' )\n",
        "#smearing_loop( DataSet_Test7_, 500, bst7_binary_logloss_scale_pos_weight, 'anomalo7_parte2' )\n",
        "#smearing_loop( DataSet_Test6_, 500, bst6_binary_logloss_scale_pos_weight, 'anomalo6_parte2' )\n",
        "#smearing_loop( DataSet_Test5_, 500, bst5_binary_logloss_scale_pos_weight, 'anomalo5_parte2' )\n",
        "#smearing_loop( DataSet_Test4_, 500, bst4_binary_logloss_scale_pos_weight, 'anomalo4_parte2' )\n",
        "#smearing_loop( DataSet_Test3_, 500, bst3_binary_logloss_scale_pos_weight, 'anomalo3_parte2' )\n",
        "#smearing_loop( DataSet_Test2_, 500, bst2_binary_logloss_scale_pos_weight, 'anomalo2_parte2' )\n",
        "#smearing_loop( DataSet_Test1_, 500, bst1_binary_logloss_scale_pos_weight, 'anomalo1_parte2' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1t_h9Ex0u7j"
      },
      "source": [
        "predict_proba_LGBM8_binary_logloss_scale_pos_weight = bst8_binary_logloss_scale_pos_weight.predict(DataSet_Test8_[select_columns])\n",
        "print('Predict Proba Anomalo 8 --> \\n', predict_proba_LGBM8_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM7_binary_logloss_scale_pos_weight = bst7_binary_logloss_scale_pos_weight.predict(DataSet_Test7_[select_columns])\n",
        "print('Predict Proba Anomalo 7 --> \\n', predict_proba_LGBM7_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM6_binary_logloss_scale_pos_weight = bst6_binary_logloss_scale_pos_weight.predict(DataSet_Test6_[select_columns])\n",
        "print('Predict Proba Anomalo 6 --> \\n', predict_proba_LGBM6_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM5_binary_logloss_scale_pos_weight = bst5_binary_logloss_scale_pos_weight.predict(DataSet_Test5_[select_columns])\n",
        "print('Predict Proba Anomalo 5 --> \\n', predict_proba_LGBM5_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM1_binary_logloss_scale_pos_weight = bst1_binary_logloss_scale_pos_weight.predict(DataSet_Test1_[select_columns])\n",
        "print('Predict Proba Anomalo 1 --> \\n', predict_proba_LGBM1_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM2_binary_logloss_scale_pos_weight = bst2_binary_logloss_scale_pos_weight.predict(DataSet_Test2_[select_columns])\n",
        "print('Predict Proba Anomalo 2 --> \\n', predict_proba_LGBM2_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM3_binary_logloss_scale_pos_weight = bst3_binary_logloss_scale_pos_weight.predict(DataSet_Test3_[select_columns])\n",
        "print('Predict Proba Anomalo 3 --> \\n', predict_proba_LGBM3_binary_logloss_scale_pos_weight)\n",
        "\n",
        "predict_proba_LGBM4_binary_logloss_scale_pos_weight = bst4_binary_logloss_scale_pos_weight.predict(DataSet_Test4_[select_columns])\n",
        "print('Predict Proba Anomalo 4 --> \\n', predict_proba_LGBM4_binary_logloss_scale_pos_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6HrbGc00u7k"
      },
      "source": [
        "y_pred_1 = [np.argmax(line) for line in predict_proba_LGBM1_binary_logloss_scale_pos_weight > best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )]\n",
        "y_pred_2 = [np.argmax(line) for line in predict_proba_LGBM2_binary_logloss_scale_pos_weight > best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )]\n",
        "y_pred_3 = [np.argmax(line) for line in predict_proba_LGBM3_binary_logloss_scale_pos_weight > best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )]\n",
        "y_pred_4 = [np.argmax(line) for line in predict_proba_LGBM4_binary_logloss_scale_pos_weight > best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )]\n",
        "y_pred_5 = [np.argmax(line) for line in predict_proba_LGBM5_binary_logloss_scale_pos_weight > best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )]\n",
        "y_pred_6 = [np.argmax(line) for line in predict_proba_LGBM6_binary_logloss_scale_pos_weight > best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )]\n",
        "y_pred_7 = [np.argmax(line) for line in predict_proba_LGBM7_binary_logloss_scale_pos_weight > best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )]\n",
        "y_pred_8 = [np.argmax(line) for line in predict_proba_LGBM8_binary_logloss_scale_pos_weight > best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea45ctp20u7k"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst4_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test4_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', weights = DataSet_Test4_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM4_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight),best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 0 ][:,1] > best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 1 ][:,1] > best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut4 = predict_proba_LGBM4_binary_logloss_scale_pos_weight[ y_test4 == 2 ][:,1] > best_cut(y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut4 = predict_dados[ predict_dados >= best_cut( y_test4,predict_proba_LGBM4_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(\" ------------------ Anomalo 4 ------------------ \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test4_weight_signal[ n_events_signal_after_cut4].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test4,y_pred_4, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test4, predict_proba_LGBM4_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst1_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test1_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=2.0 \\times 10^{-5}$', weights = DataSet_Test1_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM1_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight),best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 0 ][:,1] > best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 1 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut1 = predict_proba_LGBM1_binary_logloss_scale_pos_weight[ y_test1 == 2 ][:,1] > best_cut(y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut1 = predict_dados[ predict_dados >= best_cut( y_test1,predict_proba_LGBM1_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' ------------------ Anomalo 1 ------------------ ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test1_weight_signal[ n_events_signal_after_cut1].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut1 ) )\n",
        "print('Recall Score',recall_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test1,y_pred_1, average = 'weighted') )\n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst3_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test3_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test3_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM3_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight),best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 0 ][:,1] >= best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 1 ][:,1] >= best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut3 = predict_proba_LGBM3_binary_logloss_scale_pos_weight[ y_test3 == 2 ][:,1] >= best_cut(y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut3 = predict_dados[ predict_dados >= best_cut( y_test3,predict_proba_LGBM3_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' ------------------ Anomalo 3 ------------------ ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test3_weight_signal[ n_events_signal_after_cut3].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut3 ) )\n",
        "print('Recall Score',recall_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test3,y_pred_3, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test3, predict_proba_LGBM3_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst2_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test2_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test2_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM2_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight),best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 0 ][:,1] > best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 1 ][:,1] > best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut2 = predict_proba_LGBM2_binary_logloss_scale_pos_weight[ y_test2 == 2 ][:,1] > best_cut(y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut2 = predict_dados[ predict_dados > best_cut( y_test2,predict_proba_LGBM2_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(' ------------------ Anomalo 2 ------------------ ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut2 ) )\n",
        "print('Recall Score',recall_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test2,y_pred_2, average = 'weighted') )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test2, predict_proba_LGBM2_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwBKpe3F0u7l"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = bst5_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test5_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=0.5 \\times 10^{-6}$', weights = DataSet_Test5_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM5_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight),best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 0 ][:,1] > best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 1 ][:,1] > best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut5 = predict_proba_LGBM5_binary_logloss_scale_pos_weight[ y_test5 == 2 ][:,1] > best_cut(y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut5 = predict_dados[ predict_dados >= best_cut( y_test5,predict_proba_LGBM5_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print(\"Anomalo 5 \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test5_weight_backgr[ n_events_back_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test5_weight_signal[ n_events_signal_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM5_weight_signal[ n_events_SM_after_cut5 ].sum() / test_size )\n",
        "print('Recall Score',recall_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test5,y_pred_5, average = 'weighted') )\n",
        "\n",
        "ax[0,0].text(best_cut( y_test5, predict_proba_LGBM5_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = bst6_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test6_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=1.0 \\times 10^{-5}$', weights = DataSet_Test6_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM6_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight),best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight)+0.02],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight)+0.02 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 0 ][:,1] > best_cut( y_test6, predict_proba_LGBM6_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 1 ][:,1] > best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut6 = predict_proba_LGBM6_binary_logloss_scale_pos_weight[ y_test6 == 2 ][:,1] > best_cut(y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut6 = predict_dados[ predict_dados >= best_cut( y_test6,predict_proba_LGBM6_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 6')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test6_weight_backgr[ n_events_back_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test6_weight_signal[ n_events_signal_after_cut6].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM6_weight_signal[ n_events_SM_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut6 ) )\n",
        "print('Recall Score',recall_score(y_test6,y_pred_6, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test6,y_pred_6, average = 'weighted') )\n",
        "\n",
        "ax[0,1].text(best_cut( y_test1, predict_proba_LGBM1_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = bst7_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test7_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test7_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM7_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight),best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 0 ][:,1] >= best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 1 ][:,1] >= best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut7 = predict_proba_LGBM7_binary_logloss_scale_pos_weight[ y_test7 == 2 ][:,1] >= best_cut(y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut7 = predict_dados[ predict_dados >= best_cut( y_test7,predict_proba_LGBM7_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 7')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test7_weight_backgr[ n_events_back_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test7_weight_signal[ n_events_signal_after_cut7].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM7_weight_signal[ n_events_SM_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut7 ) )\n",
        "print('Recall Score',recall_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test7,y_pred_7, average = 'weighted') )\n",
        "\n",
        "ax[1,0].text(best_cut( y_test7, predict_proba_LGBM7_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = bst8_binary_logloss_scale_pos_weight.predict(data_set_dados_multirp[ select_columns ])[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test8_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test8_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standar Model', weights = DataSet_TestSM8_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight),best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight)],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight) ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 0 ][:,1] > best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight ) \n",
        "n_events_signal_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 1 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_events_SM_after_cut8 = predict_proba_LGBM8_binary_logloss_scale_pos_weight[ y_test8 == 2 ][:,1] > best_cut(y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight )\n",
        "n_eventos_Data_after_cut8 = predict_dados[ predict_dados > best_cut( y_test8,predict_proba_LGBM8_binary_logloss_scale_pos_weight ) ]\n",
        "\n",
        "print('Anomalo 8')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test8_weight_backgr[ n_events_back_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test8_weight_signal[ n_events_signal_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM8_weight_signal[ n_events_SM_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut8 ) )\n",
        "print('Recall Score',recall_score(y_test8,y_pred_8, average = 'weighted') )\n",
        "print('Precision Score',precision_score(y_test8,y_pred_8, average = 'weighted') ) \n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut( y_test8, predict_proba_LGBM8_binary_logloss_scale_pos_weight )+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFVLawdJ0u7l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}