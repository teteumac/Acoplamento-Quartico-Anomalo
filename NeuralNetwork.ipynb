{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8yf5vG//gUSs3XwhcMPhK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teteumac/Acoplamento-Quartico-Anomalo/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xZSz3H3wn5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367e9eb3-7c6e-47b8-9f75-31f390b2abd8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp7GddWQyV99"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBQqWh_Jzkq"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_shape, learning_rate):\n",
        "        self.input_shape = input_shape\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def __call__(self, n_hidden, n_neurons, dropout):\n",
        "\n",
        "        input_shape = self.input_shape\n",
        "        learning_rate = self.learning_rate\n",
        "\n",
        "        print( \"Building model with:\" )\n",
        "        print( \"Input shape: {}\".format(input_shape) )\n",
        "        print( \"Learning rate: {}\".format(learning_rate) )\n",
        "        print( \"Number of hidden layers: {}\".format(n_hidden) )\n",
        "        print( \"Number of neurons per layer: {}\".format(n_neurons) )\n",
        "        print( \"Dropout rate: {}\".format(dropout) )\n",
        "\n",
        "        model = keras.models.Sequential()\n",
        "        model.add( keras.layers.InputLayer(input_shape=input_shape) )\n",
        "        for layer in range(n_hidden):\n",
        "            if dropout > 0.:\n",
        "                model.add( keras.layers.Dropout(rate=dropout) )\n",
        "            model.add( keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") )\n",
        "        if dropout > 0.:\n",
        "            model.add( keras.layers.Dropout(rate=dropout) )\n",
        "        model.add( keras.layers.Dense(1, activation=\"sigmoid\") )\n",
        "\n",
        "        #optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
        "        metrics = [\n",
        "        keras.metrics.Precision(name=\"precision\"),\n",
        "        keras.metrics.Recall(name=\"recall\") ]\n",
        "        optimizer = keras.optimizers.Nadam(lr=learning_rate)\n",
        "        model.compile( loss=\"binary_crossentropy\", optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "        return model\n",
        "\n",
        "def build_model(input_shape, learning_rate=5e-4, n_hidden=1, n_neurons=50, dropout=0.20 ):\n",
        "    build_fn_ = Model( input_shape=input_shape, learning_rate=learning_rate )\n",
        "    return build_fn_( n_hidden, n_neurons, dropout )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF_xlyldwqy_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bea38538-2224-490f-f52b-2fcabb6c5fd0"
      },
      "source": [
        "!python3 -m pip install coffea mplhep\n",
        "!pip install matplotlib==3.1.3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting coffea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/f3/2082092c317bfbbda97cf71889b4865a809a43d12a78449f8fc281a9aa2d/coffea-0.7.3-py2.py3-none-any.whl (161kB)\n",
            "\r\u001b[K     |██                              | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 30kB 24.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40kB 22.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 92kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 102kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 112kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 122kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 133kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 143kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 153kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 11.7MB/s \n",
            "\u001b[?25hCollecting mplhep\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/36/c26fa0cdf2735b1f807a38c6bfe703780263cd5f2ad879a5ffc017f8e310/mplhep-0.3.6-py3-none-any.whl\n",
            "Collecting uproot3>=3.14.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/69/d893c6eba0dd0d8f82d841d4b85b6e63c52a1b472aec7cf7ae0efedf5a92/uproot3-3.14.4-py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 19.9MB/s \n",
            "\u001b[?25hCollecting uproot>=4.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/85/06dda0d02fa68f726d49eacce836eb501c1dfde33fe1162ee80358f4ca6b/uproot-4.0.7-py2.py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from coffea) (20.9)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from coffea) (3.7.4.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from coffea) (4.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.7/dist-packages (from coffea) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (4.41.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (3.0.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from coffea) (7.6.3)\n",
            "Collecting lz4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/52/151c815a486290608e4dc6699a0cfd74141dc5191f8fe928e7d1b28b569e/lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 21.7MB/s \n",
            "\u001b[?25hCollecting uproot3-methods>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/57/598207abeb64bf3e0af3fdc19217e56936b6bebabaac6ee270fb151790ce/uproot3_methods-0.10.1-py3-none-any.whl\n",
            "Requirement already satisfied: numba>=0.50.0 in /usr/local/lib/python3.7/dist-packages (from coffea) (0.51.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coffea) (1.1.5)\n",
            "Collecting awkward>=1.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/55/9de22fd5aed55d5b4dfca01dcfe9ee5a8f40adf99d0728454a95930895d3/awkward-1.2.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 36.7MB/s \n",
            "\u001b[?25hCollecting hist>=2\n",
            "  Downloading https://files.pythonhosted.org/packages/25/ff/a2c633f5caa2282501e390ddac0191b87d90779ef97416ae7fc39335f8b2/hist-2.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from coffea) (1.3.0)\n",
            "Collecting uhi>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/30/7046fd215607e8af7b3360553ecadd37543251557186d0a584b7d83f6bca/uhi-0.2.1-py3-none-any.whl\n",
            "Collecting mplhep-data\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/61/83bdeb0a0a32b9b7234d0472c23a6d8172df085390d59219bb24d842d8f8/mplhep_data-0.0.2-py3-none-any.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 39.2MB/s \n",
            "\u001b[?25hCollecting awkward0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b3/376b258ea021eed2c9bdaa1011e0f7b25365157de472d9fae8a2443d9ff5/awkward0-0.15.5-py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->coffea) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->coffea) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->coffea) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->coffea) (0.10.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->coffea) (5.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.50.0->coffea) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.50.0->coffea) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->coffea) (2018.9)\n",
            "Collecting boost-histogram~=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/3d/b42a7a9ba4b39fc7cce19fe57844df0c347dbcedd9774cb690cc7c7593fe/boost_histogram-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 35.8MB/s \n",
            "\u001b[?25hCollecting histoprint>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/56/3b/9a755bf7c63a9e74432d6ece9d187000e0e32db9eaed7f1c48e2ae8fef20/histoprint-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3->coffea) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.1->ipywidgets->coffea) (0.2.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->coffea) (5.3.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->coffea) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->coffea) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (2.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->coffea) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->coffea) (4.7.1)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from histoprint>=1.6->hist>=2->coffea) (8.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.9.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (1.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->coffea) (22.0.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->coffea) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (2.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->coffea) (0.5.1)\n",
            "\u001b[31mERROR: mplhep 0.3.6 has requirement matplotlib>=3.4, but you'll have matplotlib 3.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: awkward0, uproot3-methods, uproot3, uproot, lz4, uhi, mplhep-data, mplhep, awkward, boost-histogram, histoprint, hist, coffea\n",
            "Successfully installed awkward-1.2.3 awkward0-0.15.5 boost-histogram-1.0.2 coffea-0.7.3 hist-2.3.0 histoprint-2.0.1 lz4-3.1.3 mplhep-0.3.6 mplhep-data-0.0.2 uhi-0.2.1 uproot-4.0.7 uproot3-3.14.4 uproot3-methods-0.10.1\n",
            "Collecting matplotlib==3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/9b/35ab3469fd1509f7636a344940569ebfd33239673fd2318e80b4700a257c/matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "\u001b[31mERROR: mplhep 0.3.6 has requirement matplotlib>=3.4, but you'll have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: matplotlib\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed matplotlib-3.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHtHJVnxw8NT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7df203-bac6-4b33-e56f-88a967cb4177"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from joblib import dump, load\n",
        "\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print ( \"sklearn: {}\".format(sklearn.__version__) )\n",
        "print ( \"tensorflow: {}\".format(tf.__version__) )\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "import mplhep as hep\n",
        "from pandas import set_option\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import auc,make_scorer,fbeta_score,precision_score,recall_score,accuracy_score,log_loss,roc_auc_score,classification_report,f1_score,confusion_matrix,roc_curve,precision_recall_curve,average_precision_score\n",
        "\n",
        "import argparse\n",
        "\n",
        "from joblib import dump"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sklearn: 0.22.2.post1\n",
            "tensorflow: 2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46vvuo5rwirM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e145bacb-5664-4999-ddd8-183860522a63"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/'\n",
        "plt.style.use(hep.style.ROOT)\n",
        "\n",
        "def open_file_MC( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi       \n",
        "        return dataframe\n",
        "\n",
        "def open_file_DD( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]        \n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt',\n",
        "       'jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt',\n",
        "       'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'MultiRP1', 'MultiRP2',\n",
        "       'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi       \n",
        "        return dataframe\n",
        "\n",
        "def open_file_Data( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]        \n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2','Mx', 'Yx', \n",
        "'Mww/Mx', 'Yww_Yx'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi        \n",
        "        return dataframe\n",
        "\n",
        "#select_columns = ['Mww', 'Pt_W_lep', 'jetAK8_pt','jetAK8_eta', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2','Mx', 'Mww/Mx']\n",
        "select_columns = ['Mww', 'Pt_W_lep', 'Acoplanaridade_jatos_MET', 'Acoplanaridade_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'xi1', 'xi2', 'Mx', 'Yx', 'Mww/Mx']\n",
        "\n",
        "\n",
        "SM =       open_file_MC( PATH + 'DataSet_SM_multiRP.h5')\n",
        "ANOMALO1 = open_file_MC( PATH + 'DataSet_ANOMALO1_multiRP.h5')\n",
        "ANOMALO2 = open_file_MC( PATH + 'DataSet_ANOMALO2_multiRP.h5')\n",
        "ANOMALO3 = open_file_MC( PATH + 'DataSet_ANOMALO3_multiRP.h5')\n",
        "ANOMALO4 = open_file_MC( PATH + 'DataSet_ANOMALO4_multiRP.h5')\n",
        "ANOMALO5 = open_file_MC( PATH + 'DataSet_ANOMALO5_multiRP.h5')\n",
        "ANOMALO6 = open_file_MC( PATH + 'DataSet_ANOMALO6_multiRP.h5')\n",
        "ANOMALO7 = open_file_MC( PATH + 'DataSet_ANOMALO7_multiRP.h5')\n",
        "ANOMALO8 = open_file_MC( PATH + 'DataSet_ANOMALO8_multiRP.h5')\n",
        "\n",
        "label_signal1  = pd.DataFrame( [1]*len( ANOMALO1 ) )\n",
        "label_signal2  = pd.DataFrame( [1]*len( ANOMALO2 ) )\n",
        "label_signal3  = pd.DataFrame( [1]*len( ANOMALO3 ) )\n",
        "label_signal4  = pd.DataFrame( [1]*len( ANOMALO4 ) )\n",
        "label_signal5  = pd.DataFrame( [1]*len( ANOMALO5 ) )\n",
        "label_signal6  = pd.DataFrame( [1]*len( ANOMALO6 ) )\n",
        "label_signal7  = pd.DataFrame( [1]*len( ANOMALO7 ) )\n",
        "label_signal8  = pd.DataFrame( [1]*len( ANOMALO8 ) )\n",
        "label_signalSM = pd.DataFrame( [1]*len( SM ) )\n",
        "\n",
        "SM = pd.concat( [ SM, label_signalSM ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Standard Model --> ', SM.shape)\n",
        "\n",
        "ANOMALO1 = pd.concat( [ ANOMALO1, label_signal1 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 1 --> ', ANOMALO1.shape)\n",
        "\n",
        "ANOMALO2 = pd.concat( [ ANOMALO2, label_signal2 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 2 --> ', ANOMALO2.shape)\n",
        "\n",
        "ANOMALO3 = pd.concat( [ ANOMALO3, label_signal3 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 3 --> ', ANOMALO3.shape)\n",
        "\n",
        "ANOMALO4 = pd.concat( [ ANOMALO4, label_signal4 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 4 --> ', ANOMALO4.shape)\n",
        "\n",
        "ANOMALO5 = pd.concat( [ ANOMALO5, label_signal5 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 5 --> ', ANOMALO5.shape)\n",
        "\n",
        "ANOMALO6 = pd.concat( [ ANOMALO6, label_signal6 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 6 --> ', ANOMALO6.shape)\n",
        "\n",
        "ANOMALO7 = pd.concat( [ ANOMALO7, label_signal7 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 7 --> ', ANOMALO7.shape)\n",
        "\n",
        "ANOMALO8 = pd.concat( [ ANOMALO8, label_signal8 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 8 --> ', ANOMALO8.shape)\n",
        "\n",
        "data_set_back_multirp = open_file_DD( PATH + 'DataDriven_Background_multiRP.h5' )\n",
        "\n",
        "label_back = pd.DataFrame( [0]*len( data_set_back_multirp ) )\n",
        "data_set_back_multirp = pd.concat( [ data_set_back_multirp, label_back ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "Dataset_Signal_Back1  = pd.concat( [ ANOMALO1 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back2  = pd.concat( [ ANOMALO2 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back3  = pd.concat( [ ANOMALO3 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back4  = pd.concat( [ ANOMALO4 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back5  = pd.concat( [ ANOMALO5 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back6  = pd.concat( [ ANOMALO6 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back7  = pd.concat( [ ANOMALO7 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back8  = pd.concat( [ ANOMALO8 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "\n",
        "data_set_dados_multirp = open_file_Data( PATH + 'DataSet_dados_multiRP.h5' )\n",
        "data_set_dados_multirp = data_set_dados_multirp[select_columns]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.35\n",
        "DataSet_Train1_, DataSet_Test1_ = train_test_split( Dataset_Signal_Back1, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back1.label )\n",
        "DataSet_Train2_, DataSet_Test2_ = train_test_split( Dataset_Signal_Back2, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back2.label )\n",
        "DataSet_Train3_, DataSet_Test3_ = train_test_split( Dataset_Signal_Back3, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back3.label )\n",
        "DataSet_Train4_, DataSet_Test4_ = train_test_split( Dataset_Signal_Back4, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back4.label )\n",
        "DataSet_Train5_, DataSet_Test5_ = train_test_split( Dataset_Signal_Back5, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back5.label )\n",
        "DataSet_Train6_, DataSet_Test6_ = train_test_split( Dataset_Signal_Back6, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back6.label )\n",
        "DataSet_Train7_, DataSet_Test7_ = train_test_split( Dataset_Signal_Back7, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back7.label )\n",
        "DataSet_Train8_, DataSet_Test8_ = train_test_split( Dataset_Signal_Back8, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back8.label )\n",
        "\n",
        "\n",
        "DataSet_Train8 = DataSet_Train8_[select_columns] \n",
        "DataSet_Test8 = DataSet_Test8_[select_columns] \n",
        "\n",
        "DataSet_Train7 = DataSet_Train7_[select_columns] \n",
        "DataSet_Test7 = DataSet_Test7_[select_columns] \n",
        "\n",
        "DataSet_Train6 = DataSet_Train6_[select_columns] \n",
        "DataSet_Test6 = DataSet_Test6_[select_columns] \n",
        "\n",
        "DataSet_Train5 = DataSet_Train5_[select_columns] \n",
        "DataSet_Test5 = DataSet_Test5_[select_columns] \n",
        "\n",
        "DataSet_Train1 = DataSet_Train1_[select_columns] \n",
        "DataSet_Test1 = DataSet_Test1_[select_columns] \n",
        "\n",
        "DataSet_Train2 = DataSet_Train2_[select_columns] \n",
        "DataSet_Test2 = DataSet_Test2_[select_columns] \n",
        "\n",
        "DataSet_Train3 = DataSet_Train3_[select_columns] \n",
        "DataSet_Test3 = DataSet_Test3_[select_columns] \n",
        "\n",
        "DataSet_Train4 = DataSet_Train4_[select_columns] \n",
        "DataSet_Test4 = DataSet_Test4_[select_columns] \n",
        "\n",
        "y_train1 = DataSet_Train1_['label']\n",
        "y_test1  = DataSet_Test1_['label']\n",
        "\n",
        "y_train2 = DataSet_Train2_['label']\n",
        "y_test2  = DataSet_Test2_['label']\n",
        "\n",
        "y_train3 = DataSet_Train3_['label']\n",
        "y_test3  = DataSet_Test3_['label']\n",
        "\n",
        "y_train4 = DataSet_Train4_['label']\n",
        "y_test4  = DataSet_Test4_['label']\n",
        "\n",
        "y_train5 = DataSet_Train5_['label']\n",
        "y_test5  = DataSet_Test5_['label']\n",
        "\n",
        "y_train6 = DataSet_Train6_['label']\n",
        "y_test6  = DataSet_Test6_['label']\n",
        "\n",
        "y_train7 = DataSet_Train7_['label']\n",
        "y_test7  = DataSet_Test7_['label']\n",
        "\n",
        "y_train8 = DataSet_Train8_['label']\n",
        "y_test8  = DataSet_Test8_['label']\n",
        "\n",
        "print('--- Weight Anomalo 8 --- \\n')\n",
        "DataSet_Test8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==1]['weight'] \n",
        "DataSet_Test8_weight_backgr =DataSet_Test8_[DataSet_Test8_['label']==0]['weight'] \n",
        "DataSet_TestSM8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train8_weight_signal = DataSet_Train8_[DataSet_Train8_['label']==1]['weight']\n",
        "DataSet_Train8_weight_backgr = DataSet_Train8_[DataSet_Train8_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test8_weight =  DataSet_Test8_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test8_weight_signal.shape)\n",
        "print( 'Shape Test Background', DataSet_Test8_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 7 --- \\n')\n",
        "\n",
        "DataSet_Test7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==1]['weight'] \n",
        "DataSet_Test7_weight_backgr =  DataSet_Test7_[DataSet_Test7_['label']==0]['weight'] \n",
        "DataSet_TestSM7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train7_weight_signal =  DataSet_Train7_[DataSet_Train7_['label']==1]['weight']\n",
        "DataSet_Train7_weight_backgr =  DataSet_Train7_[DataSet_Train7_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test7_weight =  DataSet_Test7_['weight']\n",
        "\n",
        "print('--- Weight Anomalo 6 --- \\n')\n",
        "\n",
        "DataSet_Test6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==1]['weight'] \n",
        "DataSet_Test6_weight_backgr =  DataSet_Test6_[DataSet_Test6_['label']==0]['weight'] \n",
        "DataSet_TestSM6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train6_weight_signal =  DataSet_Train6_[DataSet_Train6_['label']==1]['weight']\n",
        "DataSet_Train6_weight_backgr =  DataSet_Train6_[DataSet_Train6_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test6_weight =  DataSet_Test6_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 5 --- \\n')\n",
        "\n",
        "DataSet_Test5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==1]['weight'] \n",
        "DataSet_Test5_weight_backgr =  DataSet_Test5_[DataSet_Test5_['label']==0]['weight'] \n",
        "DataSet_TestSM5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train5_weight_signal =  DataSet_Train5_[DataSet_Train5_['label']==1]['weight']\n",
        "DataSet_Train5_weight_backgr =  DataSet_Train5_[DataSet_Train5_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test5_weight =  DataSet_Test5_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 1 --- \\n')\n",
        "\n",
        "DataSet_Test1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==1]['weight'] \n",
        "DataSet_Test1_weight_backgr =  DataSet_Test1_[DataSet_Test1_['label']==0]['weight'] \n",
        "DataSet_TestSM1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train1_weight_signal =  DataSet_Train1_[DataSet_Train1_['label']==1]['weight']\n",
        "DataSet_Train1_weight_backgr = DataSet_Train1_[DataSet_Train1_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test1_weight =  DataSet_Test1_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test1_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test1_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 2 --- \\n')\n",
        "\n",
        "DataSet_Test2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==1]['weight'] \n",
        "DataSet_Test2_weight_backgr =  DataSet_Test2_[DataSet_Test2_['label']==0]['weight'] \n",
        "DataSet_TestSM2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train2_weight_signal =  DataSet_Train2_[DataSet_Train2_['label']==1]['weight']\n",
        "DataSet_Train2_weight_backgr = DataSet_Train2_[DataSet_Train2_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test2_weight =  DataSet_Test2_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test2_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test2_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 3 --- \\n')\n",
        "\n",
        "DataSet_Test3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==1]['weight'] \n",
        "DataSet_Test3_weight_backgr =  DataSet_Test3_[DataSet_Test3_['label']==0]['weight'] \n",
        "DataSet_TestSM3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train3_weight_signal =  DataSet_Train3_[DataSet_Train3_['label']==1]['weight']\n",
        "DataSet_Train3_weight_backgr = DataSet_Train3_[DataSet_Train3_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test3_weight =  DataSet_Test3_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test3_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test3_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 4 --- \\n')\n",
        "\n",
        "DataSet_Test4_weight_signal = DataSet_Test4_[DataSet_Test4_['label']==1]['weight'] \n",
        "DataSet_Test4_weight_backgr = DataSet_Test4_[DataSet_Test4_['label']==0]['weight'] \n",
        "DataSet_TestSM4_weight_signal =  DataSet_Test4_[DataSet_Test4_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train4_weight_signal = DataSet_Train4_[DataSet_Train4_['label']==1]['weight']\n",
        "DataSet_Train4_weight_backgr = DataSet_Train4_[DataSet_Train4_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test4_weight =  DataSet_Test4_['weight']\n",
        "\n",
        "print( 'Shape Test Signal' , DataSet_Test4_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test4_weight_backgr.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape for Standard Model -->  (112, 24)\n",
            "Shape for Anomalo 1 -->  (1544, 24)\n",
            "Shape for Anomalo 2 -->  (174, 24)\n",
            "Shape for Anomalo 3 -->  (434, 24)\n",
            "Shape for Anomalo 4 -->  (742, 24)\n",
            "Shape for Anomalo 5 -->  (169, 24)\n",
            "Shape for Anomalo 6 -->  (299, 24)\n",
            "Shape for Anomalo 7 -->  (672, 24)\n",
            "Shape for Anomalo 8 -->  (1627, 24)\n",
            "--- Weight Anomalo 8 --- \n",
            "\n",
            "Shape Test Signal (609,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 7 --- \n",
            "\n",
            "--- Weight Anomalo 6 --- \n",
            "\n",
            "Shape Test Signal (274,)\n",
            "Shape Test Background (146072,) \n",
            "\n",
            "--- Weight Anomalo 5 --- \n",
            "\n",
            "Shape Test Signal (274,)\n",
            "Shape Test Background (146072,) \n",
            "\n",
            "--- Weight Anomalo 1 --- \n",
            "\n",
            "Shape Test Signal (580,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 2 --- \n",
            "\n",
            "Shape Test Signal (100,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 3 --- \n",
            "\n",
            "Shape Test Signal (191,)\n",
            "Shape Test Background (146071,) \n",
            "\n",
            "--- Weight Anomalo 4 --- \n",
            "\n",
            "Shape Test Signal (299,)\n",
            "Shape Test Background (146071,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cudK-0u3IBja"
      },
      "source": [
        "X_train1, X_valid1, y_train1, y_valid1 = train_test_split( DataSet_Train1, y_train1, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train2, X_valid2, y_train2, y_valid2 = train_test_split( DataSet_Train2, y_train2, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train3, X_valid3, y_train3, y_valid3 = train_test_split( DataSet_Train3, y_train3, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train4, X_valid4, y_train4, y_valid4 = train_test_split( DataSet_Train4, y_train4, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train5, X_valid5, y_train5, y_valid5 = train_test_split( DataSet_Train5, y_train5, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train6, X_valid6, y_train6, y_valid6 = train_test_split( DataSet_Train6, y_train6, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train7, X_valid7, y_train7, y_valid7 = train_test_split( DataSet_Train7, y_train7, test_size=0.20, shuffle=True, random_state=42 )\n",
        "X_train8, X_valid8, y_train8, y_valid8 = train_test_split( DataSet_Train8, y_train8, test_size=0.20, shuffle=True, random_state=42 )\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled1 = scaler.fit_transform( X_train1 )\n",
        "X_valid_scaled1 = scaler.transform( X_valid1)\n",
        "X_test_scaled1 = scaler.transform( DataSet_Test1 )\n",
        "\n",
        "X_train_scaled2 = scaler.fit_transform( X_train2 )\n",
        "X_valid_scaled2 = scaler.transform( X_valid2 )\n",
        "X_test_scaled2 = scaler.transform( DataSet_Test2 )\n",
        "\n",
        "X_train_scaled3 = scaler.fit_transform( X_train3 )\n",
        "X_valid_scaled3 = scaler.transform( X_valid3 )\n",
        "X_test_scaled3 = scaler.transform( DataSet_Test3 )\n",
        "\n",
        "X_train_scaled4 = scaler.fit_transform( X_train4 )\n",
        "X_valid_scaled4 = scaler.transform( X_valid4 )\n",
        "X_test_scaled4 = scaler.transform( DataSet_Test4 )\n",
        "\n",
        "X_train_scaled5 = scaler.fit_transform( X_train5 )\n",
        "X_valid_scaled5 = scaler.transform( X_valid5 )\n",
        "X_test_scaled5 = scaler.transform( DataSet_Test5 )\n",
        "\n",
        "X_train_scaled6 = scaler.fit_transform( X_train6 )\n",
        "X_valid_scaled6 = scaler.transform( X_valid6 )\n",
        "X_test_scaled6 = scaler.transform( DataSet_Test6 )\n",
        "\n",
        "X_train_scaled7 = scaler.fit_transform( X_train7 )\n",
        "X_valid_scaled7 = scaler.transform( X_valid7 )\n",
        "X_test_scaled7 = scaler.transform( DataSet_Test7 )\n",
        "\n",
        "X_train_scaled8 = scaler.fit_transform( X_train8 )\n",
        "X_valid_scaled8 = scaler.transform( X_valid8 )\n",
        "X_test_scaled8 = scaler.transform( DataSet_Test8 )\n",
        "\n",
        "grid_search = None\n",
        "run_tables = False\n",
        "train_model = True\n",
        "run_grid_search = True\n",
        "save_model = True\n",
        "n_iter_search_ = 100\n",
        "\n",
        "learning_rate = 5e-4\n",
        "epochs_grid_search = 20"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9NBUfOeJIVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed676b52-fd60-431a-f971-36d9edd76261"
      },
      "source": [
        "learning_rate = 5e-4\n",
        "epochs_grid_search = 20\n",
        "\n",
        "print ( scaler )\n",
        "\n",
        "'''if train_model and save_model:\n",
        "    import time\n",
        "    id_ = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "    fileName_ = \"standard_scaler_{}_{}.joblib\".format( label_anomalo, id_ )\n",
        "    print ( \"Saving scaler to {}\".format( fileName_ ) )\n",
        "    dump( scaler, fileName_ )\n",
        "'''\n",
        "print('X_train_scaled --> ', X_train_scaled1 )\n",
        "\n",
        "\n",
        "def get_run_logdir(log_dir):\n",
        "    import time\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(log_dir, run_id)\n",
        "\n",
        "def callbacks(patience=10, log_dir=\"\"):\n",
        "    callbacks_ = []\n",
        "    # Early stopping\n",
        "    if patience > 0:\n",
        "        early_stopping_cb_ = keras.callbacks.EarlyStopping( patience=patience, restore_best_weights=True )\n",
        "        callbacks_.append( early_stopping_cb_ )\n",
        "\n",
        "    # TensorBoard\n",
        "    if log_dir:\n",
        "        run_logdir = get_run_logdir(log_dir)\n",
        "        print ( \"Log dir: {}\".format(run_logdir) )\n",
        "        tensorboard_cb_ = keras.callbacks.TensorBoard( run_logdir )\n",
        "        callbacks_.append( tensorboard_cb_ )\n",
        "\n",
        "    return callbacks_\n",
        "\n",
        "grid_search = None\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "X_train_scaled -->  [[-1.78258180e-01 -5.18161560e-03 -7.58058168e-01 ... -1.40345129e+00\n",
            "   1.30959803e+00  4.87261128e-02]\n",
            " [-2.56317444e-03 -3.44127926e-03 -4.87964280e-01 ... -9.61138782e-02\n",
            "   2.65879059e-01 -1.81266971e-02]\n",
            " [ 4.71130822e-01 -8.96809555e-03 -5.78768826e-01 ...  2.59822319e-01\n",
            "   2.98671486e-01  3.14498644e-01]\n",
            " ...\n",
            " [-1.46552280e-01 -4.56944309e-03 -4.63490524e-01 ...  1.39366217e-01\n",
            "  -1.28141105e-01 -1.72401361e-01]\n",
            " [ 2.69940925e-01 -1.03431065e-02 -2.50121758e-01 ... -7.60007198e-01\n",
            "  -9.26890729e-01  3.68948169e-01]\n",
            " [-2.40142570e-01 -1.35441609e-03 -6.32688708e-01 ...  8.43180493e-01\n",
            "   1.09897599e+00 -3.16514719e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQu40zfCJNXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "e41f6cf8-3c4f-4348-f943-bdcfbb060258"
      },
      "source": [
        "'''if train_model and run_grid_search:\n",
        "    import time\n",
        "    print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\n",
        "    time_s_ = time.time()\n",
        "\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    #from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    build_fn_ = Model( input_shape=X_train_scaled1.shape[1:], learning_rate=learning_rate )\n",
        "    keras_clf = keras.wrappers.scikit_learn.KerasClassifier( build_fn_ )\n",
        "\n",
        "#     param_grid = [\n",
        "#         { \"n_hidden\": np.arange(1,3),\n",
        "#           \"n_neurons\": [20,50] }\n",
        "#         ]\n",
        "\n",
        "    param_distribs = {\n",
        "        \"n_hidden\": np.arange(2,6),\n",
        "        \"n_neurons\": 2 ** np.arange(4,8),\n",
        "        \"dropout\":  0.1 * np.arange(2,6),\n",
        "        \"batch_size\": 2 ** np.arange(5,8)\n",
        "        }\n",
        "\n",
        "    #grid_search = GridSearchCV( keras_clf, param_grid, cv=3, scoring='f1', refit=False )\n",
        "\n",
        "    grid_search = RandomizedSearchCV(\n",
        "        keras_clf,\n",
        "        param_distribs,\n",
        "        n_iter=n_iter_search_, cv=3, verbose=20, n_jobs=-1, scoring='f1', refit=False, random_state=42\n",
        "        )\n",
        "\n",
        "    callbacks_ = callbacks(patience=5)\n",
        "    print ( callbacks_ )\n",
        "    grid_search.fit( X_train_scaled1, y_train8, epochs=epochs_grid_search, validation_data=(X_valid_scaled1, y_valid8), callbacks=callbacks_ )\n",
        "\n",
        "    print ( grid_search.best_params_ )\n",
        "    print ( grid_search.best_score_ )\n",
        "    print ( grid_search.cv_results_ )\n",
        "\n",
        "    time_e_ = time.time()\n",
        "    print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "model_final = None'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'if train_model and run_grid_search:\\n    import time\\n    print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\\n    time_s_ = time.time()\\n\\n    from sklearn.model_selection import RandomizedSearchCV\\n    #from sklearn.model_selection import GridSearchCV\\n\\n    build_fn_ = Model( input_shape=X_train_scaled1.shape[1:], learning_rate=learning_rate )\\n    keras_clf = keras.wrappers.scikit_learn.KerasClassifier( build_fn_ )\\n\\n#     param_grid = [\\n#         { \"n_hidden\": np.arange(1,3),\\n#           \"n_neurons\": [20,50] }\\n#         ]\\n\\n    param_distribs = {\\n        \"n_hidden\": np.arange(2,6),\\n        \"n_neurons\": 2 ** np.arange(4,8),\\n        \"dropout\":  0.1 * np.arange(2,6),\\n        \"batch_size\": 2 ** np.arange(5,8)\\n        }\\n\\n    #grid_search = GridSearchCV( keras_clf, param_grid, cv=3, scoring=\\'f1\\', refit=False )\\n\\n    grid_search = RandomizedSearchCV(\\n        keras_clf,\\n        param_distribs,\\n        n_iter=n_iter_search_, cv=3, verbose=20, n_jobs=-1, scoring=\\'f1\\', refit=False, random_state=42\\n        )\\n\\n    callbacks_ = callbacks(patience=5)\\n    print ( callbacks_ )\\n    grid_search.fit( X_train_scaled1, y_train8, epochs=epochs_grid_search, validation_data=(X_valid_scaled1, y_valid8), callbacks=callbacks_ )\\n\\n    print ( grid_search.best_params_ )\\n    print ( grid_search.best_score_ )\\n    print ( grid_search.cv_results_ )\\n\\n    time_e_ = time.time()\\n    print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\\n\\nmodel_final = None'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gIc4ywXM3MV"
      },
      "source": [
        "def traning_keras( X_train_scaled, y_train, X_valid_scaled, y_valid, X_test_scaled, y_test, DataSet_Test_weight_backgr, DataSet_Test_weight_signal, label_anomalo ):\n",
        "    if train_model and run_grid_search:\n",
        "        import time\n",
        "        print( time.strftime(\"%Y/%m/%d %H:%M:%S\", time.localtime() ) )\n",
        "        time_s_ = time.time()\n",
        "\n",
        "        from sklearn.model_selection import RandomizedSearchCV\n",
        "        #from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "        build_fn_ = Model( input_shape=X_train_scaled.shape[1:], learning_rate=learning_rate )\n",
        "        keras_clf = keras.wrappers.scikit_learn.KerasClassifier( build_fn_ )\n",
        "\n",
        "    #     param_grid = [\n",
        "    #         { \"n_hidden\": np.arange(1,3),\n",
        "    #           \"n_neurons\": [20,50] }\n",
        "    #         ]\n",
        "\n",
        "        param_distribs = {\n",
        "            \"n_hidden\": np.arange(2,10),\n",
        "            \"n_neurons\": 2 ** np.arange(5,9),\n",
        "            \"dropout\":  0.1 * np.arange(3,8),\n",
        "            \"batch_size\": 2 ** np.arange(5,8)\n",
        "            }\n",
        "\n",
        "        #grid_search = GridSearchCV( keras_clf, param_grid, cv=3, scoring='f1', refit=False )\n",
        "\n",
        "        grid_search = RandomizedSearchCV(\n",
        "            keras_clf,\n",
        "            param_distribs,\n",
        "            n_iter=n_iter_search_, cv=2, verbose=20, n_jobs=-1, scoring='f1', refit=False, random_state=42\n",
        "            )\n",
        "\n",
        "        callbacks_ = callbacks(patience=5)\n",
        "        print ( callbacks_ )\n",
        "        grid_search.fit( X_train_scaled, y_train, epochs=epochs_grid_search, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks_ )\n",
        "\n",
        "        print ( grid_search.best_params_ )\n",
        "        print ( grid_search.best_score_ )\n",
        "        print ( grid_search.cv_results_ )\n",
        "\n",
        "        time_e_ = time.time()\n",
        "        print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "\n",
        "    model_final = None\n",
        "    '''\n",
        "    if train_model:\n",
        "        params = {'n_hidden': 1, 'n_neurons': 50, 'dropout': 0.20}\n",
        "        batch_size = 32\n",
        "        if run_grid_search:\n",
        "            params = grid_search.best_params_.copy()\n",
        "            batch_size = params[ 'batch_size' ]\n",
        "            params.pop( 'batch_size' )\n",
        "        print ( params, \"batch_size: {}\".format( batch_size ) )\n",
        "\n",
        "        model_final = build_model(input_shape=X_train_scaled.shape[1:], learning_rate=learning_rate, **params )\n",
        "        model_final.summary()\n",
        "        #log_dir=\"keras_logs\"\n",
        "        #callbacks_ = callbacks(patience=5, log_dir=log_dir)\n",
        "        callbacks_ = callbacks( patience=5 )\n",
        "        print ( callbacks_ )\n",
        "        model_final.fit( X_train_scaled, y_train, epochs=200, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=None, verbose = 1 )\n",
        "    else:\n",
        "        model_final = keras.models.load_model( \"/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/keras_model_\"+label_anomalo+\".h5\" )\n",
        "    '''\n",
        "\n",
        "    model_final = build_model(input_shape=X_train_scaled.shape[1:], learning_rate=learning_rate, **grid_search.best_params_ )\n",
        "    model_final.summary()\n",
        "    callbacks_ = callbacks( patience=5 )\n",
        "    print ( 'callbacks--> ', callbacks_ )\n",
        "    model_final.fit( X_train_scaled, y_train, epochs=200, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=None, verbose = 1 )\n",
        "\n",
        "    # Evaluate on training data (without dropout)\n",
        "\n",
        "    model_final.evaluate( X_train_scaled, y_train )\n",
        "\n",
        "    model_final.evaluate( X_valid_scaled, y_valid )\n",
        "\n",
        "    # Evaluate on test data\n",
        "\n",
        "    model_final.evaluate( X_test_scaled, y_test )    \n",
        "\n",
        "    model_final.save(\"/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/keras_model_\"+label_anomalo+\".h5\") \n",
        "\n",
        "    y_test_proba = model_final.predict( X_test_scaled )\n",
        "    print( 'y_test_proba --> ', y_test_proba )\n",
        "\n",
        "    y_train_proba = model_final.predict( X_train_scaled )\n",
        "    print( 'y_train_proba --> ', y_train_proba )\n",
        "\n",
        "    prec_test, rec_test, thresh_test = precision_recall_curve(y_test, y_test_proba)\n",
        "    bidx_test = np.argmax(prec_test * rec_test)\n",
        "    prob_cut_test = thresh_test[bidx_test]\n",
        "\n",
        "    prec_train, rec_train, thresh_train = precision_recall_curve(y_train, y_train_proba)\n",
        "    bidx_train = np.argmax(prec_train * rec_train)\n",
        "    prob_cut_train = thresh_train[bidx_train]\n",
        "\n",
        "    print ( \"Prob. cut test: {}\".format( prob_cut_test ) )\n",
        "\n",
        "    y_test_pred = ( y_test_proba >= prob_cut_test ).astype( \"int32\" )\n",
        "    print ( y_test_pred )\n",
        "\n",
        "    print ( \"Prob. cut train: {}\".format( prob_cut_train ) )\n",
        "\n",
        "    y_train_pred = ( y_train_proba >= prob_cut_train ).astype( \"int32\" )\n",
        "    print ( y_train_pred )\n",
        "\n",
        "    print( \"F1_score in TEST sample  : {:2.2f}%\".format(100 * f1_score(y_test, y_test_pred)))\n",
        "    print( \"F1_score in TRAIN sample  : {:2.2f}%\".format(100 * f1_score(y_train, y_train_pred)))\n",
        "\n",
        "    #print ( f1_score( y_test1[ y_test1 == 1 ], y_test_pred[ y_test1 == 1 ] ) )\n",
        "    #print ( f1_score( y_test1[ y_test1 == 0 ], y_test_pred[ y_test1 == 0 ] ) )    \n",
        "\n",
        "    plt.hist( y_test_proba[y_test == 0], bins = 25, range = (0,1) , weights=DataSet_Test_weight_backgr )\n",
        "    plt.hist( y_test_proba[y_test == 1], bins = 25, range = (0,1) , histtype='step', weights=DataSet_Test_weight_signal )\n",
        "    plt.yscale('log')    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25c9rLsfRI0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798ef0a3-4b3f-4767-9e2d-b896c039296e"
      },
      "source": [
        "traning_keras( X_train_scaled1, y_train1, X_valid_scaled1, y_valid1, X_test_scaled1, y_test1, DataSet_Test1_weight_backgr, DataSet_Test1_weight_signal, \"anomalo1_SM_como_Sinal\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021/05/17 11:42:42\n",
            "[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f387870d690>]\n",
            "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  9.9min\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 10.0min\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 15.8min\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 17.9min\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 19.5min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 20.0min\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 20.0min\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 23.4min\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 23.9min\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 28.3min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 29.0min\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 29.1min\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 29.4min\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 30.7min\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed: 31.0min\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed: 41.7min\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed: 44.8min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 44.8min\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed: 44.9min\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed: 46.4min\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 46.4min\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 49.2min\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 51.0min\n",
            "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 54.2min\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 55.5min\n",
            "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 60.0min\n",
            "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 67.8min\n",
            "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed: 68.5min\n",
            "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed: 70.5min\n",
            "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 72.8min\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 73.4min\n",
            "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed: 74.8min\n",
            "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed: 77.9min\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 79.2min\n",
            "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed: 81.8min\n",
            "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed: 82.3min\n",
            "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 82.9min\n",
            "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 83.8min\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 84.5min\n",
            "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed: 89.1min\n",
            "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed: 91.3min\n",
            "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 93.2min\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 97.4min\n",
            "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed: 98.9min\n",
            "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 99.7min\n",
            "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed: 100.6min\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed: 101.3min\n",
            "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed: 104.1min\n",
            "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed: 105.0min\n",
            "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 108.2min\n",
            "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed: 109.0min\n",
            "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed: 109.6min\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 110.4min\n",
            "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 111.0min\n",
            "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed: 111.9min\n",
            "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed: 114.3min\n",
            "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed: 115.9min\n",
            "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 119.9min\n",
            "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed: 119.9min\n",
            "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed: 121.6min\n",
            "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 123.1min\n",
            "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed: 125.3min\n",
            "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed: 125.8min\n",
            "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed: 127.5min\n",
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed: 127.8min\n",
            "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 130.1min\n",
            "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed: 130.6min\n",
            "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed: 133.1min\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed: 137.4min\n",
            "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed: 142.3min\n",
            "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 144.0min\n",
            "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed: 144.2min\n",
            "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed: 145.1min\n",
            "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 145.3min\n",
            "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed: 145.7min\n",
            "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed: 149.8min\n",
            "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed: 150.6min\n",
            "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 155.1min\n",
            "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 155.3min\n",
            "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed: 160.8min\n",
            "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed: 161.5min\n",
            "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed: 165.0min\n",
            "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed: 167.1min\n",
            "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed: 172.1min\n",
            "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed: 172.6min\n",
            "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 174.2min\n",
            "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 175.6min\n",
            "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed: 177.0min\n",
            "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed: 178.1min\n",
            "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed: 178.4min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 178.8min\n",
            "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed: 196.1min\n",
            "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 196.3min\n",
            "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 196.9min\n",
            "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed: 197.0min\n",
            "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed: 201.3min\n",
            "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed: 201.5min\n",
            "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed: 205.9min\n",
            "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed: 206.0min\n",
            "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed: 206.5min\n",
            "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 206.7min\n",
            "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 208.0min\n",
            "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed: 208.3min\n",
            "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed: 211.1min\n",
            "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed: 211.4min\n",
            "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed: 211.5min\n",
            "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed: 218.1min\n",
            "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed: 220.0min\n",
            "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 220.3min\n",
            "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed: 230.2min\n",
            "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed: 232.4min\n",
            "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed: 233.4min\n",
            "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed: 234.1min\n",
            "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed: 235.4min\n",
            "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed: 236.9min\n",
            "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed: 237.1min\n",
            "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 238.4min\n",
            "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 243.9min\n",
            "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed: 245.2min\n",
            "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed: 247.9min\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 253.3min\n",
            "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed: 260.1min\n",
            "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed: 260.3min\n",
            "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed: 263.2min\n",
            "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed: 263.4min\n",
            "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 266.2min\n",
            "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 268.3min\n",
            "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed: 268.5min\n",
            "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed: 269.3min\n",
            "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed: 271.9min\n",
            "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed: 272.9min\n",
            "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed: 279.3min\n",
            "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed: 280.4min\n",
            "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 283.3min\n",
            "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 286.5min\n",
            "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed: 287.1min\n",
            "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed: 288.6min\n",
            "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 289.6min\n",
            "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed: 290.9min\n",
            "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed: 299.3min\n",
            "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed: 300.2min\n",
            "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed: 303.9min\n",
            "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 305.3min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dvXDXZJwte2"
      },
      "source": [
        "traning_keras( X_train_scaled2, y_train2, X_valid_scaled2, y_valid2, X_test_scaled2, y_test2, DataSet_Test2_weight_backgr, DataSet_Test2_weight_signal, \"anomalo2_SM_como_sinal\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEhD9MigooL2"
      },
      "source": [
        "traning_keras( X_train_scaled3, y_train3, X_valid_scaled3, y_valid3, X_test_scaled3, y_test3, DataSet_Test3_weight_backgr, DataSet_Test3_weight_signal, \"anomalo3_SM_como_sinal\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBwCyA2tovRI"
      },
      "source": [
        "traning_keras( X_train_scaled4, y_train4, X_valid_scaled4, y_valid4, X_test_scaled4, y_test4, DataSet_Test4_weight_backgr, DataSet_Test4_weight_signal, \"anomalo4_SM_como_sinal\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tm_TQzJJQkE"
      },
      "source": [
        "if train_model:\n",
        "    params = {'n_hidden': 1, 'n_neurons': 50, 'dropout': 0.20}\n",
        "    batch_size = 32\n",
        "    if run_grid_search:\n",
        "        params = grid_search.best_params_.copy()\n",
        "        batch_size = params[ 'batch_size' ]\n",
        "        params.pop( 'batch_size' )\n",
        "    print ( params, \"batch_size: {}\".format( batch_size ) )\n",
        "\n",
        "    model_final = build_model(input_shape=X_train_scaled1.shape[1:], learning_rate=learning_rate, **params )\n",
        "    model_final.summary()\n",
        "    #log_dir=\"keras_logs\"\n",
        "    #callbacks_ = callbacks(patience=5, log_dir=log_dir)\n",
        "    callbacks_ = callbacks( patience=5 )\n",
        "    print ( callbacks_ )\n",
        "    model_final.fit( X_train_scaled1, y_train8, epochs=100, batch_size=batch_size, validation_data=(X_valid_scaled1, y_valid8), callbacks=callbacks_ )\n",
        "else:\n",
        "    model_final = keras.models.load_model( \"/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/keras_model_anomalo8.h5\" )\n",
        "\n",
        "model_final.summary()\n",
        "# Evaluate on training data (without dropout)\n",
        "\n",
        "model_final.evaluate( X_train_scaled1, y_train8 )\n",
        "\n",
        "model_final.evaluate( X_valid_scaled1, y_valid8 )\n",
        "\n",
        "# Evaluate on test data\n",
        "\n",
        "model_final.evaluate( X_test_scaled1, y_test8 )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUzMoEQyRdK"
      },
      "source": [
        "model_final.save(\"/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/keras_model_anomalo8.h5\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viEEdQPPlXBh"
      },
      "source": [
        "from sklearn.metrics import auc,make_scorer,fbeta_score,precision_score,recall_score,accuracy_score,log_loss,roc_auc_score,classification_report,f1_score,confusion_matrix,roc_curve,precision_recall_curve,average_precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW7YG8hqkh9r"
      },
      "source": [
        "y_test_proba = model_final.predict( X_test_scaled1 )\n",
        "print( 'y_test_proba --> ', y_test_proba )\n",
        "\n",
        "y_train_proba = model_final.predict( X_train_scaled1 )\n",
        "print( 'y_train_proba --> ', y_train_proba )\n",
        "\n",
        "prec_test, rec_test, thresh_test = precision_recall_curve(y_test8, y_test_proba)\n",
        "bidx_test = np.argmax(prec_test * rec_test)\n",
        "prob_cut_test = thresh_test[bidx_test]\n",
        "\n",
        "prec_train, rec_train, thresh_train = precision_recall_curve(y_train8, y_train_proba)\n",
        "bidx_train = np.argmax(prec_train * rec_train)\n",
        "prob_cut_train = thresh_train[bidx_train]\n",
        "\n",
        "print ( \"Prob. cut test: {}\".format( prob_cut_test ) )\n",
        "\n",
        "y_test_pred = ( y_test_proba >= prob_cut_test ).astype( \"int32\" )\n",
        "print ( y_test_pred )\n",
        "\n",
        "print ( \"Prob. cut train: {}\".format( prob_cut_train ) )\n",
        "\n",
        "y_train_pred = ( y_train_proba >= prob_cut_train ).astype( \"int32\" )\n",
        "print ( y_train_pred )\n",
        "\n",
        "print( \"F1_score in TEST sample  : {:2.2f}%\".format(100 * f1_score(y_test8, y_test_pred)))\n",
        "print( \"F1_score in TRAIN sample  : {:2.2f}%\".format(100 * f1_score(y_train8, y_train_pred)))\n",
        "\n",
        "#print ( f1_score( y_test1[ y_test1 == 1 ], y_test_pred[ y_test1 == 1 ] ) )\n",
        "#print ( f1_score( y_test1[ y_test1 == 0 ], y_test_pred[ y_test1 == 0 ] ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3Vlb-A7oaIu"
      },
      "source": [
        "plt.hist( y_test_proba[y_test8 == 0], bins = 25, range = (0,1) , weights=DataSet_Test8_weight_backgr )\n",
        "plt.hist( y_test_proba[y_test8 == 1], bins = 25, range = (0,1) , histtype='step', weights=DataSet_Test8_weight_signal )\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJhtT-h5IdEs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}