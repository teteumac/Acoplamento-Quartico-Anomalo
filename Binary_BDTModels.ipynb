{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Binary_BDTModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/p9PdMftoQAGyQwNmI4F8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teteumac/Acoplamento-Quartico-Anomalo/blob/main/Binary_BDTModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut2V3habLKFV",
        "outputId": "70827e2f-17ca-45a2-a69e-feb6483ee30a"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 17 15:01:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEh7S45ByRnS"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYHxXVW4yGg1",
        "outputId": "21f82d12-40a3-44fb-802b-cb5b5ef41caf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2UCAeYv1BHg"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOqu5uFAypVq",
        "outputId": "5288a0ed-e635-4308-b82d-ada171cf0909"
      },
      "source": [
        "#!python3 -m pip install coffea mplhep\n",
        "!pip install matplotlib==3.1.3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "#import mplhep as hep\n",
        "from pandas import set_option\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import h5py\n",
        "from sklearn.metrics import auc,make_scorer,fbeta_score,precision_score,recall_score,accuracy_score,log_loss,roc_auc_score,classification_report,f1_score,confusion_matrix,roc_curve,precision_recall_curve,average_precision_score\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib==3.1.3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3WH0l2x0Iyi"
      },
      "source": [
        "def do_scaler(X,Y, scaler_X, scaler_Y):\n",
        "    if scaler_X == None:\n",
        "        X_scaled = X\n",
        "    else:\n",
        "        X_scaled = scaler_X.fit_transform(X)\n",
        "    if scaler_Y == None:\n",
        "        Y_scaled = Y\n",
        "    else:\n",
        "        Y_scaled = scaler_Y.fit_transform(Y)\n",
        "    return X_scaled, Y_scaled, scaler_X, scaler_Y\n",
        "\n",
        "def test_model(X,Y, model, scaler_X = None, verbose = 1):\n",
        "    if scaler_X is not None:\n",
        "        X_scaled = scaler_X.transform(X)\n",
        "    else:\n",
        "        X_scaled = X\n",
        "    y_probs = model.predict_proba(X_scaled)  # calculate the probability\n",
        "    preds = model.predict(X_scaled)\n",
        "    prec, rec, thresh = precision_recall_curve(Y, y_probs[:,1])\n",
        "    bidx = np.argmax(prec * rec)\n",
        "    best_cut = thresh[bidx]\n",
        "    print(best_cut)\n",
        "\n",
        "    preds = y_probs[:,1] >= best_cut\n",
        "\n",
        "    if verbose == 1:\n",
        "\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(Y, y_probs[:,1], drop_intermediate=False)\n",
        "        print(f'AUC = {metrics.auc(fpr, tpr)}')\n",
        "        print(\"Purity in test sample     : {:2.2f}%\".format(100 * precision_score(Y, preds)))\n",
        "        print(\"Efficiency in test sample : {:2.2f}%\".format(100 * recall_score(Y, preds)))\n",
        "        print(\"Accuracy in test sample   : {:2.2f}%\".format(100 * accuracy_score(Y, preds)))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbzWCd_ny2l6",
        "outputId": "09ad154a-980a-4506-f01e-363f89c3cb0a"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/'\n",
        "#plt.style.use(hep.style.ROOT)\n",
        "\n",
        "def open_file_MC( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi       \n",
        "        return dataframe\n",
        "\n",
        "def open_file_DD( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]        \n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt',\n",
        "       'jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt',\n",
        "       'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'MultiRP1', 'MultiRP2',\n",
        "       'Mx', 'Yx', 'Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi       \n",
        "        return dataframe\n",
        "\n",
        "def open_file_Data( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]        \n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2','Mx', 'Yx', \n",
        "'Mww/Mx', 'Yww_Yx'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi        \n",
        "        return dataframe\n",
        "\n",
        "#select_columns = ['Mww', 'Pt_W_lep', 'jetAK8_pt','jetAK8_eta', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2','Mx', 'Mww/Mx']\n",
        "select_columns = ['Mww', 'Pt_W_lep', 'Acoplanaridade_jatos_MET', 'Acoplanaridade_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Mx', 'Mww/Mx']\n",
        "\n",
        "\n",
        "SM =       open_file_MC( PATH + 'DataSet_SM_multiRP.h5')\n",
        "ANOMALO1 = open_file_MC( PATH + 'DataSet_ANOMALO1_multiRP.h5')\n",
        "ANOMALO2 = open_file_MC( PATH + 'DataSet_ANOMALO2_multiRP.h5')\n",
        "ANOMALO3 = open_file_MC( PATH + 'DataSet_ANOMALO3_multiRP.h5')\n",
        "ANOMALO4 = open_file_MC( PATH + 'DataSet_ANOMALO4_multiRP.h5')\n",
        "ANOMALO5 = open_file_MC( PATH + 'DataSet_ANOMALO5_multiRP.h5')\n",
        "ANOMALO6 = open_file_MC( PATH + 'DataSet_ANOMALO6_multiRP.h5')\n",
        "ANOMALO7 = open_file_MC( PATH + 'DataSet_ANOMALO7_multiRP.h5')\n",
        "ANOMALO8 = open_file_MC( PATH + 'DataSet_ANOMALO8_multiRP.h5')\n",
        "\n",
        "label_signal1  = pd.DataFrame( [1]*len( ANOMALO1 ) )\n",
        "label_signal2  = pd.DataFrame( [1]*len( ANOMALO2 ) )\n",
        "label_signal3  = pd.DataFrame( [1]*len( ANOMALO3 ) )\n",
        "label_signal4  = pd.DataFrame( [1]*len( ANOMALO4 ) )\n",
        "label_signal5  = pd.DataFrame( [1]*len( ANOMALO5 ) )\n",
        "label_signal6  = pd.DataFrame( [1]*len( ANOMALO6 ) )\n",
        "label_signal7  = pd.DataFrame( [1]*len( ANOMALO7 ) )\n",
        "label_signal8  = pd.DataFrame( [1]*len( ANOMALO8 ) )\n",
        "label_signalSM = pd.DataFrame( [1]*len( SM ) )\n",
        "\n",
        "SM = pd.concat( [ SM, label_signalSM ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Standard Model --> ', SM.shape)\n",
        "\n",
        "ANOMALO1 = pd.concat( [ ANOMALO1, label_signal1 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 1 --> ', ANOMALO1.shape)\n",
        "\n",
        "ANOMALO2 = pd.concat( [ ANOMALO2, label_signal2 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 2 --> ', ANOMALO2.shape)\n",
        "\n",
        "ANOMALO3 = pd.concat( [ ANOMALO3, label_signal3 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 3 --> ', ANOMALO3.shape)\n",
        "\n",
        "ANOMALO4 = pd.concat( [ ANOMALO4, label_signal4 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 4 --> ', ANOMALO4.shape)\n",
        "\n",
        "ANOMALO5 = pd.concat( [ ANOMALO5, label_signal5 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 5 --> ', ANOMALO5.shape)\n",
        "\n",
        "ANOMALO6 = pd.concat( [ ANOMALO6, label_signal6 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 6 --> ', ANOMALO6.shape)\n",
        "\n",
        "ANOMALO7 = pd.concat( [ ANOMALO7, label_signal7 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 7 --> ', ANOMALO7.shape)\n",
        "\n",
        "ANOMALO8 = pd.concat( [ ANOMALO8, label_signal8 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 8 --> ', ANOMALO8.shape)\n",
        "\n",
        "data_set_back_multirp = open_file_DD( PATH + 'DataDriven_Background_multiRP.h5' )\n",
        "\n",
        "label_back = pd.DataFrame( [0]*len( data_set_back_multirp ) )\n",
        "data_set_back_multirp = pd.concat( [ data_set_back_multirp, label_back ], axis = 1 ).rename(columns={0: 'label'})\n",
        "\n",
        "Dataset_Signal_Back1  = pd.concat( [ ANOMALO1 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back2  = pd.concat( [ ANOMALO2 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back3  = pd.concat( [ ANOMALO3 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back4  = pd.concat( [ ANOMALO4 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back5  = pd.concat( [ ANOMALO5 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back6  = pd.concat( [ ANOMALO6 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back7  = pd.concat( [ ANOMALO7 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back8  = pd.concat( [ ANOMALO8 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "\n",
        "data_set_dados_multirp = open_file_Data( PATH + 'DataSet_dados_multiRP.h5' )\n",
        "data_set_dados_multirp = data_set_dados_multirp[select_columns]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.40\n",
        "DataSet_Train1_, DataSet_Test1_ = train_test_split( Dataset_Signal_Back1, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back1.label )\n",
        "DataSet_Train2_, DataSet_Test2_ = train_test_split( Dataset_Signal_Back2, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back2.label )\n",
        "DataSet_Train3_, DataSet_Test3_ = train_test_split( Dataset_Signal_Back3, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back3.label )\n",
        "DataSet_Train4_, DataSet_Test4_ = train_test_split( Dataset_Signal_Back4, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back4.label )\n",
        "DataSet_Train5_, DataSet_Test5_ = train_test_split( Dataset_Signal_Back5, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back5.label )\n",
        "DataSet_Train6_, DataSet_Test6_ = train_test_split( Dataset_Signal_Back6, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back6.label )\n",
        "DataSet_Train7_, DataSet_Test7_ = train_test_split( Dataset_Signal_Back7, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back7.label )\n",
        "DataSet_Train8_, DataSet_Test8_ = train_test_split( Dataset_Signal_Back8, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back8.label )\n",
        "\n",
        "\n",
        "DataSet_Train8 = DataSet_Train8_[select_columns] \n",
        "DataSet_Test8 = DataSet_Test8_[select_columns] \n",
        "\n",
        "DataSet_Train7 = DataSet_Train7_[select_columns] \n",
        "DataSet_Test7 = DataSet_Test7_[select_columns] \n",
        "\n",
        "DataSet_Train6 = DataSet_Train6_[select_columns] \n",
        "DataSet_Test6 = DataSet_Test6_[select_columns] \n",
        "\n",
        "DataSet_Train5 = DataSet_Train5_[select_columns] \n",
        "DataSet_Test5 = DataSet_Test5_[select_columns] \n",
        "\n",
        "DataSet_Train1 = DataSet_Train1_[select_columns] \n",
        "DataSet_Test1 = DataSet_Test1_[select_columns] \n",
        "\n",
        "DataSet_Train2 = DataSet_Train2_[select_columns] \n",
        "DataSet_Test2 = DataSet_Test2_[select_columns] \n",
        "\n",
        "DataSet_Train3 = DataSet_Train3_[select_columns] \n",
        "DataSet_Test3 = DataSet_Test3_[select_columns] \n",
        "\n",
        "DataSet_Train4 = DataSet_Train4_[select_columns] \n",
        "DataSet_Test4 = DataSet_Test4_[select_columns] \n",
        "\n",
        "y_train1 = DataSet_Train1_['label']\n",
        "y_test1  = DataSet_Test1_['label']\n",
        "\n",
        "y_train2 = DataSet_Train2_['label']\n",
        "y_test2  = DataSet_Test2_['label']\n",
        "\n",
        "y_train3 = DataSet_Train3_['label']\n",
        "y_test3  = DataSet_Test3_['label']\n",
        "\n",
        "y_train4 = DataSet_Train4_['label']\n",
        "y_test4  = DataSet_Test4_['label']\n",
        "\n",
        "y_train5 = DataSet_Train5_['label']\n",
        "y_test5  = DataSet_Test5_['label']\n",
        "\n",
        "y_train6 = DataSet_Train6_['label']\n",
        "y_test6  = DataSet_Test6_['label']\n",
        "\n",
        "y_train7 = DataSet_Train7_['label']\n",
        "y_test7  = DataSet_Test7_['label']\n",
        "\n",
        "y_train8 = DataSet_Train8_['label']\n",
        "y_test8  = DataSet_Test8_['label']\n",
        "\n",
        "print('--- Weight Anomalo 8 --- \\n')\n",
        "DataSet_Test8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==1]['weight'] \n",
        "DataSet_Test8_weight_backgr =DataSet_Test8_[DataSet_Test8_['label']==0]['weight'] \n",
        "DataSet_TestSM8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train8_weight_signal = DataSet_Train8_[DataSet_Train8_['label']==1]['weight']\n",
        "DataSet_Train8_weight_backgr = DataSet_Train8_[DataSet_Train8_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test8_weight =  DataSet_Test8_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test8_weight_signal.shape)\n",
        "print( 'Shape Test Background', DataSet_Test8_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 7 --- \\n')\n",
        "\n",
        "DataSet_Test7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==1]['weight'] \n",
        "DataSet_Test7_weight_backgr =  DataSet_Test7_[DataSet_Test7_['label']==0]['weight'] \n",
        "DataSet_TestSM7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train7_weight_signal =  DataSet_Train7_[DataSet_Train7_['label']==1]['weight']\n",
        "DataSet_Train7_weight_backgr =  DataSet_Train7_[DataSet_Train7_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test7_weight =  DataSet_Test7_['weight']\n",
        "\n",
        "print('--- Weight Anomalo 6 --- \\n')\n",
        "\n",
        "DataSet_Test6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==1]['weight'] \n",
        "DataSet_Test6_weight_backgr =  DataSet_Test6_[DataSet_Test6_['label']==0]['weight'] \n",
        "DataSet_TestSM6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train6_weight_signal =  DataSet_Train6_[DataSet_Train6_['label']==1]['weight']\n",
        "DataSet_Train6_weight_backgr =  DataSet_Train6_[DataSet_Train6_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test6_weight =  DataSet_Test6_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 5 --- \\n')\n",
        "\n",
        "DataSet_Test5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==1]['weight'] \n",
        "DataSet_Test5_weight_backgr =  DataSet_Test5_[DataSet_Test5_['label']==0]['weight'] \n",
        "DataSet_TestSM5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train5_weight_signal =  DataSet_Train5_[DataSet_Train5_['label']==1]['weight']\n",
        "DataSet_Train5_weight_backgr =  DataSet_Train5_[DataSet_Train5_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test5_weight =  DataSet_Test5_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 1 --- \\n')\n",
        "\n",
        "DataSet_Test1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==1]['weight'] \n",
        "DataSet_Test1_weight_backgr =  DataSet_Test1_[DataSet_Test1_['label']==0]['weight'] \n",
        "DataSet_TestSM1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train1_weight_signal =  DataSet_Train1_[DataSet_Train1_['label']==1]['weight']\n",
        "DataSet_Train1_weight_backgr = DataSet_Train1_[DataSet_Train1_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test1_weight =  DataSet_Test1_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test1_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test1_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 2 --- \\n')\n",
        "\n",
        "DataSet_Test2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==1]['weight'] \n",
        "DataSet_Test2_weight_backgr =  DataSet_Test2_[DataSet_Test2_['label']==0]['weight'] \n",
        "DataSet_TestSM2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train2_weight_signal =  DataSet_Train2_[DataSet_Train2_['label']==1]['weight']\n",
        "DataSet_Train2_weight_backgr = DataSet_Train2_[DataSet_Train2_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test2_weight =  DataSet_Test2_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test2_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test2_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 3 --- \\n')\n",
        "\n",
        "DataSet_Test3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==1]['weight'] \n",
        "DataSet_Test3_weight_backgr =  DataSet_Test3_[DataSet_Test3_['label']==0]['weight'] \n",
        "DataSet_TestSM3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train3_weight_signal =  DataSet_Train3_[DataSet_Train3_['label']==1]['weight']\n",
        "DataSet_Train3_weight_backgr = DataSet_Train3_[DataSet_Train3_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test3_weight =  DataSet_Test3_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test3_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test3_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 4 --- \\n')\n",
        "\n",
        "DataSet_Test4_weight_signal = DataSet_Test4_[DataSet_Test4_['label']==1]['weight'] \n",
        "DataSet_Test4_weight_backgr = DataSet_Test4_[DataSet_Test4_['label']==0]['weight'] \n",
        "DataSet_TestSM4_weight_signal =  DataSet_Test4_[DataSet_Test4_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train4_weight_signal = DataSet_Train4_[DataSet_Train4_['label']==1]['weight']\n",
        "DataSet_Train4_weight_backgr = DataSet_Train4_[DataSet_Train4_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test4_weight =  DataSet_Test4_['weight']\n",
        "\n",
        "print( 'Shape Test Signal' , DataSet_Test4_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test4_weight_backgr.shape)        "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape for Standard Model -->  (112, 24)\n",
            "Shape for Anomalo 1 -->  (1544, 24)\n",
            "Shape for Anomalo 2 -->  (174, 24)\n",
            "Shape for Anomalo 3 -->  (434, 24)\n",
            "Shape for Anomalo 4 -->  (742, 24)\n",
            "Shape for Anomalo 5 -->  (169, 24)\n",
            "Shape for Anomalo 6 -->  (299, 24)\n",
            "Shape for Anomalo 7 -->  (672, 24)\n",
            "Shape for Anomalo 8 -->  (1627, 24)\n",
            "--- Weight Anomalo 8 --- \n",
            "\n",
            "Shape Test Signal (696,)\n",
            "Shape Test Background (166938,) \n",
            "\n",
            "--- Weight Anomalo 7 --- \n",
            "\n",
            "--- Weight Anomalo 6 --- \n",
            "\n",
            "Shape Test Signal (314,)\n",
            "Shape Test Background (166938,) \n",
            "\n",
            "--- Weight Anomalo 5 --- \n",
            "\n",
            "Shape Test Signal (314,)\n",
            "Shape Test Background (166938,) \n",
            "\n",
            "--- Weight Anomalo 1 --- \n",
            "\n",
            "Shape Test Signal (662,)\n",
            "Shape Test Background (166939,) \n",
            "\n",
            "--- Weight Anomalo 2 --- \n",
            "\n",
            "Shape Test Signal (114,)\n",
            "Shape Test Background (166939,) \n",
            "\n",
            "--- Weight Anomalo 3 --- \n",
            "\n",
            "Shape Test Signal (218,)\n",
            "Shape Test Background (166939,) \n",
            "\n",
            "--- Weight Anomalo 4 --- \n",
            "\n",
            "Shape Test Signal (342,)\n",
            "Shape Test Background (166938,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-_gSpDSy36V"
      },
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "scaler_prediction = None\n",
        "\n",
        "# scaler only on the input\n",
        "X_train_norm1, Y_train_norm1, scaler, scaler_prediction = do_scaler(DataSet_Train1, y_train1, scaler, scaler_prediction)\n",
        "X_train_norm2, Y_train_norm2, scaler, scaler_prediction = do_scaler(DataSet_Train2, y_train2, scaler, scaler_prediction)\n",
        "X_train_norm3, Y_train_norm3, scaler, scaler_prediction = do_scaler(DataSet_Train3, y_train3, scaler, scaler_prediction)\n",
        "X_train_norm4, Y_train_norm4, scaler, scaler_prediction = do_scaler(DataSet_Train4, y_train4, scaler, scaler_prediction)\n",
        "X_train_norm5, Y_train_norm5, scaler, scaler_prediction = do_scaler(DataSet_Train5, y_train5, scaler, scaler_prediction)\n",
        "X_train_norm6, Y_train_norm6, scaler, scaler_prediction = do_scaler(DataSet_Train6, y_train6, scaler, scaler_prediction)\n",
        "X_train_norm7, Y_train_norm7, scaler, scaler_prediction = do_scaler(DataSet_Train7, y_train7, scaler, scaler_prediction)\n",
        "X_train_norm8, Y_train_norm8, scaler, scaler_prediction = do_scaler(DataSet_Train8, y_train8, scaler, scaler_prediction)\n",
        "\n",
        "X_test_norm1, Y_test_norm1, scaler, scaler_prediction = do_scaler(DataSet_Test1, y_test1, scaler, scaler_prediction)\n",
        "X_test_norm2, Y_test_norm2, scaler, scaler_prediction = do_scaler(DataSet_Test2, y_test2, scaler, scaler_prediction)\n",
        "X_test_norm3, Y_test_norm3, scaler, scaler_prediction = do_scaler(DataSet_Test3, y_test3, scaler, scaler_prediction)\n",
        "X_test_norm4, Y_test_norm4, scaler, scaler_prediction = do_scaler(DataSet_Test4, y_test4, scaler, scaler_prediction)\n",
        "X_test_norm5, Y_test_norm5, scaler, scaler_prediction = do_scaler(DataSet_Test5, y_test5, scaler, scaler_prediction)\n",
        "X_test_norm6, Y_test_norm6, scaler, scaler_prediction = do_scaler(DataSet_Test6, y_test6, scaler, scaler_prediction)\n",
        "X_test_norm7, Y_test_norm7, scaler, scaler_prediction = do_scaler(DataSet_Test7, y_test7, scaler, scaler_prediction)\n",
        "X_test_norm8, Y_test_norm8, scaler, scaler_prediction = do_scaler(DataSet_Test8, y_test8, scaler, scaler_prediction)\n",
        "\n",
        "X_dados_norm = scaler.fit_transform( data_set_dados_multirp )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUdxCK7MIrII"
      },
      "source": [
        "class_weight1 = { 0: (Y_train_norm1.value_counts()[0] / Y_train_norm1.value_counts()[1]),\n",
        "                     1: (Y_train_norm1.value_counts()[1] / Y_train_norm1.value_counts()[0])}\n",
        "class_weight2 = { 0: (Y_train_norm2.value_counts()[0] / Y_train_norm2.value_counts()[1]),\n",
        "                     1: (Y_train_norm2.value_counts()[1] / Y_train_norm2.value_counts()[0])}\n",
        "class_weight1 = { 0: (Y_train_norm1.value_counts()[0] / Y_train_norm1.value_counts()[1]),\n",
        "                     1: (Y_train_norm1.value_counts()[1] / Y_train_norm1.value_counts()[0])}  \n",
        "class_weight1 = { 0: (Y_train_norm1.value_counts()[0] / Y_train_norm1.value_counts()[1]),\n",
        "                     1: (Y_train_norm1.value_counts()[1] / Y_train_norm1.value_counts()[0])}\n",
        "class_weight1 = { 0: (Y_train_norm1.value_counts()[0] / Y_train_norm1.value_counts()[1]),\n",
        "                     1: (Y_train_norm1.value_counts()[1] / Y_train_norm1.value_counts()[0])}\n",
        "class_weight1 = { 0: (Y_train_norm1.value_counts()[0] / Y_train_norm1.value_counts()[1]),\n",
        "                     1: (Y_train_norm1.value_counts()[1] / Y_train_norm1.value_counts()[0])}                                                                                                       "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osn4rbqy2bkr"
      },
      "source": [
        "\n",
        "model_number = 4\n",
        "# Create Model\n",
        "if model_number == 1:\n",
        "    model_name = \"RF\"\n",
        "    # Random Forest / Best Result - AUC = 0.8663\n",
        "    param_search = {\n",
        "        'RF__n_estimators': [100, 200, 300, 400],\n",
        "        'RF__max_depth': list(range(4,8))\n",
        "\n",
        "    }\n",
        "    # create pipeline\n",
        "    estimators = []\n",
        "    estimators.append(('RF', RandomForestClassifier()))\n",
        "    model = Pipeline(estimators)\n",
        "elif model_number == 2:\n",
        "    model_name = \"ADAB\"\n",
        "    param_search = {\"ADAB__base_estimator__max_depth\": [1, 2, 3, 4, 5],\n",
        "                  \"ADAB__n_estimators\": [700, 800, 900]\n",
        "                  }\n",
        "\n",
        "    DTC = DecisionTreeClassifier(max_features=\"auto\", class_weight=\"balanced\", max_depth=None)\n",
        "\n",
        "    # create pipeline\n",
        "    estimators = []\n",
        "    estimators.append(('ADAB', AdaBoostClassifier(base_estimator=DTC)))\n",
        "    model = Pipeline(estimators)\n",
        "elif model_number == 3:\n",
        "    model_name = \"XGB\"\n",
        "    param_search = {'XGB__max_depth': [2, 3, 4, 5],\n",
        "                  'XGB__n_estimators': [200, 400, 600, 800],\n",
        "                  'XGB__learning_rate': [0.15, 0.2],\n",
        "                  'XGB__gamma': [0, 0.1]\n",
        "                  }\n",
        "\n",
        "    # create pipeline\n",
        "    estimators = []\n",
        "    #estimators.append(('standardize', StandardScaler()))\n",
        "    estimators.append(('XGB', XGBClassifier( booster='gbtree')))\n",
        "    model = Pipeline(estimators)\n",
        "elif model_number == 4:\n",
        "    # Lightgbm\n",
        "    model_name = \"LGBM\"\n",
        "    param_search = {'LGB__num_leaves': [24,128,62],\n",
        "                 'LGB__n_estimators': [ 1000, 2000],\n",
        "                 'LGB__learning_rate': [0.0005,0.0001],\n",
        "                 'LGB__min_child_samples': [100,500],\n",
        "                 'LGB__colsample_bytree': [ 0.8, 0.9],\n",
        "                 'LGB__subsample': [ 0.2, 0.3],\n",
        "                 'LGB__boosting_type': ['gbdt'],\n",
        "                 'LGB__class_weight': ['balanced', None]\n",
        "                 }\n",
        "\n",
        "\n",
        "    # create pipeline\n",
        "    estimators = []\n",
        "    #estimators.append(('standardize', StandardScaler()))\n",
        "    estimators.append(('LGB', LGBMClassifier( max_depth=0 )))\n",
        "    model = Pipeline(estimators)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUkjXmX9owW"
      },
      "source": [
        "from joblib import dump, load\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-pE9HINIvUM"
      },
      "source": [
        "def make_model(X_train_norm,Y_train_norm,param_search,filename):\n",
        "    n_iter = 8\n",
        "    cv = 2\n",
        "    #scoring = make_scorer(fbeta_score, beta=0.5)\n",
        "    #scoring = 'roc_auc'\n",
        "    scoring = 'f1'\n",
        "    #scoring = None\n",
        "    time_s_ = time.time()\n",
        "    #search object\n",
        "\n",
        "    search = GridSearchCV(estimator=model, param_grid=param_search, scoring = scoring ,cv=cv, verbose=1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    search_result = search.fit(X_train_norm, Y_train_norm)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    filename = filename\n",
        "    dump(search_result, '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/Binary_BDT_'+filename+'.joblib')\n",
        "    # Results\n",
        "    print(\"Model: %s Best: %f using %s\" % (model_name, search_result.best_score_, search_result.best_params_))\n",
        "    means = search_result.cv_results_['mean_test_score']\n",
        "    stds = search_result.cv_results_['std_test_score']\n",
        "    params = search_result.cv_results_['params']\n",
        "    time_e_ = time.time()\n",
        "    print ( \"Total time elapsed: {:.0f}\".format( time_e_ - time_s_ ) )\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "    return search_result   "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDniYiM1Fue_"
      },
      "source": [
        "def test_model( X_test, Y_test, X_train, Y_train, model, label, verbose = 1 ):\n",
        "  \n",
        "    print(' ---------------- ' + label + '---------------- \\n' )\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_test = scaler.fit_transform(X_test)\n",
        "    X_scaled_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    y_probs_test = model.predict_proba(X_scaled_test)  # calculate the probability\n",
        "    prec_test, rec_test, thresh_test = precision_recall_curve(Y_test, y_probs_test[:,1])\n",
        "    bidx_test = np.argmax(prec_test * rec_test)\n",
        "    best_cut_test = thresh_test[bidx_test]\n",
        "\n",
        "    preds_test = y_probs_test[:,1] >= best_cut_test\n",
        "\n",
        "    y_probs_train = model.predict_proba(X_scaled_train)  # calculate the probability\n",
        "    prec_train, rec_train, thresh_train = precision_recall_curve(Y_train, y_probs_train[:,1])\n",
        "    bidx_train = np.argmax(prec_train * rec_train)\n",
        "    best_cut_train = thresh_train[bidx_train]\n",
        "    \n",
        "\n",
        "    preds_train = y_probs_train[:,1] >= best_cut_train\n",
        "\n",
        "    if verbose == 1:\n",
        "\n",
        "        fpr_test, tpr_test, thresholds_test = roc_curve(Y_test, y_probs_test[:,1], drop_intermediate=False)\n",
        "        print('best_cut_test-->', best_cut_test)\n",
        "        print(f'AUC = {auc(fpr_test, tpr_test)}')\n",
        "        print(\"Purity in test sample     : {:2.2f}%\".format(100 * precision_score(Y_test, preds_test)))\n",
        "        print(\"Efficiency in test sample : {:2.2f}%\".format(100 * recall_score(Y_test, preds_test)))\n",
        "        print(\"Accuracy in test sample   : {:2.2f}%\".format(100 * accuracy_score(Y_test, preds_test)))\n",
        "        print( \"F1_score in test sample  : {:2.2f}%\".format(100 * f1_score(Y_test, preds_test)))\n",
        "        print(\"\\n\")\n",
        "        fpr_train, tpr_train, thresholds_train = roc_curve(Y_train, y_probs_train[:,1], drop_intermediate=False)\n",
        "        print('best_cut_train-->', best_cut_train)\n",
        "        print(f'AUC = {auc(fpr_train, tpr_train)}')\n",
        "        print(\"Purity in train sample     : {:2.2f}%\".format(100 * precision_score(Y_train, preds_train)))\n",
        "        print(\"Efficiency in train sample : {:2.2f}%\".format(100 * recall_score(Y_train, preds_train)))\n",
        "        print(\"Accuracy in train sample   : {:2.2f}%\".format(100 * accuracy_score(Y_train, preds_train)))\n",
        "        print( \"F1_score in train sample  : {:2.2f}%\".format(100 * f1_score(Y_train, preds_train)))        \n",
        "    return y_probs_test[:,1], best_cut_test, preds_test\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhujVH7XEb0I"
      },
      "source": [
        "def Volta_Lula( X_train_norm, Y_train_norm, DataSet_Test, y_test, DataSet_Train, y_train,DataSet_Test_weight_signal, DataSet_Test_weight_backgr, label_anomalo, label_save_model ):\n",
        "    plt.figure(figsize=(14,8))\n",
        "    search_result = make_model( X_train_norm,Y_train_norm, param_search,label_save_model)\n",
        "    y_probs,best_cut, preds_test = test_model( DataSet_Test, y_test, DataSet_Train, y_train, search_result, label_anomalo)\n",
        "    bins = 25\n",
        "\n",
        "    predict_dados = search_result.predict_proba(np.array(X_dados_norm))[:,1]\n",
        "\n",
        "    counts_, bin_edges_ = np.histogram( predict_dados[ predict_dados < best_cut], bins = np.linspace(0,1,bins) )\n",
        "    errors_ = np.sqrt( counts_ )\n",
        "    bin_centres_ = ( bin_edges_[:-1] + bin_edges_[1:] ) / 2.\n",
        "    plt.errorbar( bin_centres_, counts_, yerr=errors_, xerr=abs(bin_centres_[1] - bin_centres_[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "    marcia = plt.hist(y_probs[y_test == 0], bins = np.linspace(0,1,bins), weights = DataSet_Test_weight_backgr / test_size, label = 'Background' )\n",
        "    saco = np.max(marcia[0])\n",
        "    plt.hist(y_probs[y_test == 1], bins = np.linspace(0,1,bins), histtype = 'step', weights = DataSet_Test_weight_signal / test_size, label = label_anomalo)\n",
        "    plt.plot( [best_cut,best_cut],[0,saco],  \"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut ) ) \n",
        "    plt.legend( loc = \"upper right\", fontsize = 16 )\n",
        "    plt.text(best_cut+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "    plt.yscale('log')\n",
        "\n",
        "    n_events_back_after_cut = y_probs[ y_test == 0 ] > best_cut\n",
        "    n_events_signal_after_cut = y_probs[ y_test == 1 ] > best_cut\n",
        "    n_events_SM_after_cut = y_probs[ y_test == 2 ] > best_cut\n",
        "    n_eventos_Data_after_cut = predict_dados[ predict_dados >= best_cut ]\n",
        "\n",
        "    print(' ---------------' +label_anomalo+' --------------- ')\n",
        "    print('Numero de eventos de background depois do corte -->', DataSet_Test_weight_backgr[ n_events_back_after_cut ].sum() / test_size )\n",
        "    print('Numero de eventos de signal depois do corte -->', DataSet_Test_weight_signal[ n_events_signal_after_cut].sum() / test_size )\n",
        "    print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut ) )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k6sq3DXpsbQ",
        "outputId": "f5895e8d-e6dc-4aa6-d998-3113f92543a8"
      },
      "source": [
        "Volta_Lula( X_train_norm1, Y_train_norm1, DataSet_Test1, y_test1, DataSet_Train1, y_train1,DataSet_Test1_weight_signal, DataSet_Test1_weight_backgr, r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', 'anomalo1_SM_como_signal_cut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 192 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDrfxzTUto2"
      },
      "source": [
        "Volta_Lula( X_train_norm2, Y_train_norm2, DataSet_Test2, y_test2, DataSet_Train2, y_train2,DataSet_Test2_weight_signal, DataSet_Test2_weight_backgr, r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', 'anomalo2_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Jrygy8wRpv"
      },
      "source": [
        "Volta_Lula( X_train_norm3, Y_train_norm3, DataSet_Test3, y_test3, DataSet_Train3, y_train3,DataSet_Test3_weight_signal, DataSet_Test3_weight_backgr, r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', 'anomalo3_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raFuSr734Viu"
      },
      "source": [
        "Volta_Lula( X_train_norm4, Y_train_norm4, DataSet_Test4, y_test4, DataSet_Train4, y_train4,DataSet_Test4_weight_signal, DataSet_Test4_weight_backgr, r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', 'anomalo4_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0P-63lVLINX"
      },
      "source": [
        "Volta_Lula( X_train_norm5, Y_train_norm5, DataSet_Test5, y_test5, DataSet_Train5, y_train5,DataSet_Test5_weight_signal, DataSet_Test5_weight_backgr, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=0.5 \\times 10^{-5}$', 'anomalo5_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71LCWpbkLZto"
      },
      "source": [
        "Volta_Lula( X_train_norm6, Y_train_norm6, DataSet_Test6, y_test6, DataSet_Train6, y_train6,DataSet_Test6_weight_signal, DataSet_Test6_weight_backgr, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=1.0 \\times 10^{-6}$', 'anomalo6_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55f-l_TLjjk"
      },
      "source": [
        "Volta_Lula( X_train_norm7, Y_train_norm7, DataSet_Test7, y_test7, DataSet_Train7, y_train7,DataSet_Test7_weight_signal, DataSet_Test7_weight_backgr, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=2.0 \\times 10^{-6}$', 'anomalo7_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlz_S0ANLqcf"
      },
      "source": [
        "Volta_Lula( X_train_norm8, Y_train_norm8, DataSet_Test8, y_test8, DataSet_Train8, y_train8,DataSet_Test8_weight_signal, DataSet_Test8_weight_backgr, r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=5.0 \\times 10^{-6}$', 'anomalo8_SM_como_signalcut_Mww600' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1-uRr2_GpGr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}