{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDT_Output_Multiclass_DataDriven.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMTEFbWY/NnIOkc+yMHYqKL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teteumac/Acoplamento-Quartico-Anomalo/blob/main/BDT_Output_Multiclass_DataDriven.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLK7VcYBIpMv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZHyv0IvOem2"
      },
      "source": [
        "!python3 -m pip install coffea mplhep\n",
        "!pip install matplotlib==3.1.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzsKsz1WIMLv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import mplhep as hep\n",
        "from pandas import set_option\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import h5py\n",
        "from sklearn.metrics import auc,make_scorer,fbeta_score,precision_score,recall_score,accuracy_score,log_loss,roc_auc_score,classification_report,f1_score,confusion_matrix,roc_curve,precision_recall_curve,average_precision_score\n",
        "import math\n",
        "from joblib import dump,load\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_-vgZlrNyZ-"
      },
      "source": [
        "data_set_back_multirp = open_file_DD( PATH + 'DataDriven_Background_multiRP.h5' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mfwp0PIIrhC"
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/Colab Notebooks/output_for_LGBM/'\n",
        "plt.style.use(hep.style.ROOT)\n",
        "\n",
        "def open_file_MC( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi \n",
        "        return dataframe\n",
        "\n",
        "def open_file_DD( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'PUWeight', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi \n",
        "        return dataframe         \n",
        "\n",
        "def open_file_Data( file ):\n",
        "    df = None\n",
        "    with h5py.File( file , 'r' ) as f:\n",
        "        dset = f[ 'dados' ]\n",
        "        array = np.array( dset ) \n",
        "        array_cut = (array[:,0] > 600) #& (array[:,1] >= 200) & (array[:,2] >= 2) & (array[:,3] >= 2) & (array[:,4] >= 200) & (array[:,5] <= 2.4) & (array[:,7] <=0.6) & (array[:,8] >= 40) & (array[:,9] >= 53)  & (array[:,10] <= 2.4)\n",
        "        DataSet_ = array[array_cut]\n",
        "        dataframe = pd.DataFrame( DataSet_ , columns = ['Mww', 'Pt_W_lep', 'dPhi_Whad_Wlep', 'dPhi_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass',\n",
        "'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Yww', 'xi1', 'xi2', 'Mx', 'Yx','Mww/Mx', 'Yww_Yx', 'weight'] )\n",
        "        dataframe['Acoplanaridade_Whad_Wlep'] = 1 - dataframe['dPhi_Whad_Wlep'].abs()/math.pi\n",
        "        dataframe['Acoplanaridade_jatos_MET'] = 1 - dataframe['dPhi_jatos_MET'].abs()/math.pi \n",
        "        return dataframe       \n",
        "\n",
        "\n",
        "select_columns = ['Mww', 'Pt_W_lep', 'Acoplanaridade_Whad_Wlep', 'Acoplanaridade_jatos_MET', 'jetAK8_pt','jetAK8_eta', 'jetAK8_prunedMass', 'jetAK8_tau21', 'METPt', 'muon_pt', 'muon_eta', 'ExtraTracks', 'Mx',  'Mww/Mx']\n",
        "\n",
        "SM =       open_file_MC( PATH + 'DataSet_SM_multiRP.h5')\n",
        "ANOMALO1 = open_file_MC( PATH + 'DataSet_ANOMALO1_multiRP.h5')\n",
        "ANOMALO2 = open_file_MC( PATH + 'DataSet_ANOMALO2_multiRP.h5')\n",
        "ANOMALO3 = open_file_MC( PATH + 'DataSet_ANOMALO3_multiRP.h5')\n",
        "ANOMALO4 = open_file_MC( PATH + 'DataSet_ANOMALO4_multiRP.h5')\n",
        "ANOMALO5 = open_file_MC( PATH + 'DataSet_ANOMALO5_multiRP.h5')\n",
        "ANOMALO6 = open_file_MC( PATH + 'DataSet_ANOMALO6_multiRP.h5')\n",
        "ANOMALO7 = open_file_MC( PATH + 'DataSet_ANOMALO7_multiRP.h5')\n",
        "ANOMALO8 = open_file_MC( PATH + 'DataSet_ANOMALO8_multiRP.h5')\n",
        "\n",
        "label_signal1  = pd.DataFrame( [1]*len( ANOMALO1 ) )\n",
        "label_signal2  = pd.DataFrame( [1]*len( ANOMALO2 ) )\n",
        "label_signal3  = pd.DataFrame( [1]*len( ANOMALO3 ) )\n",
        "label_signal4  = pd.DataFrame( [1]*len( ANOMALO4 ) )\n",
        "label_signal5  = pd.DataFrame( [1]*len( ANOMALO5 ) )\n",
        "label_signal6  = pd.DataFrame( [1]*len( ANOMALO6 ) )\n",
        "label_signal7  = pd.DataFrame( [1]*len( ANOMALO7 ) )\n",
        "label_signal8  = pd.DataFrame( [1]*len( ANOMALO8 ) )\n",
        "label_signalSM = pd.DataFrame( [2]*len( SM ) )\n",
        "\n",
        "SM = pd.concat( [ SM, label_signalSM ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Standard Model --> ', SM.shape)\n",
        "\n",
        "ANOMALO1 = pd.concat( [ ANOMALO1, label_signal1 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 1 --> ', ANOMALO1.shape)\n",
        "\n",
        "ANOMALO2 = pd.concat( [ ANOMALO2, label_signal2 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 2 --> ', ANOMALO2.shape)\n",
        "\n",
        "ANOMALO3 = pd.concat( [ ANOMALO3, label_signal3 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 3 --> ', ANOMALO3.shape)\n",
        "\n",
        "ANOMALO4 = pd.concat( [ ANOMALO4, label_signal4 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 4 --> ', ANOMALO4.shape)\n",
        "\n",
        "ANOMALO5 = pd.concat( [ ANOMALO5, label_signal5 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 5 --> ', ANOMALO5.shape)\n",
        "\n",
        "ANOMALO6 = pd.concat( [ ANOMALO6, label_signal6 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 6 --> ', ANOMALO6.shape)\n",
        "\n",
        "ANOMALO7 = pd.concat( [ ANOMALO7, label_signal7 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 7 --> ', ANOMALO7.shape)\n",
        "\n",
        "ANOMALO8 = pd.concat( [ ANOMALO8, label_signal8 ], axis = 1 ).rename(columns={0: 'label'})\n",
        "print('Shape for Anomalo 8 --> ', ANOMALO8.shape)\n",
        "\n",
        "data_set_back_multirp = open_file_DD( PATH + 'DataDriven_Background_multiRP.h5' )\n",
        "\n",
        "label_back = pd.DataFrame( [0]*len( data_set_back_multirp ) )\n",
        "data_set_back_multirp = pd.concat( [ data_set_back_multirp, label_back ], axis = 1 ).rename(columns={0: 'label'})\n",
        "data_set_back_multirp\n",
        "\n",
        "Dataset_Signal_Back1  = pd.concat( [ ANOMALO1 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back2  = pd.concat( [ ANOMALO2 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back3  = pd.concat( [ ANOMALO3 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back4  = pd.concat( [ ANOMALO4 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back5  = pd.concat( [ ANOMALO5 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back6  = pd.concat( [ ANOMALO6 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back7  = pd.concat( [ ANOMALO7 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "Dataset_Signal_Back8  = pd.concat( [ ANOMALO8 , data_set_back_multirp, SM  ], axis = 0, sort = False )\n",
        "\n",
        "data_set_dados_multirp = open_file_Data( PATH + 'DataSet_dados_multiRP.h5' )\n",
        "data_set_dados_multirp = data_set_dados_multirp[select_columns]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.35\n",
        "DataSet_Train1_, DataSet_Test1_ = train_test_split( Dataset_Signal_Back1, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back1.label )\n",
        "DataSet_Train2_, DataSet_Test2_ = train_test_split( Dataset_Signal_Back2, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back2.label )\n",
        "DataSet_Train3_, DataSet_Test3_ = train_test_split( Dataset_Signal_Back3, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back3.label )\n",
        "DataSet_Train4_, DataSet_Test4_ = train_test_split( Dataset_Signal_Back4, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back4.label )\n",
        "DataSet_Train5_, DataSet_Test5_ = train_test_split( Dataset_Signal_Back5, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back5.label )\n",
        "DataSet_Train6_, DataSet_Test6_ = train_test_split( Dataset_Signal_Back6, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back6.label )\n",
        "DataSet_Train7_, DataSet_Test7_ = train_test_split( Dataset_Signal_Back7, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back7.label )\n",
        "DataSet_Train8_, DataSet_Test8_ = train_test_split( Dataset_Signal_Back8, test_size = test_size, random_state = 42, stratify = Dataset_Signal_Back8.label )\n",
        "\n",
        "\n",
        "DataSet_Train8 = DataSet_Train8_[select_columns] \n",
        "DataSet_Test8 = DataSet_Test8_[select_columns] \n",
        "\n",
        "DataSet_Train7 = DataSet_Train7_[select_columns] \n",
        "DataSet_Test7 = DataSet_Test7_[select_columns] \n",
        "\n",
        "DataSet_Train6 = DataSet_Train6_[select_columns] \n",
        "DataSet_Test6 = DataSet_Test6_[select_columns] \n",
        "\n",
        "DataSet_Train5 = DataSet_Train5_[select_columns] \n",
        "DataSet_Test5 = DataSet_Test5_[select_columns] \n",
        "\n",
        "DataSet_Train1 = DataSet_Train1_[select_columns] \n",
        "DataSet_Test1 = DataSet_Test1_[select_columns] \n",
        "\n",
        "DataSet_Train2 = DataSet_Train2_[select_columns] \n",
        "DataSet_Test2 = DataSet_Test2_[select_columns] \n",
        "\n",
        "DataSet_Train3 = DataSet_Train3_[select_columns] \n",
        "DataSet_Test3 = DataSet_Test3_[select_columns] \n",
        "\n",
        "DataSet_Train4 = DataSet_Train4_[select_columns] \n",
        "DataSet_Test4 = DataSet_Test4_[select_columns] \n",
        "\n",
        "y_train1 = DataSet_Train1_['label']\n",
        "y_test1  = DataSet_Test1_['label']\n",
        "\n",
        "y_train2 = DataSet_Train2_['label']\n",
        "y_test2  = DataSet_Test2_['label']\n",
        "\n",
        "y_train3 = DataSet_Train3_['label']\n",
        "y_test3  = DataSet_Test3_['label']\n",
        "\n",
        "y_train4 = DataSet_Train4_['label']\n",
        "y_test4  = DataSet_Test4_['label']\n",
        "\n",
        "y_train5 = DataSet_Train5_['label']\n",
        "y_test5  = DataSet_Test5_['label']\n",
        "\n",
        "y_train6 = DataSet_Train6_['label']\n",
        "y_test6  = DataSet_Test6_['label']\n",
        "\n",
        "y_train7 = DataSet_Train7_['label']\n",
        "y_test7  = DataSet_Test7_['label']\n",
        "\n",
        "y_train8 = DataSet_Train8_['label']\n",
        "y_test8  = DataSet_Test8_['label']\n",
        "\n",
        "print('--- Weight Anomalo 8 --- \\n')\n",
        "DataSet_Test8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==1]['weight'] \n",
        "DataSet_Test8_weight_backgr =DataSet_Test8_[DataSet_Test8_['label']==0]['weight'] \n",
        "DataSet_TestSM8_weight_signal =  DataSet_Test8_[DataSet_Test8_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train8_weight_signal = DataSet_Train8_[DataSet_Train8_['label']==1]['weight']\n",
        "DataSet_Train8_weight_backgr = DataSet_Train8_[DataSet_Train8_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test8_weight =  DataSet_Test8_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test8_weight_signal.shape)\n",
        "print( 'Shape Test Background', DataSet_Test8_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 7 --- \\n')\n",
        "\n",
        "DataSet_Test7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==1]['weight'] \n",
        "DataSet_Test7_weight_backgr =  DataSet_Test7_[DataSet_Test7_['label']==0]['weight'] \n",
        "DataSet_TestSM7_weight_signal =  DataSet_Test7_[DataSet_Test7_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train7_weight_signal =  DataSet_Train7_[DataSet_Train7_['label']==1]['weight']\n",
        "DataSet_Train7_weight_backgr =  DataSet_Train7_[DataSet_Train7_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test7_weight =  DataSet_Test7_['weight']\n",
        "\n",
        "print('--- Weight Anomalo 6 --- \\n')\n",
        "\n",
        "DataSet_Test6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==1]['weight'] \n",
        "DataSet_Test6_weight_backgr =  DataSet_Test6_[DataSet_Test6_['label']==0]['weight'] \n",
        "DataSet_TestSM6_weight_signal =  DataSet_Test6_[DataSet_Test6_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train6_weight_signal =  DataSet_Train6_[DataSet_Train6_['label']==1]['weight']\n",
        "DataSet_Train6_weight_backgr =  DataSet_Train6_[DataSet_Train6_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test6_weight =  DataSet_Test6_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 5 --- \\n')\n",
        "\n",
        "DataSet_Test5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==1]['weight'] \n",
        "DataSet_Test5_weight_backgr =  DataSet_Test5_[DataSet_Test5_['label']==0]['weight'] \n",
        "DataSet_TestSM5_weight_signal =  DataSet_Test5_[DataSet_Test5_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train5_weight_signal =  DataSet_Train5_[DataSet_Train5_['label']==1]['weight']\n",
        "DataSet_Train5_weight_backgr =  DataSet_Train5_[DataSet_Train5_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test5_weight =  DataSet_Test5_['weight']\n",
        "\n",
        "print( 'Shape Test Signal',  DataSet_Test7_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test7_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 1 --- \\n')\n",
        "\n",
        "DataSet_Test1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==1]['weight'] \n",
        "DataSet_Test1_weight_backgr =  DataSet_Test1_[DataSet_Test1_['label']==0]['weight'] \n",
        "DataSet_TestSM1_weight_signal =  DataSet_Test1_[DataSet_Test1_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train1_weight_signal =  DataSet_Train1_[DataSet_Train1_['label']==1]['weight']\n",
        "DataSet_Train1_weight_backgr = DataSet_Train1_[DataSet_Train1_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test1_weight =  DataSet_Test1_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test1_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test1_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 2 --- \\n')\n",
        "\n",
        "DataSet_Test2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==1]['weight'] \n",
        "DataSet_Test2_weight_backgr =  DataSet_Test2_[DataSet_Test2_['label']==0]['weight'] \n",
        "DataSet_TestSM2_weight_signal =  DataSet_Test2_[DataSet_Test2_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train2_weight_signal =  DataSet_Train2_[DataSet_Train2_['label']==1]['weight']\n",
        "DataSet_Train2_weight_backgr = DataSet_Train2_[DataSet_Train2_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test2_weight =  DataSet_Test2_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test2_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test2_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 3 --- \\n')\n",
        "\n",
        "DataSet_Test3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==1]['weight'] \n",
        "DataSet_Test3_weight_backgr =  DataSet_Test3_[DataSet_Test3_['label']==0]['weight'] \n",
        "DataSet_TestSM3_weight_signal =  DataSet_Test3_[DataSet_Test3_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train3_weight_signal =  DataSet_Train3_[DataSet_Train3_['label']==1]['weight']\n",
        "DataSet_Train3_weight_backgr = DataSet_Train3_[DataSet_Train3_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test3_weight =  DataSet_Test3_['weight']\n",
        "\n",
        "print( 'Shape Test Signal', DataSet_Test3_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test3_weight_backgr.shape, '\\n')\n",
        "\n",
        "print('--- Weight Anomalo 4 --- \\n')\n",
        "\n",
        "DataSet_Test4_weight_signal = DataSet_Test4_[DataSet_Test4_['label']==1]['weight'] \n",
        "DataSet_Test4_weight_backgr = DataSet_Test4_[DataSet_Test4_['label']==0]['weight'] \n",
        "DataSet_TestSM4_weight_signal =  DataSet_Test4_[DataSet_Test4_['label']==2]['weight'] \n",
        "\n",
        "DataSet_Train4_weight_signal = DataSet_Train4_[DataSet_Train4_['label']==1]['weight']\n",
        "DataSet_Train4_weight_backgr = DataSet_Train4_[DataSet_Train4_['label']==0]['weight'] \n",
        "\n",
        "DataSet_Test4_weight =  DataSet_Test4_['weight']\n",
        "\n",
        "print( 'Shape Test Signal' , DataSet_Test4_weight_signal.shape)\n",
        "print( 'Shape Test Background' , DataSet_Test4_weight_backgr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vV21L_7bvM3"
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def plot_precision_recall_curve( label_test, predicit, title, nome ):\n",
        "    label_test = label_binarize( label_test, classes = [*range(3) ] )\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    threshs = dict()\n",
        "    fontsize = 18\n",
        "\n",
        "    fig, axes = plt.subplots( 1, 2, figsize=(19,10) )\n",
        "\n",
        "    for i in range(3):\n",
        "        precision[i], recall[i], threshs[i] = precision_recall_curve( label_test[:,i], predicit[:, i])\n",
        "        axes[0].plot( recall[i], precision[i], lw = 2, label = ' class {}'.format(i) )\n",
        "    axes[0].set_xlabel( \"Recall\", fontsize = fontsize )\n",
        "    axes[0].set_ylabel( \"Precision\", fontsize = fontsize )\n",
        "    axes[0].legend( loc = \"best\", fontsize = 14 )\n",
        "    axes[0].set_title( title)\n",
        "\n",
        "    thresholds = np.concatenate( [ threshs[1], [1] ], axis = 0 )\n",
        "    bidx = np.argmax( precision[1] * recall[1] )\n",
        "    best_cut = threshs[1][bidx]\n",
        "    print( '\\n',' Best Cut ', best_cut,'\\n' )\n",
        "\n",
        "    axes[1].plot( thresholds , precision[1] * recall[1],  color = 'blue' )\n",
        "    axes[1].plot( [ best_cut , best_cut ] , [-0.1,(precision[1] * recall[1]).max()*2] ,\"-.r\",label='Best Cut : {:2.5f}'.format(best_cut) )\n",
        "    axes[1].plot( [-0.1,1.1] , [ ( precision[1]*recall[1] ).max(),( precision[1]*recall[1] ).max() ], \":g\",label=r'Precision $\\times$ Recall : {:2.5f}'.format( (precision[1]*recall[1]).max() ) )\n",
        "    axes[1].set_ylabel( r'Precision $\\times$ Recall', fontsize = fontsize )\n",
        "    axes[1].set_xlabel(    'Thresholds', fontsize = fontsize    )\n",
        "    axes[1].set_ylim( -0.1 , 1 )\n",
        "    axes[1].set_xlim( -0.01 , 1 )\n",
        "    axes[1].set_title( title )\n",
        "    axes[1].legend( loc = \"best\", fontsize = 14)\n",
        "\n",
        "def best_cut( label_test, predicit ):\n",
        "    label_test = label_binarize( label_test, classes = [ *range(3) ] )\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    threshs = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision[i], recall[i], threshs[i] = precision_recall_curve( label_test[:,i], predicit[:, i] )\n",
        "\n",
        "    thresholds = np.concatenate( [ threshs[1], [1] ], axis = 0 )\n",
        "    bidx = np.argmax( precision[1] * recall[1] )\n",
        "    return threshs[1][bidx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDRK_XocoKXT"
      },
      "source": [
        "def result_model(joblib, DataSet_Test, DataSet_Train, y_test, y_train):   \n",
        "    model_ANOMALO = load( PATH + joblib )\n",
        "    predict_ANOMALO = model_ANOMALO.predict_proba(DataSet_Test)\n",
        "    predict_ANOMALO_train = model_ANOMALO.predict_proba(DataSet_Train)\n",
        "    best_cut_ANOMALO = best_cut(y_train,predict_ANOMALO_train)\n",
        "    best_cut_ANOMALO_train = best_cut(y_test,predict_ANOMALO)\n",
        "\n",
        "    print( 'best cut', best_cut_ANOMALO )\n",
        "    label_pred_ANOMALO = [np.argmax(line) for line in predict_ANOMALO > best_cut_ANOMALO]\n",
        "\n",
        "    print('best_cut_test-->', best_cut_ANOMALO)\n",
        "    print(\"Purity in test sample     : {:2.2f}%\".format(100 * precision_score(y_test, predict_ANOMALO, average='weighted')))\n",
        "    print(\"Efficiency in test sample : {:2.2f}%\".format(100 * recall_score(y_test, predict_ANOMALO, average='weighted')))\n",
        "    print(\"Accuracy in test sample   : {:2.2f}%\".format(100 * accuracy_score(y_test, predict_ANOMALO, average='weighted')))\n",
        "    print( \"F1_score in test sample  : {:2.2f}%\".format(100 * f1_score(y_test, predict_ANOMALO, average='weighted')))\n",
        "    print(\"\\n\")\n",
        "    print('best_cut_train-->', best_cut_ANOMALO_train)\n",
        "    print(\"Purity in train sample     : {:2.2f}%\".format(100 * precision_score(y_train, predict_ANOMALO_train, average='weighted')))\n",
        "    print(\"Efficiency in train sample : {:2.2f}%\".format(100 * recall_score(y_train, predict_ANOMALO_train, average='weighted')))\n",
        "    print(\"Accuracy in train sample   : {:2.2f}%\".format(100 * accuracy_score(y_train, predict_ANOMALO_train, average='weighted')))\n",
        "    print( \"F1_score in train sample  : {:2.2f}%\".format(100 * f1_score(y_train, predict_ANOMALO_train, average='weighted')))\n",
        "    return predict_ANOMALO, best_cut_ANOMALO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rerAvoNvuQio"
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "def result( X_test, Y_test, X_train, Y_train, joblib, label, verbose = 1 ):\n",
        "    print(' ---------------- ' + label + '----------------' )\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_test = scaler.fit_transform(X_test)\n",
        "    X_scaled_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    model = load( PATH + joblib )\n",
        "    \n",
        "    predict_proba_test = model.predict_proba(X_scaled_test)  # calculate the probability\n",
        "    predict_proba_train = model.predict_proba(X_scaled_train)  # calculate the probability\n",
        "\n",
        "    label_test = label_binarize( Y_test, classes = [ *range(3) ] )\n",
        "    precision_test = dict()\n",
        "    recall_test = dict()\n",
        "    threshs_test = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision_test[i], recall_test[i], threshs_test[i] = precision_recall_curve( label_test[:,i], predict_proba_test[:,i] )\n",
        "\n",
        "    thresholds_test = np.concatenate( [ threshs_test[1], [1] ], axis = 0 )\n",
        "    bidx_test = np.argmax( precision_test[1] * recall_test[1] )\n",
        "    best_cut_test = threshs_test[1][bidx_test]\n",
        "    print('best cut test--> ', best_cut_test)\n",
        "    #y_preds_test = [ np.argmax(line) for line in predict_proba_test[:,1] > best_cut_test ]\n",
        "\n",
        "    label_predict_test = model.predict(X_scaled_test)\n",
        "    label_predict_train = model.predict(X_scaled_train)\n",
        "    print('label_predict_test', pd.DataFrame(label_predict_test).value_counts())\n",
        "    print('label_predict_train', pd.DataFrame(label_predict_train).value_counts())\n",
        "\n",
        "    label_train = label_binarize( Y_train, classes = [ *range(3) ] )\n",
        "    precision_train = dict()\n",
        "    recall_train = dict()\n",
        "    threshs_train = dict()\n",
        "\n",
        "    for i in range(3):\n",
        "        precision_train[i], recall_train[i], threshs_train[i] = precision_recall_curve( label_train[:,i], predict_proba_train[:,i] )\n",
        "\n",
        "    thresholds_train = np.concatenate( [ threshs_train[1], [1] ], axis = 0 )\n",
        "    bidx_train = np.argmax( precision_train[1] * recall_train[1] )\n",
        "    best_cut_train = threshs_train[1][bidx_train]\n",
        "    print('best cut train -->', best_cut_train)\n",
        "    y_preds_train = [ np.argmax(line) for line in predict_proba_train > best_cut_train ]    \n",
        "\n",
        "    if verbose == 1:\n",
        "        print(\"Purity in test sample     : {:2.2f}%\".format(100 * precision_score(Y_test, label_predict_test, average = 'weighted')))\n",
        "        print(\"Efficiency in test sample : {:2.2f}%\".format(100 * recall_score(Y_test, label_predict_test, average = 'weighted')))\n",
        "        print(\"F1_score in test sample   : {:2.2f}%\".format(100 * f1_score(Y_test, label_predict_test, average = 'weighted')))\n",
        "        print('\\n')\n",
        "        print(\"Purity in train sample     : {:2.2f}%\".format(100 * precision_score(Y_train, label_predict_train, average = 'weighted')))\n",
        "        print(\"Efficiency in train sample : {:2.2f}%\".format(100 * recall_score(Y_train, label_predict_train, average = 'weighted')))\n",
        "        print(\"F1_score in train sample   : {:2.2f}%\".format(100 * f1_score(Y_train, label_predict_train, average = 'weighted')))    \n",
        "\n",
        "    return predict_proba_test[:,1], best_cut_test, label_predict_test   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVvah5EqggKo"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled_dados = scaler.fit_transform(data_set_dados_multirp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6nKbwebfzl9"
      },
      "source": [
        "model_ANOMALO5 = load( PATH + 'LGBM_Multiclass_ANOMALO5_DataDriven.joblib' )\n",
        "model_ANOMALO6 = load( PATH + 'LGBM_Multiclass_ANOMALO6_DataDriven.joblib' )\n",
        "model_ANOMALO7 = load( PATH + 'LGBM_Multiclass_ANOMALO7_DataDriven.joblib' )\n",
        "model_ANOMALO8 = load( PATH + 'LGBM_Multiclass_ANOMALO8_DataDriven.joblib' )\n",
        "\n",
        "model_ANOMALO1 = load( PATH + 'LGBM_Multiclass_ANOMALO1_DataDriven.joblib' )\n",
        "model_ANOMALO2 = load( PATH + 'LGBM_Multiclass_ANOMALO2_DataDriven.joblib' )\n",
        "model_ANOMALO3 = load( PATH + 'LGBM_Multiclass_ANOMALO3_DataDriven.joblib' )\n",
        "model_ANOMALO4 = load( PATH + 'LGBM_Multiclass_ANOMALO4_DataDriven.joblib' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbgRl9wZNAGc"
      },
      "source": [
        "DataSet_Test8.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVhV4qZcufp1"
      },
      "source": [
        "predict_ANOMALO8, best_cut_ANOMALO8, label_predict_test8 = result(DataSet_Test8, y_test8, DataSet_Train8, y_train8, 'LGBM_Multiclass_ANOMALO8_DataDriven.joblib', 'ANOMALO8' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxmFm_6ez97h"
      },
      "source": [
        "predict_ANOMALO7, best_cut_ANOMALO7, label_predict_test7 = result(DataSet_Test7, y_test7, DataSet_Train7, y_train7, 'LGBM_Multiclass_ANOMALO7_DataDriven.joblib', 'ANOMALO7' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9xayfRF0DQ8"
      },
      "source": [
        "predict_ANOMALO6, best_cut_ANOMALO6, label_predict_test6 = result(DataSet_Test6, y_test6, DataSet_Train6, y_train6, 'LGBM_Multiclass_ANOMALO6_DataDriven.joblib', 'ANOMALO6' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR6vxKbF0JRX"
      },
      "source": [
        "predict_ANOMALO5, best_cut_ANOMALO5 = result(DataSet_Test5, y_test5, DataSet_Train5, y_train5, 'LGBM_Multiclass_ANOMALO5_DataDriven.joblib', 'ANOMALO5' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdgWW6jC0-PH"
      },
      "source": [
        "predict_ANOMALO4, best_cut_ANOMALO4 = result(DataSet_Test4, y_test4, DataSet_Train4, y_train4, 'LGBM_Multiclass_ANOMALO4_DataDriven.joblib', 'ANOMALO4' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QgTm9BM1CAr"
      },
      "source": [
        "predict_ANOMALO3, best_cut_ANOMALO3 = result(DataSet_Test3, y_test3, DataSet_Train3, y_train3, 'LGBM_Multiclass_ANOMALO3_DataDriven.joblib', 'ANOMALO3' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1LsRtoJ1Gbe"
      },
      "source": [
        "predict_ANOMALO2, best_cut_ANOMALO2 = result(DataSet_Test2, y_test2, DataSet_Train2, y_train2, 'LGBM_Multiclass_ANOMALO2_DataDriven.joblib', 'ANOMALO2' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1lYju5n1MiY"
      },
      "source": [
        "predict_ANOMALO1, best_cut_ANOMALO1 = result(DataSet_Test1, y_test1, DataSet_Train1, y_train1, 'LGBM_Multiclass_ANOMALO1_DataDriven.joblib', 'ANOMALO1' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR2s902EkjTn"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = model_ANOMALO4.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_ANOMALO4[ y_test4 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test4_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_ANOMALO4[ y_test4 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=8.0 \\times 10^{-6}$', weights = DataSet_Test4_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_ANOMALO4[ y_test4 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM4_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut_ANOMALO4,best_cut_ANOMALO4],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO4 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO4 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut4 = predict_ANOMALO4[ y_test4 == 0 ] > best_cut_ANOMALO4 \n",
        "n_events_signal_after_cut4 = predict_ANOMALO4[ y_test4 == 1 ] > best_cut_ANOMALO4\n",
        "n_events_SM_after_cut4 = predict_ANOMALO4[ y_test4 == 2 ] > best_cut_ANOMALO4\n",
        "n_eventos_Data_after_cut4 = predict_dados[ predict_dados >= best_cut_ANOMALO4]\n",
        "\n",
        "print(\" --------------- Anomalo 4 --------------- \")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test4_weight_backgr[ n_events_back_after_cut4 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test4_weight_signal[ n_events_signal_after_cut4].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM4_weight_signal[ n_events_SM_after_cut4 ].sum() / test_size )\n",
        "\n",
        "\n",
        "ax[0,0].text(best_cut_ANOMALO4+0.05 , 100, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper center\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = model_ANOMALO1.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_ANOMALO1[ y_test1 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Drive Background', weights = DataSet_Test1_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_ANOMALO1[ y_test1 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2}=20 \\times 10^{-6}$', weights = DataSet_Test1_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_ANOMALO1[ y_test1 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM1_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut_ANOMALO1,best_cut_ANOMALO1],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO1 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO1] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut1 = predict_ANOMALO1[ y_test1 == 0 ] > best_cut_ANOMALO1\n",
        "n_events_signal_after_cut1 = predict_ANOMALO1[ y_test1 == 1 ] > best_cut_ANOMALO1\n",
        "n_events_SM_after_cut1 = predict_ANOMALO1[ y_test1 == 2 ] > best_cut_ANOMALO1\n",
        "n_eventos_Data_after_cut1 = predict_dados[ predict_dados >= best_cut_ANOMALO1 ]\n",
        "\n",
        "print(' --------------- Anomalo 1 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test1_weight_backgr[ n_events_back_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test1_weight_signal[ n_events_signal_after_cut1].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM1_weight_signal[ n_events_SM_after_cut1 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut1 ) )\n",
        "\n",
        "\n",
        "ax[0,1].text(best_cut_ANOMALO1 + 0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16, rotation = 90 )    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper center\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = model_ANOMALO3.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_ANOMALO3[ y_test3 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test3_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_ANOMALO3[ y_test3 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test3_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_ANOMALO3[ y_test3 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM3_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut_ANOMALO3,best_cut_ANOMALO3],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO3 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO3 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Dados-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut3 = predict_ANOMALO3[ y_test3 == 0 ] >= best_cut_ANOMALO3\n",
        "n_events_signal_after_cut3 = predict_ANOMALO3[ y_test3 == 1 ] >= best_cut_ANOMALO3\n",
        "n_events_SM_after_cut3 = predict_ANOMALO3[ y_test3 == 2 ] >= best_cut_ANOMALO3\n",
        "n_eventos_Data_after_cut3 = predict_dados[ predict_dados >= best_cut_ANOMALO3 ]\n",
        "\n",
        "print(' --------------- Anomalo 3 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test3_weight_backgr[ n_events_back_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test3_weight_signal[ n_events_signal_after_cut3].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM3_weight_signal[ n_events_SM_after_cut3 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut3 ) )\n",
        "\n",
        "\n",
        "ax[1,0].text(best_cut_ANOMALO3+0.05 , 2, 'Signal Region \\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper right\", fontsize = 15  )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = model_ANOMALO2.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_ANOMALO2[ y_test2 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test2_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_ANOMALO2[ y_test2 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{C}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test2_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_ANOMALO2[ y_test2 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM2_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut_ANOMALO2,best_cut_ANOMALO2],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO2 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO2 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut2 = predict_ANOMALO2[ y_test2 == 0 ] > best_cut_ANOMALO2 \n",
        "n_events_signal_after_cut2 = predict_ANOMALO2[ y_test2 == 1 ] > best_cut_ANOMALO2\n",
        "n_events_SM_after_cut2 = predict_ANOMALO2[ y_test2 == 2 ] > best_cut_ANOMALO2\n",
        "n_eventos_Data_after_cut2 = predict_dados[ predict_dados > best_cut_ANOMALO2 ]\n",
        "\n",
        "print(' --------------- Anomalo 2 --------------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test2_weight_backgr[ n_events_back_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test2_weight_signal[ n_events_signal_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM2_weight_signal[ n_events_SM_after_cut2 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut2 ) )\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper right\", fontsize = 15 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut_ANOMALO2+0.05 , 2, 'Signal Region \\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUsVoZWEWD5p"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jQ26e3HJOz2"
      },
      "source": [
        "fig, ax = plt.subplots( 2, 2, figsize=(24,15) )\n",
        "predict_dados = model_ANOMALO5.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,0].hist( predict_ANOMALO5[ y_test5 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Background', weights = DataSet_Test5_weight_backgr/test_size )\n",
        "ax[0,0].hist( predict_ANOMALO5[ y_test5 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=0.5 \\times 10^{-6}$', weights = DataSet_Test5_weight_signal/test_size, color = 'green' )\n",
        "ax[0,0].hist( predict_ANOMALO5[ y_test5 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Standard Model', weights = DataSet_TestSM5_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,0].plot( [ best_cut_ANOMALO5,best_cut_ANOMALO5],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO5) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO5 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut5 = predict_ANOMALO5[ y_test5 == 0 ] > best_cut_ANOMALO5 \n",
        "n_events_signal_after_cut5 = predict_ANOMALO5[ y_test5 == 1 ] > best_cut_ANOMALO5\n",
        "n_events_SM_after_cut5 = predict_ANOMALO5[ y_test5 == 2 ] > best_cut_ANOMALO5\n",
        "n_eventos_Data_after_cut5 = predict_dados[ predict_dados >= best_cut_ANOMALO5 ]\n",
        "\n",
        "print(\"----------- Anomalo 5 -----------\")\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test5_weight_backgr[ n_events_back_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test5_weight_signal[ n_events_signal_after_cut5 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM5_weight_signal[ n_events_SM_after_cut5 ].sum() / test_size )\n",
        "\n",
        "\n",
        "ax[0,0].text(best_cut_ANOMALO5+0.05 , 6, 'Signal Region \\n\\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,0].set_ylabel( \"Events (Log Scale)\" )\n",
        "ax[0,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,0].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,0].set_yscale( 'log' )\n",
        "ax[0,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,0] )\n",
        "\n",
        "\n",
        "predict_dados = model_ANOMALO6.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[0,1].hist( predict_ANOMALO6[ y_test6 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test6_weight_backgr/test_size )\n",
        "ax[0,1].hist( predict_ANOMALO6[ y_test6 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2}=1.0 \\times 10^{-5}$', weights = DataSet_Test6_weight_signal/test_size, color = 'green' )\n",
        "ax[0,1].hist( predict_ANOMALO6[ y_test6 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM6_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[0,1].plot( [ best_cut_ANOMALO6,best_cut_ANOMALO6],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO6 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO6 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[0,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut6 = predict_ANOMALO6[ y_test6 == 0 ] > best_cut_ANOMALO6 \n",
        "n_events_signal_after_cut6 = predict_ANOMALO6[ y_test6 == 1 ] > best_cut_ANOMALO6\n",
        "n_events_SM_after_cut6 = predict_ANOMALO6[ y_test6 == 2 ] > best_cut_ANOMALO6\n",
        "n_eventos_Data_after_cut6 = predict_dados[ predict_dados >= best_cut_ANOMALO6 ]\n",
        "\n",
        "print('----------- Anomalo 6 -----------')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test6_weight_backgr[ n_events_back_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test6_weight_signal[ n_events_signal_after_cut6].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM6_weight_signal[ n_events_SM_after_cut6 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut6 ) )\n",
        "\n",
        "\n",
        "ax[0,1].text(best_cut_ANOMALO6+0.09 , 0.5, 'Signal Region \\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[0,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[0,1].legend( loc = \"upper right\", fontsize = 16 )\n",
        "ax[0,1].set_yscale( 'log' )\n",
        "ax[0,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[0,1] )\n",
        "\n",
        "predict_dados = model_ANOMALO7.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,0].hist( predict_ANOMALO7[ y_test7 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test7_weight_backgr/test_size )\n",
        "ax[1,0].hist( predict_ANOMALO7[ y_test7 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 2.0 \\times 10^{-6} $', weights = DataSet_Test7_weight_signal/test_size, color = 'green' )\n",
        "ax[1,0].hist( predict_ANOMALO7[ y_test7 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM7_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,0].plot( [ best_cut_ANOMALO7,best_cut_ANOMALO7],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO7 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO7 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,0].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut7 = predict_ANOMALO7[ y_test7 == 0 ] >= best_cut_ANOMALO7 \n",
        "n_events_signal_after_cut7 = predict_ANOMALO7[ y_test7 == 1 ] >= best_cut_ANOMALO7\n",
        "n_events_SM_after_cut7 = predict_ANOMALO7[ y_test7 == 2 ] >= best_cut_ANOMALO7\n",
        "n_eventos_Data_after_cut7 = predict_dados[ predict_dados >= best_cut_ANOMALO7 ]\n",
        "\n",
        "print('----------- Anomalo 7 -----------')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test7_weight_backgr[ n_events_back_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test7_weight_signal[ n_events_signal_after_cut7].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM7_weight_signal[ n_events_SM_after_cut7 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut7 ) )\n",
        "\n",
        "\n",
        "ax[1,0].text(best_cut_ANOMALO7+0.05 , 1, 'Signal Region \\n Blinded Data', color = 'red', fontsize = 16)    \n",
        "ax[1,0].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,0].legend( loc = \"upper center\", fontsize = 14 )\n",
        "ax[1,0].set_yscale( 'log' )\n",
        "ax[1,0] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,0] )\n",
        "\n",
        "predict_dados = model_ANOMALO8.predict_proba(X_scaled_dados)[:,1]\n",
        "bins = 25\n",
        "\n",
        "hbgt_lgb = ax[1,1].hist( predict_ANOMALO8[ y_test8 == 0 ], bins = np.linspace(0,1,bins), alpha=1, histtype = 'bar', color = 'pink', label = 'Data-Driven Background', weights = DataSet_Test8_weight_backgr/test_size )\n",
        "ax[1,1].hist( predict_ANOMALO8[ y_test8 == 1 ], bins = np.linspace(0,1,bins), ls='-', linewidth=2.5, histtype = 'step', label = r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6} $', weights = DataSet_Test8_weight_signal/test_size, color = 'green' )\n",
        "ax[1,1].hist( predict_ANOMALO8[ y_test8 == 2 ], bins = np.linspace(0,1,bins), ls='--', linewidth=2.5, histtype = 'step', label = 'Modelo Padrão', weights = DataSet_TestSM8_weight_signal/test_size, color = 'blue' )\n",
        "\n",
        "uppery_lgb=np.max(hbgt_lgb[0])\n",
        "ax[1,1].plot( [ best_cut_ANOMALO8,best_cut_ANOMALO8],[0,uppery_lgb],\"-.r\", linewidth=2.5,  label='Best cut : {:2.3f}'.format( best_cut_ANOMALO8 ) )\n",
        "  \n",
        "counts_1, bin_edges_1 = np.histogram( predict_dados[ predict_dados < best_cut_ANOMALO8 ] , bins = np.linspace(0,1,bins) )\n",
        "errors_1 = np.sqrt( counts_1 )\n",
        "bin_centres_1 = ( bin_edges_1[:-1] + bin_edges_1[1:] ) / 2.\n",
        "ax[1,1].errorbar( bin_centres_1, counts_1, yerr=errors_1, xerr=abs(bin_centres_1[1] - bin_centres_1[2])/2, fmt='.', label = 'Data-2016', color = 'black' )\n",
        "\n",
        "n_events_back_after_cut8 = predict_ANOMALO8[ y_test8 == 0 ] > best_cut_ANOMALO8 \n",
        "n_events_signal_after_cut8 = predict_ANOMALO8[ y_test8 == 1 ] > best_cut_ANOMALO8\n",
        "n_events_SM_after_cut8 = predict_ANOMALO8[ y_test8 == 2 ] > best_cut_ANOMALO8\n",
        "n_eventos_Data_after_cut8 = predict_dados[ predict_dados > best_cut_ANOMALO8 ]\n",
        "\n",
        "print(' ----------- Anomalo 8 ----------- ')\n",
        "print('Numero de eventos de background depois do corte -->', DataSet_Test8_weight_backgr[ n_events_back_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de signal depois do corte -->', DataSet_Test8_weight_signal[ n_events_signal_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de SM depois do corte -->', DataSet_TestSM8_weight_signal[ n_events_SM_after_cut8 ].sum() / test_size )\n",
        "print('Numero de eventos de dados depois do corte -->', len( n_eventos_Data_after_cut8 ) )\n",
        "\n",
        "\n",
        "ax[1,1].set_xlabel( \"Probability\" )\n",
        "ax[1,1].set_title( \"Mangueira Plot\", fontsize = 16  )\n",
        "ax[1,1].legend( loc = \"upper center\", fontsize = 16 )\n",
        "ax[1,1].set_yscale( 'log' )\n",
        "ax[1,1].text(best_cut_ANOMALO8 + 0.02 , 50, 'Signal Region \\n Blinded Data', color = 'red', fontsize = 16, rotation = 90)    \n",
        "ax[1,1] = hep.cms.label( llabel=\"Preliminary\", rlabel=r\"$\\mathcal{L} = 9.792\\;fb^{-1}$ 13(TeV)\", fontsize = 16, ax = ax[1,1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHEsACmtKKac"
      },
      "source": [
        "best_cut_ANOMALO1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sTVY6znF8iT"
      },
      "source": [
        "y_pred_1 = [np.argmax(line) for line in predict_ANOMALO1 > best_cut_ANOMALO1 ]\n",
        "y_pred_2 = [np.argmax(line) for line in predict_ANOMALO2 > best_cut_ANOMALO2 ]\n",
        "y_pred_3 = [np.argmax(line) for line in predict_ANOMALO3 > best_cut_ANOMALO3 ]\n",
        "y_pred_4 = [np.argmax(line) for line in predict_ANOMALO4 > best_cut_ANOMALO4 ]\n",
        "y_pred_5 = [np.argmax(line) for line in predict_ANOMALO5 > best_cut_ANOMALO5 ]\n",
        "y_pred_6 = [np.argmax(line) for line in predict_ANOMALO6 > best_cut_ANOMALO6 ]\n",
        "y_pred_7 = [np.argmax(line) for line in predict_ANOMALO7 > best_cut_ANOMALO7 ]\n",
        "y_pred_8 = [np.argmax(line) for line in predict_ANOMALO8 > best_cut_ANOMALO8 ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfoGxUl9bG_J"
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "def ConfusionMatrix( class_true, class_predict, cmap, title, sample_weight, fmt = 'f' ):\n",
        "    plt.subplots( figsize=(10,7) )\n",
        "    conf_mat = multilabel_confusion_matrix( y_true = class_true, y_pred = class_predict, sample_weight = sample_weight)\n",
        "\n",
        "    sns.heatmap(conf_mat, annot=True, fmt=fmt, cmap = cmap) # fmt = d ou f\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted', fontsize = 30 )\n",
        "    plt.ylabel('Expected' , fontsize = 30 )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe83wSrHA3Ga"
      },
      "source": [
        "ConfusionMatrix(y_test8,y_pred_8,'YlGnBu',r'WWCEP $\\alpha_{0}^{W}/\\Lambda^{2} = 5.0 \\times 10^{-6}$', DataSet_Test8_weight/test_size )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkFse6YNFRDM"
      },
      "source": [
        "conf_mat = multilabel_confusion_matrix( y_true = y_test8, y_pred = y_pred_8, sample_weight = DataSet_Test8_weight/test_size )\n",
        "print(conf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7wF_mAhExX7"
      },
      "source": [
        "y_test8.hist()\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COw5cDnrINAs"
      },
      "source": [
        "pd.DataFrame(y_pred_1).hist()\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pbGMvgrIX8K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}